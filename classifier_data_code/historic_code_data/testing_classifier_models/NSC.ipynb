{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "400c5041-4cab-4159-bbf0-06c429be3cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from pathlib import Path\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import NearestCentroid\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e596abd4-8429-472e-9871-3e065fa229a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nation</th>\n",
       "      <th>gender</th>\n",
       "      <th>category</th>\n",
       "      <th>mean_sen_len</th>\n",
       "      <th>male_pronouns</th>\n",
       "      <th>female_pronouns</th>\n",
       "      <th>TTR</th>\n",
       "      <th>lex_density</th>\n",
       "      <th>VADER_sentiment</th>\n",
       "      <th>concreteness</th>\n",
       "      <th>...</th>\n",
       "      <th>again</th>\n",
       "      <th>other</th>\n",
       "      <th>must</th>\n",
       "      <th>after</th>\n",
       "      <th>go</th>\n",
       "      <th>might</th>\n",
       "      <th>too</th>\n",
       "      <th>through</th>\n",
       "      <th>himself</th>\n",
       "      <th>author</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Dickens_3787.txt</th>\n",
       "      <td>British/Irish</td>\n",
       "      <td>male</td>\n",
       "      <td>authentic</td>\n",
       "      <td>29.411765</td>\n",
       "      <td>0.013921</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.494200</td>\n",
       "      <td>0.535963</td>\n",
       "      <td>0.9962</td>\n",
       "      <td>2.506728</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00232</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002320</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006961</td>\n",
       "      <td>dickens</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bronte_119.txt</th>\n",
       "      <td>British/Irish</td>\n",
       "      <td>female</td>\n",
       "      <td>authentic</td>\n",
       "      <td>20.833333</td>\n",
       "      <td>0.004988</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.633416</td>\n",
       "      <td>0.596010</td>\n",
       "      <td>0.8328</td>\n",
       "      <td>2.606356</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002494</td>\n",
       "      <td>0.002494</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002494</td>\n",
       "      <td>0.002494</td>\n",
       "      <td>bronte</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Austen_424.txt</th>\n",
       "      <td>British/Irish</td>\n",
       "      <td>female</td>\n",
       "      <td>authentic</td>\n",
       "      <td>22.727273</td>\n",
       "      <td>0.024450</td>\n",
       "      <td>0.022005</td>\n",
       "      <td>0.418093</td>\n",
       "      <td>0.586797</td>\n",
       "      <td>0.9350</td>\n",
       "      <td>2.560488</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.004890</td>\n",
       "      <td>0.002445</td>\n",
       "      <td>0.007335</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002445</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>austen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Alcott_42.txt</th>\n",
       "      <td>American</td>\n",
       "      <td>female</td>\n",
       "      <td>authentic</td>\n",
       "      <td>20.080000</td>\n",
       "      <td>0.021792</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.518160</td>\n",
       "      <td>0.539952</td>\n",
       "      <td>0.9897</td>\n",
       "      <td>2.559505</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.002421</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004843</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>alcott</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DICKENS_synthetic_combined_146.txt</th>\n",
       "      <td>British/Irish</td>\n",
       "      <td>male</td>\n",
       "      <td>synthetic</td>\n",
       "      <td>23.809524</td>\n",
       "      <td>0.018141</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.535147</td>\n",
       "      <td>0.562358</td>\n",
       "      <td>0.5106</td>\n",
       "      <td>2.622072</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006803</td>\n",
       "      <td>0.002268</td>\n",
       "      <td>dickens</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 211 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           nation  gender   category  \\\n",
       "ID                                                                     \n",
       "Dickens_3787.txt                    British/Irish    male  authentic   \n",
       "Bronte_119.txt                      British/Irish  female  authentic   \n",
       "Austen_424.txt                      British/Irish  female  authentic   \n",
       "Alcott_42.txt                            American  female  authentic   \n",
       "DICKENS_synthetic_combined_146.txt  British/Irish    male  synthetic   \n",
       "\n",
       "                                    mean_sen_len  male_pronouns  \\\n",
       "ID                                                                \n",
       "Dickens_3787.txt                       29.411765       0.013921   \n",
       "Bronte_119.txt                         20.833333       0.004988   \n",
       "Austen_424.txt                         22.727273       0.024450   \n",
       "Alcott_42.txt                          20.080000       0.021792   \n",
       "DICKENS_synthetic_combined_146.txt     23.809524       0.018141   \n",
       "\n",
       "                                    female_pronouns       TTR  lex_density  \\\n",
       "ID                                                                           \n",
       "Dickens_3787.txt                           0.000000  0.494200     0.535963   \n",
       "Bronte_119.txt                             0.000000  0.633416     0.596010   \n",
       "Austen_424.txt                             0.022005  0.418093     0.586797   \n",
       "Alcott_42.txt                              0.000000  0.518160     0.539952   \n",
       "DICKENS_synthetic_combined_146.txt         0.000000  0.535147     0.562358   \n",
       "\n",
       "                                    VADER_sentiment  concreteness  ...  \\\n",
       "ID                                                                 ...   \n",
       "Dickens_3787.txt                             0.9962      2.506728  ...   \n",
       "Bronte_119.txt                               0.8328      2.606356  ...   \n",
       "Austen_424.txt                               0.9350      2.560488  ...   \n",
       "Alcott_42.txt                                0.9897      2.559505  ...   \n",
       "DICKENS_synthetic_combined_146.txt           0.5106      2.622072  ...   \n",
       "\n",
       "                                      again  other      must     after  \\\n",
       "ID                                                                       \n",
       "Dickens_3787.txt                    0.00232    0.0  0.000000  0.000000   \n",
       "Bronte_119.txt                      0.00000    0.0  0.000000  0.000000   \n",
       "Austen_424.txt                      0.00000    0.0  0.004890  0.002445   \n",
       "Alcott_42.txt                       0.00000    0.0  0.002421  0.000000   \n",
       "DICKENS_synthetic_combined_146.txt  0.00000    0.0  0.000000  0.000000   \n",
       "\n",
       "                                          go     might       too   through  \\\n",
       "ID                                                                           \n",
       "Dickens_3787.txt                    0.000000  0.000000  0.002320  0.000000   \n",
       "Bronte_119.txt                      0.002494  0.002494  0.000000  0.002494   \n",
       "Austen_424.txt                      0.007335  0.000000  0.002445  0.000000   \n",
       "Alcott_42.txt                       0.004843  0.000000  0.000000  0.000000   \n",
       "DICKENS_synthetic_combined_146.txt  0.000000  0.000000  0.000000  0.006803   \n",
       "\n",
       "                                     himself   author  \n",
       "ID                                                     \n",
       "Dickens_3787.txt                    0.006961  dickens  \n",
       "Bronte_119.txt                      0.002494   bronte  \n",
       "Austen_424.txt                      0.000000   austen  \n",
       "Alcott_42.txt                       0.000000   alcott  \n",
       "DICKENS_synthetic_combined_146.txt  0.002268  dickens  \n",
       "\n",
       "[5 rows x 211 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Getting list of 20 different clusters of text\n",
    "\n",
    "import random\n",
    "\n",
    "df = pd.read_csv(\"master_stats.csv\")\n",
    "master_text_list = df['ID'].tolist()\n",
    "prefixes = [name.replace('.txt', '') for name in master_text_list]\n",
    "\n",
    "# Getting list of all 500 token chunk filenames\n",
    "\n",
    "chunks_dir = Path('corpus_chunks')\n",
    "\n",
    "chunk_names = [f for f in os.listdir(chunks_dir) if os.path.isfile(os.path.join(chunks_dir, f))]\n",
    "\n",
    "\n",
    "files_by_prefix = {prefix: [] for prefix in prefixes}\n",
    "\n",
    "# Iterate over all files in the directory and group them by prefix\n",
    "for file_path in chunks_dir.iterdir():\n",
    "    if file_path.is_file():\n",
    "        # Extract the prefix (the part before the last underscore)\n",
    "        file_name = file_path.name\n",
    "        parts = file_name.rsplit('_', 1)  # Split from the right at the last underscore\n",
    "        file_prefix = parts[0] if len(parts) > 1 else file_name  # Get the part before the last underscore\n",
    "\n",
    "        # If the prefix is one of the predefined prefixes, add the file to the list\n",
    "        if file_prefix in files_by_prefix:\n",
    "            files_by_prefix[file_prefix].append(file_path)\n",
    "\n",
    "## Initialize a list to store the sampled file names\n",
    "sampled_files_list = []\n",
    "\n",
    "# Randomly sample 100 files from each prefix group\n",
    "for prefix, files in files_by_prefix.items():\n",
    "    # Check if there are at least 100 files to sample\n",
    "    if len(files) > 100:\n",
    "        sampled = random.sample(files, 100)\n",
    "    else:\n",
    "        sampled = files  # If fewer than 100 files, sample all of them\n",
    "\n",
    "    # Add the filenames of the sampled files to the list\n",
    "    sampled_files_list.extend(file.name for file in sampled)\n",
    "\n",
    "\n",
    "chunks_df = pd.read_csv(\"master_features_chunks.csv\")\n",
    "chunks_df['author'] = chunks_df['ID'].apply(lambda x: x.split('_')[0].lower())\n",
    "\n",
    "sample_df = chunks_df.loc[chunks_df['ID'].isin(sampled_files_list)]\n",
    "sample_df = sample_df.set_index('ID')\n",
    "sample_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bdaa56b9-3159-419f-962f-2ca310e5370c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 2000 entries, Dickens_3787.txt to GASKELL_synthetic_combined_182.txt\n",
      "Columns: 211 entries, nation to author\n",
      "dtypes: float64(205), int64(2), object(4)\n",
      "memory usage: 3.2+ MB\n"
     ]
    }
   ],
   "source": [
    "sample_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "31cae814-d94e-4473-84ee-6bbd105089e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['nation', 'gender', 'category', 'mean_sen_len', 'male_pronouns',\n",
       "       'female_pronouns', 'TTR', 'lex_density', 'VADER_sentiment',\n",
       "       'concreteness', 'said', 'mr', 'little', 'time', 'like', 'know',\n",
       "       'man', 'old', 'hand', 'come', 'miss', 'day', 'good', 'eye',\n",
       "       'thought', 'way', 'think', 'face', 'sir', 'great', 'came', 'thing',\n",
       "       'long', 'heart', 'away', 'young', 'went', 'look', 'word', 'lady',\n",
       "       'life', 'dear', 'head', 'room', 'house', 'looked', 'night', 'mind',\n",
       "       'shall', 'friend', 'tell', 'place', 'woman', 'child', 'took',\n",
       "       'door', 'let', 'found', 'mother', 'home', 'got', 'gentleman',\n",
       "       'father', 'saw', 'better', 'love', 'don', 'going', 'knew', 'boy',\n",
       "       'people', 'right', 'hope', 'moment', 'year', 'world', 'voice',\n",
       "       'left', 'poor', 'looking', 'asked', 'girl', 'felt', 'sat', 'new',\n",
       "       'air', 'oh', 'round', 'want', 'having', 'soon', 'heard', 'mean',\n",
       "       'stood', 'find', 'light', 'men', 'yes', 'told', 'hour', 'street',\n",
       "       'seen', 'sure', 'morning', 'returned', 'manner', 'cried', 'kind',\n",
       "       'replied', 'work', 'the', 'and', 'of', 'to', 'a', 'i', 'in',\n",
       "       'that', 'was', 'it', 'he', 'her', 'his', 'you', 'with', 'as',\n",
       "       'for', 'she', 'had', 'not', 'but', 'at', 'on', 'be', 'is', 'my',\n",
       "       'him', 'have', 'me', 'so', 'all', 'by', 'this', 'which', 'they',\n",
       "       'were', 'if', 'from', 'there', 'no', 'would', 'when', \"'s\", 'one',\n",
       "       'or', 'an', 'do', 'what', 'we', 'been', 'could', 'up', 'out',\n",
       "       'very', 'their', 'who', 'them', 'are', 'now', 'more', 'will',\n",
       "       'your', 'into', 'upon', 'then', 'some', 'did', 'any', 'about',\n",
       "       'than', 'can', 'down', \"n't\", 'much', 'such', 'see', 'before',\n",
       "       'never', 'where', 'well', 'over', 'how', 'am', 'only', 'should',\n",
       "       'made', 'say', 'its', 'has', 'own', 'here', 'again', 'other',\n",
       "       'must', 'after', 'go', 'might', 'too', 'through', 'himself',\n",
       "       'author'], dtype=object)"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_df.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "d81a07a1-908c-46cd-89cf-f7e8f9238943",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 2000 entries, Twain_2265.txt to ALCOTT_synthetic_combined_192.txt\n",
      "Columns: 211 entries, nation to author\n",
      "dtypes: float64(205), int64(2), object(4)\n",
      "memory usage: 3.2+ MB\n"
     ]
    }
   ],
   "source": [
    "sample_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "9681e452-fd6d-4ef9-aeaa-a772eb6eaee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = sample_df.drop(columns=['nation', 'gender', 'category', 'author'])  # Drop the label column to get the features\n",
    "\n",
    "#Below testing how much the classifier degrades when only using mean_sentence_length as a feature\n",
    "#X = sample_df.drop(columns=['nation', 'gender', 'category', 'author', 'male_pronouns','female_pronouns', 'TTR', 'lex_density',\n",
    "       'concreteness', 'said', 'mr', 'little', 'time', 'like', 'know',\n",
    "       'man', 'old', 'hand', 'come', 'miss', 'day', 'good', 'eye',\n",
    "       'thought', 'way', 'think', 'face', 'sir', 'great', 'came', 'thing',\n",
    "       'long', 'heart', 'away', 'young', 'went', 'look', 'word', 'lady',\n",
    "       'life', 'dear', 'head', 'room', 'house', 'looked', 'night', 'mind',\n",
    "       'shall', 'friend', 'tell', 'place', 'woman', 'child', 'took',\n",
    "       'door', 'let', 'found', 'mother', 'home', 'got', 'gentleman',\n",
    "       'father', 'saw', 'better', 'love', 'don', 'going', 'knew', 'boy',\n",
    "       'people', 'right', 'hope', 'moment', 'year', 'world', 'voice',\n",
    "       'left', 'poor', 'looking', 'asked', 'girl', 'felt', 'sat', 'new',\n",
    "       'air', 'oh', 'round', 'want', 'having', 'soon', 'heard', 'mean',\n",
    "       'stood', 'find', 'light', 'men', 'yes', 'told', 'hour', 'street',\n",
    "       'seen', 'sure', 'morning', 'returned', 'manner', 'cried', 'kind',\n",
    "       'replied', 'work', 'the', 'and', 'of', 'to', 'a', 'i', 'in',\n",
    "       'that', 'was', 'it', 'he', 'her', 'his', 'you', 'with', 'as',\n",
    "       'for', 'she', 'had', 'not', 'but', 'at', 'on', 'be', 'is', 'my',\n",
    "       'him', 'have', 'me', 'so', 'all', 'by', 'this', 'which', 'they',\n",
    "       'were', 'if', 'from', 'there', 'no', 'would', 'when', \"'s\", 'one',\n",
    "       'or', 'an', 'do', 'what', 'we', 'been', 'could', 'up', 'out',\n",
    "       'very', 'their', 'who', 'them', 'are', 'now', 'more', 'will',\n",
    "       'your', 'into', 'upon', 'then', 'some', 'did', 'any', 'about',\n",
    "       'than', 'can', 'down', \"n't\", 'much', 'such', 'see', 'before',\n",
    "       'never', 'where', 'well', 'over', 'how', 'am', 'only', 'should',\n",
    "       'made', 'say', 'its', 'has', 'own', 'here', 'again', 'other',\n",
    "       'must', 'after', 'go', 'might', 'too', 'through', 'himself'])\n",
    "y = sample_df['category']  # Target variable\n",
    "# If 'category' is a string, convert it to numerical labels\n",
    "y, original_categories = pd.factorize(y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "28f4fb72-7315-46f7-ba14-048d142617b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7175\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.75      0.71       188\n",
      "           1       0.76      0.69      0.72       212\n",
      "\n",
      "    accuracy                           0.72       400\n",
      "   macro avg       0.72      0.72      0.72       400\n",
      "weighted avg       0.72      0.72      0.72       400\n",
      "\n",
      "Weighted Precision: 0.7210775700232784\n",
      "Weighted Recall: 0.7175\n",
      "Weighted F1 Score: 0.7176677605875917\n"
     ]
    }
   ],
   "source": [
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the Nearest Centroid model with shrinkage\n",
    "model = NearestCentroid(shrink_threshold=0.1)  # Adjust shrink_threshold as needed\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy}')\n",
    "\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(y_test, y_pred, average='weighted')\n",
    "\n",
    "# Print the classification report\n",
    "print('Classification Report:')\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Save precision, recall, and f1 scores as variables\n",
    "print(f'Weighted Precision: {precision}')\n",
    "print(f'Weighted Recall: {recall}')\n",
    "print(f'Weighted F1 Score: {f1}')\n",
    "\n",
    "report = classification_report(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "e51f7df4-bf88-44bd-ae77-3e9d4e977c33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d5bc9ea-92ff-4730-b3b1-f00cc4584a33",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Print the test results in the form 'index, category'\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m test_results \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241m.\u001b[39mDataFrame({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mindex\u001b[39m\u001b[38;5;124m'\u001b[39m: X_test\u001b[38;5;241m.\u001b[39mindex, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpredicted_category\u001b[39m\u001b[38;5;124m'\u001b[39m: y_pred})\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "# Print the test results in the form 'index, category'\n",
    "test_results = pd.DataFrame({'index': X_test.index, 'predicted_category': y_pred})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "deed600e-3836-4bc8-b3ae-4c5d23869dae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Incorrectly labeled samples (with ID, actual category, and predicted category):\n",
      "                                   actual_category predicted_category\n",
      "ID                                                                   \n",
      "Alcott_145.txt                           authentic          synthetic\n",
      "Bronte_1040.txt                          authentic          synthetic\n",
      "Twain_267.txt                            authentic          synthetic\n",
      "Griggs_320.txt                           authentic          synthetic\n",
      "Hopkins_170.txt                          authentic          synthetic\n",
      "...                                            ...                ...\n",
      "GRIGGS_synthetic_combined_137.txt        synthetic          authentic\n",
      "BRONTE_synthetic_combined_159.txt        synthetic          authentic\n",
      "STOKER_synthetic_combined_20.txt         synthetic          authentic\n",
      "DICKENS_synthetic_combined_119.txt       synthetic          authentic\n",
      "DICKENS_synthetic_combined_52.txt        synthetic          authentic\n",
      "\n",
      "[113 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# Convert y_test and y_pred back to the original string categories\n",
    "y_test_original = pd.Series(original_categories[y_test], index=X_test.index)\n",
    "y_pred_original = pd.Series(original_categories[y_pred], index=X_test.index)\n",
    "\n",
    "# Create a DataFrame with the actual and predicted categories\n",
    "y_test_df = pd.DataFrame({\n",
    "    'actual_category': y_test_original,\n",
    "    'predicted_category': y_pred_original\n",
    "})\n",
    "\n",
    "# Find the incorrectly labeled rows\n",
    "incorrectly_labeled = y_test_df[y_test_df['actual_category'] != y_test_df['predicted_category']]\n",
    "incorrectly_labeled = incorrectly_labeled.sort_values(\"actual_category\")\n",
    "\n",
    "# Print the incorrectly labeled rows with ID, actual category, and predicted category\n",
    "print(\"Incorrectly labeled samples (with ID, actual category, and predicted category):\")\n",
    "print(incorrectly_labeled)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "a2d9b2d3-8ad7-471d-9dfb-c6e07f86026e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Alcott_145.txt', 'Bronte_1040.txt', 'Twain_267.txt', 'Griggs_320.txt', 'Hopkins_170.txt', 'Austen_1359.txt', 'Griggs_321.txt', 'Alcott_1286.txt', 'Chesnutt_302.txt', 'Alcott_1322.txt', 'Austen_1276.txt', 'Austen_907.txt', 'Chesnutt_587.txt', 'Gaskell_1811.txt', 'Gaskell_1388.txt', 'Twain_205.txt', 'Alcott_905.txt', 'Twain_739.txt', 'Gaskell_584.txt', 'Alcott_1863.txt', 'Dickens_3675.txt', 'Bronte_241.txt', 'Gaskell_2630.txt', 'Chesnutt_445.txt', 'Griggs_96.txt', 'Austen_1028.txt', 'Gaskell_445.txt', 'Dickens_8516.txt', 'Austen_433.txt', 'Twain_332.txt', 'Austen_663.txt', 'Austen_341.txt', 'Hopkins_51.txt', 'Austen_1475.txt', 'Hopkins_35.txt', 'Dickens_2821.txt', 'Dickens_3904.txt', 'Stoker_809.txt', 'Bronte_1350.txt', 'Alcott_1978.txt', 'Austen_563.txt', 'Dickens_3550.txt', 'Chesnutt_609.txt', 'Twain_614.txt', 'Chesnutt_122.txt', 'Bronte_1312.txt', 'Dickens_2975.txt']\n",
      "['GASKELL_synthetic_combined_126.txt', 'CHESNUTT_synthetic_combined_79.txt', 'ALCOTT_synthetic_combined_78.txt', 'AUSTEN_synthetic_combined_57.txt', 'ALCOTT_synthetic_combined_31.txt', 'AUSTEN_synthetic_combined_25.txt', 'DICKENS_synthetic_combined_121.txt', 'GASKELL_synthetic_combined_115.txt', 'BRONTE_synthetic_combined_70.txt', 'CHESNUTT_synthetic_combined_28.txt', 'GASKELL_synthetic_combined_38.txt', 'ALCOTT_synthetic_combined_51.txt', 'BRONTE_synthetic_combined_5.txt', 'GASKELL_synthetic_combined_152.txt', 'HOPKINS_synthetic_combined_22.txt', 'BRONTE_synthetic_combined_92.txt', 'ALCOTT_synthetic_combined_65.txt', 'CHESNUTT_synthetic_combined_175.txt', 'GRIGGS_synthetic_combined_34.txt', 'AUSTEN_synthetic_combined_48.txt', 'AUSTEN_synthetic_combined_58.txt', 'TWAIN_synthetic_combined_156.txt', 'TWAIN_synthetic_combined_159.txt', 'STOKER_synthetic_combined_143.txt', 'HOPKINS_synthetic_combined_41.txt', 'DICKENS_synthetic_combined_122.txt', 'ALCOTT_synthetic_combined_26.txt', 'TWAIN_synthetic_combined_78.txt', 'BRONTE_synthetic_combined_169.txt', 'BRONTE_synthetic_combined_102.txt', 'BRONTE_synthetic_combined_137.txt', 'STOKER_synthetic_combined_139.txt', 'TWAIN_synthetic_combined_130.txt', 'STOKER_synthetic_combined_54.txt', 'BRONTE_synthetic_combined_14.txt', 'STOKER_synthetic_combined_24.txt', 'CHESNUTT_synthetic_combined_127.txt', 'STOKER_synthetic_combined_70.txt', 'BRONTE_synthetic_combined_123.txt', 'BRONTE_synthetic_combined_173.txt', 'DICKENS_synthetic_combined_151.txt', 'STOKER_synthetic_combined_48.txt', 'TWAIN_synthetic_combined_190.txt', 'TWAIN_synthetic_combined_44.txt', 'BRONTE_synthetic_combined_121.txt', 'TWAIN_synthetic_combined_68.txt', 'DICKENS_synthetic_combined_163.txt', 'STOKER_synthetic_combined_17.txt', 'STOKER_synthetic_combined_80.txt', 'BRONTE_synthetic_combined_133.txt', 'STOKER_synthetic_combined_62.txt', 'BRONTE_synthetic_combined_115.txt', 'STOKER_synthetic_combined_21.txt', 'GASKELL_synthetic_combined_65.txt', 'TWAIN_synthetic_combined_143.txt', 'DICKENS_synthetic_combined_106.txt', 'TWAIN_synthetic_combined_137.txt', 'HOPKINS_synthetic_combined_31.txt', 'AUSTEN_synthetic_combined_13.txt', 'TWAIN_synthetic_combined_165.txt', 'CHESNUTT_synthetic_combined_48.txt', 'GRIGGS_synthetic_combined_137.txt', 'BRONTE_synthetic_combined_159.txt', 'STOKER_synthetic_combined_20.txt', 'DICKENS_synthetic_combined_119.txt', 'DICKENS_synthetic_combined_52.txt']\n"
     ]
    }
   ],
   "source": [
    "authentic_mislabeled = []\n",
    "synthetic_mislabeled = []\n",
    "\n",
    "for index, row in incorrectly_labeled.iterrows():\n",
    "    if row[\"actual_category\"] == \"authentic\":\n",
    "        authentic_mislabeled.append(index)\n",
    "    else:\n",
    "        synthetic_mislabeled.append(index)\n",
    "\n",
    "print(authentic_mislabeled)\n",
    "print(synthetic_mislabeled)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fefad3c-238c-4936-8447-1de7d1c27f88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "f69c9a89-0951-43c6-b5ab-a3aac0433211",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Importance:\n",
      "           feature  importance\n",
      "0     mean_sen_len    4.565209\n",
      "1  VADER_sentiment    0.087777\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "centroids = model.centroids_\n",
    "\n",
    "# Calculate the importance of each feature as the mean absolute difference between centroids\n",
    "# For binary classification, this is simply the difference between the two centroids\n",
    "if centroids.shape[0] == 2:  # Binary classification\n",
    "    feature_importance = np.abs(centroids[0] - centroids[1])\n",
    "else:\n",
    "    # For multi-class, calculate the mean absolute difference between all pairs of centroids\n",
    "    feature_importance = np.mean(np.abs(centroids[:, np.newaxis] - centroids[np.newaxis, :]), axis=(0, 1))\n",
    "\n",
    "# Create a DataFrame to hold the feature names and their corresponding importance scores\n",
    "feature_importance_df = pd.DataFrame({\n",
    "    'feature': X.columns,\n",
    "    'importance': feature_importance\n",
    "})\n",
    "\n",
    "# Sort features by importance\n",
    "feature_importance_df = feature_importance_df.sort_values(by='importance', ascending=False)\n",
    "\n",
    "# Print the feature importance\n",
    "print(\"Feature Importance:\")\n",
    "print(feature_importance_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "a04a97c7-d97f-422d-981e-38bdd65af4ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'mean_sen_len': 4.565209352467527, 'VADER_sentiment': 0.08777749548464464}\n"
     ]
    }
   ],
   "source": [
    "top_10_df = feature_importance_df[:10]\n",
    "top_10_df.head(10)\n",
    "\n",
    "top_10_features = dict(zip(top_10_df['feature'], top_10_df['importance']))\n",
    "\n",
    "print(top_10_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "f14aeb73-9b15-40fc-a9b1-53d5d238394d",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(columns=['samples', 'accuracy(ave_F1)', 'ave_precision', 'ave_recall', 'authentic_mislabeled', 'synthetic_mislabeled', 'top_10_features'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "141209dc-60e8-4150-ac46-f4ce602132cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>samples</th>\n",
       "      <th>accuracy(ave_F1)</th>\n",
       "      <th>ave_precision</th>\n",
       "      <th>ave_recall</th>\n",
       "      <th>authentic_mislabeled</th>\n",
       "      <th>synthetic_mislabeled</th>\n",
       "      <th>top_10_features</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [samples, accuracy(ave_F1), ave_precision, ave_recall, authentic_mislabeled, synthetic_mislabeled, top_10_features]\n",
       "Index: []"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "5ef301a9-d039-4c65-ae93-5f2f34ba9b76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>samples</th>\n",
       "      <th>accuracy(ave_F1)</th>\n",
       "      <th>ave_precision</th>\n",
       "      <th>ave_recall</th>\n",
       "      <th>authentic_mislabeled</th>\n",
       "      <th>synthetic_mislabeled</th>\n",
       "      <th>top_10_features</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[ALCOTT_synthetic_combined_132.txt, ALCOTT_syn...</td>\n",
       "      <td>0.715345</td>\n",
       "      <td>0.721987</td>\n",
       "      <td>0.7175</td>\n",
       "      <td>[Twain_740.txt, Twain_1485.txt, Twain_2581.txt...</td>\n",
       "      <td>[ALCOTT_synthetic_combined_23.txt, HOPKINS_syn...</td>\n",
       "      <td>{'mean_sen_len': 4.857105858499239, 'VADER_sen...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             samples accuracy(ave_F1)  \\\n",
       "0  [ALCOTT_synthetic_combined_132.txt, ALCOTT_syn...         0.715345   \n",
       "\n",
       "  ave_precision ave_recall                               authentic_mislabeled  \\\n",
       "0      0.721987     0.7175  [Twain_740.txt, Twain_1485.txt, Twain_2581.txt...   \n",
       "\n",
       "                                synthetic_mislabeled  \\\n",
       "0  [ALCOTT_synthetic_combined_23.txt, HOPKINS_syn...   \n",
       "\n",
       "                                     top_10_features  \n",
       "0  {'mean_sen_len': 4.857105858499239, 'VADER_sen...  "
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df.at[0, 'samples'] = sampled_files_list\n",
    "results_df.at[0, 'accuracy(ave_F1)'] = f1\n",
    "results_df.at[0, 'ave_precision'] = precision\n",
    "results_df.at[0, 'ave_recall'] = recall\n",
    "results_df.at[0, 'authentic_mislabeled'] = authentic_mislabeled\n",
    "results_df.at[0, 'synthetic_mislabeled'] = synthetic_mislabeled\n",
    "results_df.at[0, 'top_10_features'] = top_10_features\n",
    "\n",
    "results_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f218157e-c5b3-4520-b06b-62ab4cec2d99",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "a0ff1909-e1ce-4486-b776-3ac0c8910f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a new csv and print to first line\n",
    "\n",
    "#results_df.to_csv('NSC_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "650f6306-778c-4bd5-834e-3dd86fa7440d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the EXISTING CSV\n",
    "\n",
    "results_df.to_csv('NSC_data.csv', mode='a', header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "896a4202-b1c8-481b-9037-eb3f08f4d3a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4a96139-7f21-46f9-b94c-08f18769eb97",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a74f64e7-8328-4edc-bc22-70c9060f7c51",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "faeef105-62c8-40b0-baac-ecf1c94c8738",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>samples</th>\n",
       "      <th>accuracy(ave_F1)</th>\n",
       "      <th>ave_precision</th>\n",
       "      <th>ave_recall</th>\n",
       "      <th>authentic_mislabeled</th>\n",
       "      <th>synthetic_mislabeled</th>\n",
       "      <th>top_10_features</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>['ALCOTT_synthetic_combined_133.txt', 'ALCOTT_...</td>\n",
       "      <td>0.665000</td>\n",
       "      <td>0.665000</td>\n",
       "      <td>0.665</td>\n",
       "      <td>['Twain_2136.txt', 'Alcott_602.txt', 'Dickens_...</td>\n",
       "      <td>['DICKENS_synthetic_combined_57.txt', 'HOPKINS...</td>\n",
       "      <td>{'mean_sen_len': 4.696841728642905, 'VADER_sen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>['ALCOTT_synthetic_combined_103.txt', 'ALCOTT_...</td>\n",
       "      <td>0.729973</td>\n",
       "      <td>0.730092</td>\n",
       "      <td>0.730</td>\n",
       "      <td>['Twain_314.txt', 'Bronte_893.txt', 'Dickens_8...</td>\n",
       "      <td>['GRIGGS_synthetic_combined_52.txt', 'TWAIN_sy...</td>\n",
       "      <td>{'mean_sen_len': 4.436319726279997, 'VADER_sen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>['ALCOTT_synthetic_combined_149.txt', 'ALCOTT_...</td>\n",
       "      <td>0.798593</td>\n",
       "      <td>0.813593</td>\n",
       "      <td>0.800</td>\n",
       "      <td>['Dickens_280.txt', 'Bronte_868.txt', 'Alcott_...</td>\n",
       "      <td>['BRONTE_synthetic_combined_167.txt', 'DICKENS...</td>\n",
       "      <td>{'mean_sen_len': 4.5814546493369726, 'VADER_se...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>['ALCOTT_synthetic_combined_145.txt', 'ALCOTT_...</td>\n",
       "      <td>0.689907</td>\n",
       "      <td>0.690366</td>\n",
       "      <td>0.690</td>\n",
       "      <td>['Austen_190.txt', 'Griggs_396.txt', 'Alcott_9...</td>\n",
       "      <td>['CHESNUTT_synthetic_combined_121.txt', 'CHESN...</td>\n",
       "      <td>{'mean_sen_len': 4.370718512007027, 'VADER_sen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>['ALCOTT_synthetic_combined_102.txt', 'ALCOTT_...</td>\n",
       "      <td>0.759952</td>\n",
       "      <td>0.760512</td>\n",
       "      <td>0.760</td>\n",
       "      <td>['Alcott_2204.txt', 'Twain_2513.txt', 'Dickens...</td>\n",
       "      <td>['GRIGGS_synthetic_combined_131.txt', 'DICKENS...</td>\n",
       "      <td>{'mean_sen_len': 3.934935121492579, 'VADER_sen...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               samples  accuracy(ave_F1)  \\\n",
       "96   ['ALCOTT_synthetic_combined_133.txt', 'ALCOTT_...          0.665000   \n",
       "97   ['ALCOTT_synthetic_combined_103.txt', 'ALCOTT_...          0.729973   \n",
       "98   ['ALCOTT_synthetic_combined_149.txt', 'ALCOTT_...          0.798593   \n",
       "99   ['ALCOTT_synthetic_combined_145.txt', 'ALCOTT_...          0.689907   \n",
       "100  ['ALCOTT_synthetic_combined_102.txt', 'ALCOTT_...          0.759952   \n",
       "\n",
       "     ave_precision  ave_recall  \\\n",
       "96        0.665000       0.665   \n",
       "97        0.730092       0.730   \n",
       "98        0.813593       0.800   \n",
       "99        0.690366       0.690   \n",
       "100       0.760512       0.760   \n",
       "\n",
       "                                  authentic_mislabeled  \\\n",
       "96   ['Twain_2136.txt', 'Alcott_602.txt', 'Dickens_...   \n",
       "97   ['Twain_314.txt', 'Bronte_893.txt', 'Dickens_8...   \n",
       "98   ['Dickens_280.txt', 'Bronte_868.txt', 'Alcott_...   \n",
       "99   ['Austen_190.txt', 'Griggs_396.txt', 'Alcott_9...   \n",
       "100  ['Alcott_2204.txt', 'Twain_2513.txt', 'Dickens...   \n",
       "\n",
       "                                  synthetic_mislabeled  \\\n",
       "96   ['DICKENS_synthetic_combined_57.txt', 'HOPKINS...   \n",
       "97   ['GRIGGS_synthetic_combined_52.txt', 'TWAIN_sy...   \n",
       "98   ['BRONTE_synthetic_combined_167.txt', 'DICKENS...   \n",
       "99   ['CHESNUTT_synthetic_combined_121.txt', 'CHESN...   \n",
       "100  ['GRIGGS_synthetic_combined_131.txt', 'DICKENS...   \n",
       "\n",
       "                                       top_10_features  \n",
       "96   {'mean_sen_len': 4.696841728642905, 'VADER_sen...  \n",
       "97   {'mean_sen_len': 4.436319726279997, 'VADER_sen...  \n",
       "98   {'mean_sen_len': 4.5814546493369726, 'VADER_se...  \n",
       "99   {'mean_sen_len': 4.370718512007027, 'VADER_sen...  \n",
       "100  {'mean_sen_len': 3.934935121492579, 'VADER_sen...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = pd.read_csv(\"NSC_data.csv\")\n",
    "a.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f95c0b50-c770-4808-b834-59c7a8ecf2aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Save the updated DataFrame back to the CSV\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
