{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae2e5ff8-e23e-42e2-b741-4c21df5426ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, classification_report\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c77fefe3-048b-4d2f-a779-9ca35a1223c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nation</th>\n",
       "      <th>gender</th>\n",
       "      <th>category</th>\n",
       "      <th>mean_sen_len</th>\n",
       "      <th>male_pronouns</th>\n",
       "      <th>female_pronouns</th>\n",
       "      <th>TTR</th>\n",
       "      <th>lex_density</th>\n",
       "      <th>VADER_sentiment</th>\n",
       "      <th>concreteness</th>\n",
       "      <th>...</th>\n",
       "      <th>again</th>\n",
       "      <th>other</th>\n",
       "      <th>must</th>\n",
       "      <th>after</th>\n",
       "      <th>go</th>\n",
       "      <th>might</th>\n",
       "      <th>too</th>\n",
       "      <th>through</th>\n",
       "      <th>himself</th>\n",
       "      <th>author</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Twain_2265.txt</th>\n",
       "      <td>American</td>\n",
       "      <td>male</td>\n",
       "      <td>authentic</td>\n",
       "      <td>11.409091</td>\n",
       "      <td>0.009554</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.522293</td>\n",
       "      <td>0.547771</td>\n",
       "      <td>0.9815</td>\n",
       "      <td>2.707865</td>\n",
       "      <td>...</td>\n",
       "      <td>0.006369</td>\n",
       "      <td>0.003185</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.006369</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006369</td>\n",
       "      <td>twain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GASKELL_synthetic_combined_140.txt</th>\n",
       "      <td>British/Irish</td>\n",
       "      <td>female</td>\n",
       "      <td>synthetic</td>\n",
       "      <td>27.777778</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.011111</td>\n",
       "      <td>0.573333</td>\n",
       "      <td>0.577778</td>\n",
       "      <td>0.9901</td>\n",
       "      <td>2.485116</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004444</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>gaskell</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GRIGGS_synthetic_combined_168.txt</th>\n",
       "      <td>American</td>\n",
       "      <td>male</td>\n",
       "      <td>synthetic</td>\n",
       "      <td>26.315789</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.542222</td>\n",
       "      <td>0.520000</td>\n",
       "      <td>-0.5383</td>\n",
       "      <td>2.447906</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002222</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002222</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>griggs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Austen_613.txt</th>\n",
       "      <td>British/Irish</td>\n",
       "      <td>female</td>\n",
       "      <td>authentic</td>\n",
       "      <td>33.400000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.057554</td>\n",
       "      <td>0.501199</td>\n",
       "      <td>0.568345</td>\n",
       "      <td>-0.9435</td>\n",
       "      <td>2.231135</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.009592</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002398</td>\n",
       "      <td>0.002398</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>austen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CHESNUTT_synthetic_combined_59.txt</th>\n",
       "      <td>American</td>\n",
       "      <td>male</td>\n",
       "      <td>synthetic</td>\n",
       "      <td>31.250000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.015217</td>\n",
       "      <td>0.530435</td>\n",
       "      <td>0.556522</td>\n",
       "      <td>0.9767</td>\n",
       "      <td>2.615499</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>chesnutt</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 211 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           nation  gender   category  \\\n",
       "ID                                                                     \n",
       "Twain_2265.txt                           American    male  authentic   \n",
       "GASKELL_synthetic_combined_140.txt  British/Irish  female  synthetic   \n",
       "GRIGGS_synthetic_combined_168.txt        American    male  synthetic   \n",
       "Austen_613.txt                      British/Irish  female  authentic   \n",
       "CHESNUTT_synthetic_combined_59.txt       American    male  synthetic   \n",
       "\n",
       "                                    mean_sen_len  male_pronouns  \\\n",
       "ID                                                                \n",
       "Twain_2265.txt                         11.409091       0.009554   \n",
       "GASKELL_synthetic_combined_140.txt     27.777778       0.000000   \n",
       "GRIGGS_synthetic_combined_168.txt      26.315789       0.000000   \n",
       "Austen_613.txt                         33.400000       0.000000   \n",
       "CHESNUTT_synthetic_combined_59.txt     31.250000       0.000000   \n",
       "\n",
       "                                    female_pronouns       TTR  lex_density  \\\n",
       "ID                                                                           \n",
       "Twain_2265.txt                             0.000000  0.522293     0.547771   \n",
       "GASKELL_synthetic_combined_140.txt         0.011111  0.573333     0.577778   \n",
       "GRIGGS_synthetic_combined_168.txt          0.000000  0.542222     0.520000   \n",
       "Austen_613.txt                             0.057554  0.501199     0.568345   \n",
       "CHESNUTT_synthetic_combined_59.txt         0.015217  0.530435     0.556522   \n",
       "\n",
       "                                    VADER_sentiment  concreteness  ...  \\\n",
       "ID                                                                 ...   \n",
       "Twain_2265.txt                               0.9815      2.707865  ...   \n",
       "GASKELL_synthetic_combined_140.txt           0.9901      2.485116  ...   \n",
       "GRIGGS_synthetic_combined_168.txt           -0.5383      2.447906  ...   \n",
       "Austen_613.txt                              -0.9435      2.231135  ...   \n",
       "CHESNUTT_synthetic_combined_59.txt           0.9767      2.615499  ...   \n",
       "\n",
       "                                       again     other      must  after  \\\n",
       "ID                                                                        \n",
       "Twain_2265.txt                      0.006369  0.003185  0.000000    0.0   \n",
       "GASKELL_synthetic_combined_140.txt  0.000000  0.000000  0.000000    0.0   \n",
       "GRIGGS_synthetic_combined_168.txt   0.000000  0.000000  0.002222    0.0   \n",
       "Austen_613.txt                      0.000000  0.000000  0.009592    0.0   \n",
       "CHESNUTT_synthetic_combined_59.txt  0.000000  0.000000  0.000000    0.0   \n",
       "\n",
       "                                          go     might       too   through  \\\n",
       "ID                                                                           \n",
       "Twain_2265.txt                      0.006369  0.000000  0.000000  0.000000   \n",
       "GASKELL_synthetic_combined_140.txt  0.000000  0.000000  0.000000  0.004444   \n",
       "GRIGGS_synthetic_combined_168.txt   0.000000  0.000000  0.000000  0.002222   \n",
       "Austen_613.txt                      0.000000  0.002398  0.002398  0.000000   \n",
       "CHESNUTT_synthetic_combined_59.txt  0.000000  0.000000  0.000000  0.000000   \n",
       "\n",
       "                                     himself    author  \n",
       "ID                                                      \n",
       "Twain_2265.txt                      0.006369     twain  \n",
       "GASKELL_synthetic_combined_140.txt  0.000000   gaskell  \n",
       "GRIGGS_synthetic_combined_168.txt   0.000000    griggs  \n",
       "Austen_613.txt                      0.000000    austen  \n",
       "CHESNUTT_synthetic_combined_59.txt  0.000000  chesnutt  \n",
       "\n",
       "[5 rows x 211 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Getting list of 20 different clusters of text\n",
    "\n",
    "import random\n",
    "\n",
    "df = pd.read_csv(\"master_stats.csv\")\n",
    "master_text_list = df['ID'].tolist()\n",
    "prefixes = [name.replace('.txt', '') for name in master_text_list]\n",
    "\n",
    "# Getting list of all 500 token chunk filenames\n",
    "\n",
    "chunks_dir = Path('corpus_chunks')\n",
    "\n",
    "chunk_names = [f for f in os.listdir(chunks_dir) if os.path.isfile(os.path.join(chunks_dir, f))]\n",
    "\n",
    "\n",
    "files_by_prefix = {prefix: [] for prefix in prefixes}\n",
    "\n",
    "# Iterate over all files in the directory and group them by prefix\n",
    "for file_path in chunks_dir.iterdir():\n",
    "    if file_path.is_file():\n",
    "        # Extract the prefix (the part before the last underscore)\n",
    "        file_name = file_path.name\n",
    "        parts = file_name.rsplit('_', 1)  # Split from the right at the last underscore\n",
    "        file_prefix = parts[0] if len(parts) > 1 else file_name  # Get the part before the last underscore\n",
    "\n",
    "        # If the prefix is one of the predefined prefixes, add the file to the list\n",
    "        if file_prefix in files_by_prefix:\n",
    "            files_by_prefix[file_prefix].append(file_path)\n",
    "\n",
    "## Initialize a list to store the sampled file names\n",
    "sampled_files_list = []\n",
    "\n",
    "# Randomly sample 100 files from each prefix group\n",
    "for prefix, files in files_by_prefix.items():\n",
    "    # Check if there are at least 100 files to sample\n",
    "    if len(files) > 100:\n",
    "        sampled = random.sample(files, 100)\n",
    "    else:\n",
    "        sampled = files  # If fewer than 100 files, sample all of them\n",
    "\n",
    "    # Add the filenames of the sampled files to the list\n",
    "    sampled_files_list.extend(file.name for file in sampled)\n",
    "\n",
    "\n",
    "chunks_df = pd.read_csv(\"master_features_chunks.csv\")\n",
    "chunks_df['author'] = chunks_df['ID'].apply(lambda x: x.split('_')[0].lower())\n",
    "\n",
    "sample_df = chunks_df.loc[chunks_df['ID'].isin(sampled_files_list)]\n",
    "sample_df = sample_df.set_index('ID')\n",
    "sample_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2f0d92b6-8356-4684-9c4b-c73771dac8d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 2000 entries, Twain_2265.txt to GASKELL_synthetic_combined_182.txt\n",
      "Columns: 211 entries, nation to author\n",
      "dtypes: float64(205), int64(2), object(4)\n",
      "memory usage: 3.2+ MB\n"
     ]
    }
   ],
   "source": [
    "sample_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "1f8e7978-edb9-4944-a01a-e0ca35d3d877",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.844\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.88      0.85       513\n",
      "           1       0.86      0.81      0.84       487\n",
      "\n",
      "    accuracy                           0.84      1000\n",
      "   macro avg       0.85      0.84      0.84      1000\n",
      "weighted avg       0.84      0.84      0.84      1000\n",
      "\n",
      "Weighted Precision: 0.8448574132466706\n",
      "Weighted Recall: 0.844\n",
      "Weighted F1 Score: 0.8437634381857497\n"
     ]
    }
   ],
   "source": [
    "X = sample_df.drop(columns=['nation', 'gender', 'category', 'author'])  # Drop the label column to get the features\n",
    "y = sample_df['category']  # Target variable\n",
    "\n",
    "# If 'category' is a string, convert it to numerical labels\n",
    "y, original_categories = pd.factorize(y)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=42)\n",
    "\n",
    "# Initialize the Logistic Regression model\n",
    "model = LogisticRegression(max_iter=1000, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy}')\n",
    "\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(y_test, y_pred, average='weighted')\n",
    "\n",
    "# Print the classification report\n",
    "print('Classification Report:')\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Save precision, recall, and f1 scores as variables\n",
    "print(f'Weighted Precision: {precision}')\n",
    "print(f'Weighted Recall: {recall}')\n",
    "print(f'Weighted F1 Score: {f1}')\n",
    "\n",
    "report = classification_report(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "4ab10e68-c757-44da-bd76-8f32b080c859",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([0, 0, 0, ..., 0, 1, 0]), array([0, 1]))\n"
     ]
    }
   ],
   "source": [
    "print(pd.factorize(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "917f5bf9-c692-4888-9c64-e871410ec24e",
   "metadata": {},
   "source": [
    "## 0 = Synthetic (larger negative co-efficients indicate likilhood of synthetic)\n",
    "## 1 = Authentic (larger positive co-efficients indicate liklihood of authentic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "a91c2566-84bf-4790-9e82-0fdc2a2dc0ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Incorrectly labeled samples (with ID, actual category, and predicted category):\n",
      "                                   actual_category predicted_category\n",
      "ID                                                                   \n",
      "Hopkins_81.txt                           authentic          synthetic\n",
      "Twain_2573.txt                           authentic          synthetic\n",
      "Austen_1105.txt                          authentic          synthetic\n",
      "Gaskell_644.txt                          authentic          synthetic\n",
      "Chesnutt_565.txt                         authentic          synthetic\n",
      "...                                            ...                ...\n",
      "TWAIN_synthetic_combined_69.txt          synthetic          authentic\n",
      "TWAIN_synthetic_combined_123.txt         synthetic          authentic\n",
      "TWAIN_synthetic_combined_146.txt         synthetic          authentic\n",
      "HOPKINS_synthetic_combined_73.txt        synthetic          authentic\n",
      "DICKENS_synthetic_combined_139.txt       synthetic          authentic\n",
      "\n",
      "[156 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# Convert y_test and y_pred back to the original string categories\n",
    "y_test_original = pd.Series(original_categories[y_test], index=X_test.index)\n",
    "y_pred_original = pd.Series(original_categories[y_pred], index=X_test.index)\n",
    "\n",
    "# Create a DataFrame with the actual and predicted categories\n",
    "y_test_df = pd.DataFrame({\n",
    "    'actual_category': y_test_original,\n",
    "    'predicted_category': y_pred_original\n",
    "})\n",
    "\n",
    "# Find the incorrectly labeled rows\n",
    "incorrectly_labeled = y_test_df[y_test_df['actual_category'] != y_test_df['predicted_category']]\n",
    "incorrectly_labeled = incorrectly_labeled.sort_values(\"actual_category\")\n",
    "\n",
    "# Print the incorrectly labeled rows with ID, actual category, and predicted category\n",
    "print(\"Incorrectly labeled samples (with ID, actual category, and predicted category):\")\n",
    "print(incorrectly_labeled)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "db3ba513-142d-4ed8-ad87-d7dd1e1db8b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 156 entries, Hopkins_81.txt to DICKENS_synthetic_combined_139.txt\n",
      "Data columns (total 2 columns):\n",
      " #   Column              Non-Null Count  Dtype \n",
      "---  ------              --------------  ----- \n",
      " 0   actual_category     156 non-null    object\n",
      " 1   predicted_category  156 non-null    object\n",
      "dtypes: object(2)\n",
      "memory usage: 3.7+ KB\n"
     ]
    }
   ],
   "source": [
    "incorrectly_labeled.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "63d104c7-0e0b-496f-81d4-bfdcf738b23e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hopkins_81.txt', 'Twain_2573.txt', 'Austen_1105.txt', 'Gaskell_644.txt', 'Chesnutt_565.txt', 'Austen_674.txt', 'Chesnutt_89.txt', 'Griggs_307.txt', 'Gaskell_2386.txt', 'Alcott_277.txt', 'Twain_1036.txt', 'Twain_1677.txt', 'Twain_1278.txt', 'Chesnutt_593.txt', 'Twain_562.txt', 'Alcott_1216.txt', 'Gaskell_688.txt', 'Austen_610.txt', 'Austen_986.txt', 'Bronte_647.txt', 'Alcott_439.txt', 'Gaskell_496.txt', 'Bronte_1017.txt', 'Alcott_2535.txt', 'Austen_830.txt', 'Austen_1088.txt', 'Griggs_215.txt', 'Alcott_1547.txt', 'Alcott_1757.txt', 'Alcott_2251.txt', 'Austen_413.txt', 'Alcott_457.txt', 'Stoker_790.txt', 'Gaskell_2245.txt', 'Bronte_676.txt', 'Chesnutt_365.txt', 'Twain_2525.txt', 'Griggs_236.txt', 'Alcott_1486.txt', 'Alcott_2253.txt', 'Griggs_92.txt', 'Dickens_2498.txt', 'Hopkins_97.txt', 'Austen_1524.txt', 'Dickens_1719.txt', 'Bronte_1472.txt', 'Twain_1668.txt', 'Alcott_2247.txt', 'Austen_727.txt', 'Twain_2422.txt', 'Austen_1572.txt', 'Gaskell_466.txt', 'Dickens_3519.txt', 'Alcott_1232.txt', 'Alcott_280.txt', 'Griggs_67.txt', 'Dickens_3041.txt', 'Dickens_3837.txt', 'Griggs_395.txt', 'Alcott_2269.txt', 'Gaskell_2099.txt', 'Chesnutt_206.txt', 'Dickens_3759.txt', 'Hopkins_235.txt', 'Alcott_2290.txt', 'Chesnutt_519.txt', 'Gaskell_478.txt', 'Dickens_2062.txt', 'Gaskell_877.txt', 'Bronte_1480.txt', 'Twain_512.txt', 'Gaskell_331.txt', 'Griggs_15.txt', 'Hopkins_170.txt', 'Alcott_86.txt', 'Dickens_4563.txt', 'Alcott_1993.txt', 'Austen_1505.txt', 'Twain_1662.txt', 'Twain_1993.txt', 'Austen_1072.txt', 'Stoker_887.txt', 'Chesnutt_308.txt', 'Dickens_6035.txt', 'Hopkins_236.txt', 'Austen_445.txt', 'Twain_999.txt', 'Chesnutt_611.txt', 'Alcott_921.txt', 'Stoker_1605.txt', 'Bronte_837.txt', 'Bronte_637.txt']\n",
      "['ALCOTT_synthetic_combined_61.txt', 'TWAIN_synthetic_combined_114.txt', 'GRIGGS_synthetic_combined_5.txt', 'BRONTE_synthetic_combined_159.txt', 'TWAIN_synthetic_combined_171.txt', 'AUSTEN_synthetic_combined_20.txt', 'GRIGGS_synthetic_combined_18.txt', 'GRIGGS_synthetic_combined_163.txt', 'TWAIN_synthetic_combined_21.txt', 'HOPKINS_synthetic_combined_22.txt', 'ALCOTT_synthetic_combined_30.txt', 'ALCOTT_synthetic_combined_155.txt', 'HOPKINS_synthetic_combined_19.txt', 'TWAIN_synthetic_combined_183.txt', 'ALCOTT_synthetic_combined_60.txt', 'TWAIN_synthetic_combined_192.txt', 'BRONTE_synthetic_combined_113.txt', 'AUSTEN_synthetic_combined_49.txt', 'TWAIN_synthetic_combined_163.txt', 'TWAIN_synthetic_combined_2.txt', 'TWAIN_synthetic_combined_23.txt', 'AUSTEN_synthetic_combined_165.txt', 'ALCOTT_synthetic_combined_11.txt', 'AUSTEN_synthetic_combined_23.txt', 'GASKELL_synthetic_combined_126.txt', 'TWAIN_synthetic_combined_134.txt', 'BRONTE_synthetic_combined_152.txt', 'TWAIN_synthetic_combined_44.txt', 'GRIGGS_synthetic_combined_64.txt', 'BRONTE_synthetic_combined_150.txt', 'TWAIN_synthetic_combined_81.txt', 'TWAIN_synthetic_combined_61.txt', 'GRIGGS_synthetic_combined_11.txt', 'AUSTEN_synthetic_combined_172.txt', 'BRONTE_synthetic_combined_96.txt', 'AUSTEN_synthetic_combined_58.txt', 'TWAIN_synthetic_combined_18.txt', 'TWAIN_synthetic_combined_16.txt', 'HOPKINS_synthetic_combined_67.txt', 'AUSTEN_synthetic_combined_25.txt', 'ALCOTT_synthetic_combined_44.txt', 'TWAIN_synthetic_combined_53.txt', 'GASKELL_synthetic_combined_135.txt', 'TWAIN_synthetic_combined_112.txt', 'TWAIN_synthetic_combined_135.txt', 'TWAIN_synthetic_combined_184.txt', 'AUSTEN_synthetic_combined_82.txt', 'ALCOTT_synthetic_combined_4.txt', 'TWAIN_synthetic_combined_107.txt', 'GRIGGS_synthetic_combined_159.txt', 'TWAIN_synthetic_combined_4.txt', 'TWAIN_synthetic_combined_166.txt', 'ALCOTT_synthetic_combined_48.txt', 'DICKENS_synthetic_combined_52.txt', 'BRONTE_synthetic_combined_137.txt', 'DICKENS_synthetic_combined_146.txt', 'TWAIN_synthetic_combined_182.txt', 'ALCOTT_synthetic_combined_10.txt', 'AUSTEN_synthetic_combined_13.txt', 'TWAIN_synthetic_combined_69.txt', 'TWAIN_synthetic_combined_123.txt', 'TWAIN_synthetic_combined_146.txt', 'HOPKINS_synthetic_combined_73.txt', 'DICKENS_synthetic_combined_139.txt']\n"
     ]
    }
   ],
   "source": [
    "authentic_mislabeled = []\n",
    "synthetic_mislabeled = []\n",
    "\n",
    "for index, row in incorrectly_labeled.iterrows():\n",
    "    if row[\"actual_category\"] == \"authentic\":\n",
    "        authentic_mislabeled.append(index)\n",
    "    else:\n",
    "        synthetic_mislabeled.append(index)\n",
    "\n",
    "print(authentic_mislabeled)\n",
    "print(synthetic_mislabeled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "0f5f3764-738f-4c9f-a747-5548a8964d2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Importance (sorted by absolute values):\n",
      "Postive values indicate positive correlation between feature and prediction, negative values a negative correlation. \n",
      "    feature  importance\n",
      "107     the   -5.577709\n",
      "109      of   -4.685133\n",
      "111       a   -2.900870\n",
      "110      to    2.503878\n",
      "112       i    2.010420\n",
      "..      ...         ...\n",
      "85     want    0.001875\n",
      "97   street   -0.001853\n",
      "49    woman   -0.000394\n",
      "149      's    0.000000\n",
      "179     n't    0.000000\n",
      "\n",
      "[207 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "feature_importance = pd.DataFrame({\n",
    "    'feature': X.columns,\n",
    "    'importance': model.coef_[0]  # For binary classification; use model.coef_ for multi-class\n",
    "})\n",
    "\n",
    "# Sort features by importance\n",
    "\n",
    "feature_importance = feature_importance.reindex(feature_importance['importance'].abs().sort_values(ascending=False).index)\n",
    "\n",
    "print(\"Feature Importance (sorted by absolute values):\")\n",
    "print(\"Postive values indicate positive correlation between feature and prediction, negative values a negative correlation. \")\n",
    "print(feature_importance)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "a57aa9bb-6de0-4244-aeb5-5b5d7abe3d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_10_dict = dict(list(zip(feature_importance['feature'], feature_importance['importance']))[:10])\n",
    "top_10_pos = (feature_importance[feature_importance['importance'] > 0].nlargest(10, 'importance').set_index('feature')['importance'].to_dict())\n",
    "top_10_neg = (feature_importance[feature_importance['importance'] < 0].nsmallest(10, 'importance').set_index('feature')['importance'].to_dict())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "25c8d4d2-b3d4-4be2-bb87-26441ba3a401",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'the': -5.577708606864234,\n",
       " 'of': -4.685133375653078,\n",
       " 'a': -2.900870165189069,\n",
       " 'her': -1.279184979307204,\n",
       " 'in': -1.2611651977069163,\n",
       " 'lex_density': -1.1810828331368088,\n",
       " 'TTR': -1.0615895885585094,\n",
       " 'with': -0.9621009202387746,\n",
       " 'concreteness': -0.8947561607370739,\n",
       " 'like': -0.7402523051266985}"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_10_neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "69b01763-2a93-421b-95d4-06d8965df99e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>samples</th>\n",
       "      <th>accuracy(ave_F1)</th>\n",
       "      <th>ave_precision</th>\n",
       "      <th>ave_recall</th>\n",
       "      <th>authentic_mislabeled</th>\n",
       "      <th>synthetic_mislabeled</th>\n",
       "      <th>top_10_features</th>\n",
       "      <th>top_10_pos_features</th>\n",
       "      <th>top_10_neg_features</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[ALCOTT_synthetic_combined_131.txt, ALCOTT_syn...</td>\n",
       "      <td>0.843763</td>\n",
       "      <td>0.844857</td>\n",
       "      <td>0.844</td>\n",
       "      <td>[Hopkins_81.txt, Twain_2573.txt, Austen_1105.t...</td>\n",
       "      <td>[ALCOTT_synthetic_combined_61.txt, TWAIN_synth...</td>\n",
       "      <td>{'the': -5.577708606864234, 'of': -4.685133375...</td>\n",
       "      <td>{'to': 2.503877735269993, 'i': 2.0104201259329...</td>\n",
       "      <td>{'the': -5.577708606864234, 'of': -4.685133375...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             samples accuracy(ave_F1)  \\\n",
       "0  [ALCOTT_synthetic_combined_131.txt, ALCOTT_syn...         0.843763   \n",
       "\n",
       "  ave_precision ave_recall                               authentic_mislabeled  \\\n",
       "0      0.844857      0.844  [Hopkins_81.txt, Twain_2573.txt, Austen_1105.t...   \n",
       "\n",
       "                                synthetic_mislabeled  \\\n",
       "0  [ALCOTT_synthetic_combined_61.txt, TWAIN_synth...   \n",
       "\n",
       "                                     top_10_features  \\\n",
       "0  {'the': -5.577708606864234, 'of': -4.685133375...   \n",
       "\n",
       "                                 top_10_pos_features  \\\n",
       "0  {'to': 2.503877735269993, 'i': 2.0104201259329...   \n",
       "\n",
       "                                 top_10_neg_features  \n",
       "0  {'the': -5.577708606864234, 'of': -4.685133375...  "
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df = pd.DataFrame(columns=['samples', 'accuracy(ave_F1)', 'ave_precision', 'ave_recall', 'authentic_mislabeled', 'synthetic_mislabeled', 'top_10_features', 'top_10_pos_features', 'top_10_neg_features'])\n",
    "\n",
    "\n",
    "results_df.at[0, 'samples'] = sampled_files_list\n",
    "results_df.at[0, 'accuracy(ave_F1)'] = f1\n",
    "results_df.at[0, 'ave_precision'] = precision\n",
    "results_df.at[0, 'ave_recall'] = recall\n",
    "results_df.at[0, 'authentic_mislabeled'] = authentic_mislabeled\n",
    "results_df.at[0, 'synthetic_mislabeled'] = synthetic_mislabeled\n",
    "results_df.at[0, 'top_10_features'] = top_10_dict\n",
    "results_df.at[0, 'top_10_pos_features'] = top_10_pos\n",
    "results_df.at[0, 'top_10_neg_features'] = top_10_neg\n",
    "\n",
    "results_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "534a3848-3d4d-40e3-9397-bb53f4d84b15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "10\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "print(len(results_df.loc[0,'top_10_features']))\n",
    "print(len(results_df.loc[0,'top_10_pos_features']))\n",
    "print(len(results_df.loc[0,'top_10_neg_features']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "b6d4577e-f6d0-4bd2-8911-31984c784ebb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'the': -5.577708606864234, 'of': -4.685133375653078, 'a': -2.900870165189069, 'to': 2.503877735269993, 'i': 2.0104201259329555, 'he': 1.6899772355675153, 'male_pronouns': 1.6899772355675153, 'it': 1.3596891282174441, 'her': -1.279184979307204, 'was': 1.2767693969622573}\n"
     ]
    }
   ],
   "source": [
    "print(results_df.loc[0,'top_10_features'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9966ee3f-1123-473e-ab55-ced7a550d096",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "8ca063f6-0acf-4d51-8913-6d032404050a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#results_df.to_csv(\"log_regression.csv\", index=False)\n",
    "results_df.to_csv('log_regression.csv', mode='a', header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "79c47cc0-5f2b-4523-baed-4ce142faed16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>samples</th>\n",
       "      <th>accuracy(ave_F1)</th>\n",
       "      <th>ave_precision</th>\n",
       "      <th>ave_recall</th>\n",
       "      <th>authentic_mislabeled</th>\n",
       "      <th>synthetic_mislabeled</th>\n",
       "      <th>top_10_features</th>\n",
       "      <th>top_10_pos_features</th>\n",
       "      <th>top_10_neg_features</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>['ALCOTT_synthetic_combined_34.txt', 'ALCOTT_s...</td>\n",
       "      <td>0.829459</td>\n",
       "      <td>0.832166</td>\n",
       "      <td>0.830</td>\n",
       "      <td>['Griggs_131.txt', 'Chesnutt_273.txt', 'Austen...</td>\n",
       "      <td>['ALCOTT_synthetic_combined_155.txt', 'AUSTEN_...</td>\n",
       "      <td>{'the': 5.5614976701593335, 'of': 4.4376037893...</td>\n",
       "      <td>{'the': 5.5614976701593335, 'of': 4.4376037893...</td>\n",
       "      <td>{'to': -2.3174725017573636, 'i': -1.9932841112...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>['ALCOTT_synthetic_combined_131.txt', 'ALCOTT_...</td>\n",
       "      <td>0.843763</td>\n",
       "      <td>0.844857</td>\n",
       "      <td>0.844</td>\n",
       "      <td>['Hopkins_81.txt', 'Twain_2573.txt', 'Austen_1...</td>\n",
       "      <td>['ALCOTT_synthetic_combined_61.txt', 'TWAIN_sy...</td>\n",
       "      <td>{'the': -5.577708606864234, 'of': -4.685133375...</td>\n",
       "      <td>{'to': 2.503877735269993, 'i': 2.0104201259329...</td>\n",
       "      <td>{'the': -5.577708606864234, 'of': -4.685133375...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             samples  accuracy(ave_F1)  \\\n",
       "0  ['ALCOTT_synthetic_combined_34.txt', 'ALCOTT_s...          0.829459   \n",
       "1  ['ALCOTT_synthetic_combined_131.txt', 'ALCOTT_...          0.843763   \n",
       "\n",
       "   ave_precision  ave_recall  \\\n",
       "0       0.832166       0.830   \n",
       "1       0.844857       0.844   \n",
       "\n",
       "                                authentic_mislabeled  \\\n",
       "0  ['Griggs_131.txt', 'Chesnutt_273.txt', 'Austen...   \n",
       "1  ['Hopkins_81.txt', 'Twain_2573.txt', 'Austen_1...   \n",
       "\n",
       "                                synthetic_mislabeled  \\\n",
       "0  ['ALCOTT_synthetic_combined_155.txt', 'AUSTEN_...   \n",
       "1  ['ALCOTT_synthetic_combined_61.txt', 'TWAIN_sy...   \n",
       "\n",
       "                                     top_10_features  \\\n",
       "0  {'the': 5.5614976701593335, 'of': 4.4376037893...   \n",
       "1  {'the': -5.577708606864234, 'of': -4.685133375...   \n",
       "\n",
       "                                 top_10_pos_features  \\\n",
       "0  {'the': 5.5614976701593335, 'of': 4.4376037893...   \n",
       "1  {'to': 2.503877735269993, 'i': 2.0104201259329...   \n",
       "\n",
       "                                 top_10_neg_features  \n",
       "0  {'to': -2.3174725017573636, 'i': -1.9932841112...  \n",
       "1  {'the': -5.577708606864234, 'of': -4.685133375...  "
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = pd.read_csv(\"log_regression.csv\")\n",
    "a.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "ad4abf1b-031b-4959-a902-8e8d3ecd917b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(a.loc[1,'top_10_pos_features'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "e9bb82ad-0e2e-4a81-98d7-ba3163621e36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'to': 2.503877735269993, 'i': 2.0104201259329555, 'he': 1.6899772355675153, 'male_pronouns': 1.6899772355675153, 'it': 1.3596891282174441, 'was': 1.2767693969622573, 'you': 1.1769141067446196, 'had': 1.07543058230499, 'not': 0.9568075606441202, 'have': 0.7723651466860788}\n"
     ]
    }
   ],
   "source": [
    "print(a.loc[1,'top_10_pos_features'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "6973e644-0a31-4872-a4f7-d1b49a48ed55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ast\n",
    "\n",
    "x = ast.literal_eval(a.loc[1, 'top_10_pos_features'])\n",
    "\n",
    "len(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e542e3ad-ff30-4808-aed9-acf2b9d48e9e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b4820c6-14cc-46e1-9636-e0a11c721b6f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "4e278ce3-7865-4863-b90f-5d2a57a468c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "10\n",
      "10\n",
      "{'the': -5.577708606864234, 'of': -4.685133375653078, 'a': -2.900870165189069, 'to': 2.503877735269993, 'i': 2.0104201259329555, 'he': 1.6899772355675153, 'male_pronouns': 1.6899772355675153, 'it': 1.3596891282174441, 'her': -1.279184979307204, 'was': 1.2767693969622573}\n"
     ]
    }
   ],
   "source": [
    "print(len(ast.literal_eval(a.loc[1,'top_10_features'])))\n",
    "print(len(ast.literal_eval(a.loc[1,'top_10_pos_features'])))\n",
    "print(len(ast.literal_eval(a.loc[1,'top_10_neg_features'])))\n",
    "print(a.loc[1,'top_10_features'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ada818bd-1a52-4377-bf88-8fc964ddc7a2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
