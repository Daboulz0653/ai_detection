{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9ea98d20-3dd1-425b-9f9d-0ded0d57d1e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "022f6714-fec3-4c1b-bfdb-56eb61ce85fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nation</th>\n",
       "      <th>gender</th>\n",
       "      <th>category</th>\n",
       "      <th>mean_sen_len</th>\n",
       "      <th>male_pronouns</th>\n",
       "      <th>female_pronouns</th>\n",
       "      <th>TTR</th>\n",
       "      <th>lex_density</th>\n",
       "      <th>VADER_sentiment</th>\n",
       "      <th>concreteness</th>\n",
       "      <th>...</th>\n",
       "      <th>again</th>\n",
       "      <th>other</th>\n",
       "      <th>must</th>\n",
       "      <th>after</th>\n",
       "      <th>go</th>\n",
       "      <th>might</th>\n",
       "      <th>too</th>\n",
       "      <th>through</th>\n",
       "      <th>himself</th>\n",
       "      <th>author</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Stoker_1737.txt</th>\n",
       "      <td>British/Irish</td>\n",
       "      <td>male</td>\n",
       "      <td>authentic</td>\n",
       "      <td>15.718750</td>\n",
       "      <td>0.007692</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.484615</td>\n",
       "      <td>0.546154</td>\n",
       "      <td>0.9476</td>\n",
       "      <td>2.620188</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007692</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002564</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>stoker</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DICKENS_synthetic_combined_144.txt</th>\n",
       "      <td>British/Irish</td>\n",
       "      <td>male</td>\n",
       "      <td>synthetic</td>\n",
       "      <td>27.777778</td>\n",
       "      <td>0.011111</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.520000</td>\n",
       "      <td>0.535556</td>\n",
       "      <td>-0.9794</td>\n",
       "      <td>2.552137</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.006667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>dickens</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GASKELL_synthetic_combined_140.txt</th>\n",
       "      <td>British/Irish</td>\n",
       "      <td>female</td>\n",
       "      <td>synthetic</td>\n",
       "      <td>27.777778</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.011111</td>\n",
       "      <td>0.573333</td>\n",
       "      <td>0.577778</td>\n",
       "      <td>0.9901</td>\n",
       "      <td>2.485116</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.004444</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>gaskell</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bronte_1160.txt</th>\n",
       "      <td>British/Irish</td>\n",
       "      <td>female</td>\n",
       "      <td>authentic</td>\n",
       "      <td>18.518519</td>\n",
       "      <td>0.038554</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.597590</td>\n",
       "      <td>0.583133</td>\n",
       "      <td>0.9686</td>\n",
       "      <td>2.488449</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.002410</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>bronte</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaskell_1892.txt</th>\n",
       "      <td>British/Irish</td>\n",
       "      <td>female</td>\n",
       "      <td>authentic</td>\n",
       "      <td>50.200000</td>\n",
       "      <td>0.012077</td>\n",
       "      <td>0.021739</td>\n",
       "      <td>0.507246</td>\n",
       "      <td>0.543478</td>\n",
       "      <td>0.9515</td>\n",
       "      <td>2.480466</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002415</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.002415</td>\n",
       "      <td>0.004831</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.004831</td>\n",
       "      <td>0.002415</td>\n",
       "      <td>gaskell</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 211 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           nation  gender   category  \\\n",
       "ID                                                                     \n",
       "Stoker_1737.txt                     British/Irish    male  authentic   \n",
       "DICKENS_synthetic_combined_144.txt  British/Irish    male  synthetic   \n",
       "GASKELL_synthetic_combined_140.txt  British/Irish  female  synthetic   \n",
       "Bronte_1160.txt                     British/Irish  female  authentic   \n",
       "Gaskell_1892.txt                    British/Irish  female  authentic   \n",
       "\n",
       "                                    mean_sen_len  male_pronouns  \\\n",
       "ID                                                                \n",
       "Stoker_1737.txt                        15.718750       0.007692   \n",
       "DICKENS_synthetic_combined_144.txt     27.777778       0.011111   \n",
       "GASKELL_synthetic_combined_140.txt     27.777778       0.000000   \n",
       "Bronte_1160.txt                        18.518519       0.038554   \n",
       "Gaskell_1892.txt                       50.200000       0.012077   \n",
       "\n",
       "                                    female_pronouns       TTR  lex_density  \\\n",
       "ID                                                                           \n",
       "Stoker_1737.txt                            0.000000  0.484615     0.546154   \n",
       "DICKENS_synthetic_combined_144.txt         0.000000  0.520000     0.535556   \n",
       "GASKELL_synthetic_combined_140.txt         0.011111  0.573333     0.577778   \n",
       "Bronte_1160.txt                            0.000000  0.597590     0.583133   \n",
       "Gaskell_1892.txt                           0.021739  0.507246     0.543478   \n",
       "\n",
       "                                    VADER_sentiment  concreteness  ...  \\\n",
       "ID                                                                 ...   \n",
       "Stoker_1737.txt                              0.9476      2.620188  ...   \n",
       "DICKENS_synthetic_combined_144.txt          -0.9794      2.552137  ...   \n",
       "GASKELL_synthetic_combined_140.txt           0.9901      2.485116  ...   \n",
       "Bronte_1160.txt                              0.9686      2.488449  ...   \n",
       "Gaskell_1892.txt                             0.9515      2.480466  ...   \n",
       "\n",
       "                                       again     other  must     after  \\\n",
       "ID                                                                       \n",
       "Stoker_1737.txt                     0.007692  0.000000   0.0  0.000000   \n",
       "DICKENS_synthetic_combined_144.txt  0.000000  0.000000   0.0  0.000000   \n",
       "GASKELL_synthetic_combined_140.txt  0.000000  0.000000   0.0  0.000000   \n",
       "Bronte_1160.txt                     0.000000  0.000000   0.0  0.000000   \n",
       "Gaskell_1892.txt                    0.000000  0.002415   0.0  0.002415   \n",
       "\n",
       "                                          go  might  too   through   himself  \\\n",
       "ID                                                                             \n",
       "Stoker_1737.txt                     0.002564    0.0  0.0  0.000000  0.000000   \n",
       "DICKENS_synthetic_combined_144.txt  0.000000    0.0  0.0  0.006667  0.000000   \n",
       "GASKELL_synthetic_combined_140.txt  0.000000    0.0  0.0  0.004444  0.000000   \n",
       "Bronte_1160.txt                     0.000000    0.0  0.0  0.002410  0.000000   \n",
       "Gaskell_1892.txt                    0.004831    0.0  0.0  0.004831  0.002415   \n",
       "\n",
       "                                     author  \n",
       "ID                                           \n",
       "Stoker_1737.txt                      stoker  \n",
       "DICKENS_synthetic_combined_144.txt  dickens  \n",
       "GASKELL_synthetic_combined_140.txt  gaskell  \n",
       "Bronte_1160.txt                      bronte  \n",
       "Gaskell_1892.txt                    gaskell  \n",
       "\n",
       "[5 rows x 211 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Getting list of 20 different clusters of text\n",
    "\n",
    "import random\n",
    "\n",
    "df = pd.read_csv(\"master_stats.csv\")\n",
    "master_text_list = df['ID'].tolist()\n",
    "prefixes = [name.replace('.txt', '') for name in master_text_list]\n",
    "\n",
    "# Getting list of all 500 token chunk filenames\n",
    "\n",
    "chunks_dir = Path('corpus_chunks')\n",
    "\n",
    "chunk_names = [f for f in os.listdir(chunks_dir) if os.path.isfile(os.path.join(chunks_dir, f))]\n",
    "\n",
    "\n",
    "files_by_prefix = {prefix: [] for prefix in prefixes}\n",
    "\n",
    "# Iterate over all files in the directory and group them by prefix\n",
    "for file_path in chunks_dir.iterdir():\n",
    "    if file_path.is_file():\n",
    "        # Extract the prefix (the part before the last underscore)\n",
    "        file_name = file_path.name\n",
    "        parts = file_name.rsplit('_', 1)  # Split from the right at the last underscore\n",
    "        file_prefix = parts[0] if len(parts) > 1 else file_name  # Get the part before the last underscore\n",
    "\n",
    "        # If the prefix is one of the predefined prefixes, add the file to the list\n",
    "        if file_prefix in files_by_prefix:\n",
    "            files_by_prefix[file_prefix].append(file_path)\n",
    "\n",
    "## Initialize a list to store the sampled file names\n",
    "sampled_files_list = []\n",
    "\n",
    "# Randomly sample 100 files from each prefix group\n",
    "for prefix, files in files_by_prefix.items():\n",
    "    # Check if there are at least 100 files to sample\n",
    "    if len(files) > 100:\n",
    "        sampled = random.sample(files, 100)\n",
    "    else:\n",
    "        sampled = files  # If fewer than 100 files, sample all of them\n",
    "\n",
    "    # Add the filenames of the sampled files to the list\n",
    "    sampled_files_list.extend(file.name for file in sampled)\n",
    "\n",
    "\n",
    "chunks_df = pd.read_csv(\"master_features_chunks.csv\")\n",
    "chunks_df['author'] = chunks_df['ID'].apply(lambda x: x.split('_')[0].lower())\n",
    "\n",
    "sample_df = chunks_df.loc[chunks_df['ID'].isin(sampled_files_list)]\n",
    "sample_df = sample_df.set_index('ID')\n",
    "sample_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aec458d2-5c61-4cb9-acb5-ce462c49d811",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 2000 entries, Stoker_1737.txt to CHESNUTT_synthetic_combined_180.txt\n",
      "Columns: 211 entries, nation to author\n",
      "dtypes: float64(205), int64(2), object(4)\n",
      "memory usage: 3.2+ MB\n"
     ]
    }
   ],
   "source": [
    "sample_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a56134d9-f534-478c-8074-e0bbd8dd108d",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(columns=['samples', 'accuracy', 'authentic_mislabeled', 'synthetic_mislabeled', 'top_10_features'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a19d5c9d-e5b9-4899-8bb2-c0fbf986f3a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "73142f3a-3c3c-4b18-ba6c-6b3929545600",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_cols = ['mean_sen_len',\n",
    "       'male_pronouns', 'female_pronouns', 'TTR', 'lex_density',\n",
    "       'VADER_sentiment', 'concreteness', 'said', 'mr', 'little', 'time',\n",
    "       'like', 'know', 'man', 'old', 'hand', 'come', 'miss', 'day',\n",
    "       'good', 'eye', 'thought', 'way', 'think', 'face', 'sir', 'great',\n",
    "       'came', 'thing', 'long', 'heart', 'away', 'young', 'went', 'look',\n",
    "       'word', 'lady', 'life', 'dear', 'head', 'room', 'house', 'looked',\n",
    "       'night', 'mind', 'shall', 'friend', 'tell', 'place', 'woman',\n",
    "       'child', 'took', 'door', 'let', 'found', 'mother', 'home', 'got',\n",
    "       'gentleman', 'father', 'saw', 'better', 'love', 'don', 'going',\n",
    "       'knew', 'boy', 'people', 'right', 'hope', 'moment', 'year',\n",
    "       'world', 'voice', 'left', 'poor', 'looking', 'asked', 'girl',\n",
    "       'felt', 'sat', 'new', 'air', 'oh', 'round', 'want', 'having',\n",
    "       'soon', 'heard', 'mean', 'stood', 'find', 'light', 'men', 'yes',\n",
    "       'told', 'hour', 'street', 'seen', 'sure', 'morning', 'returned',\n",
    "       'manner', 'cried', 'kind', 'replied', 'work', 'the', 'and', 'of',\n",
    "       'to', 'a', 'i', 'in', 'that', 'was', 'it', 'he', 'her', 'his',\n",
    "       'you', 'with', 'as', 'for', 'she', 'had', 'not', 'but', 'at', 'on',\n",
    "       'be', 'is', 'my', 'him', 'have', 'me', 'so', 'all', 'by', 'this',\n",
    "       'which', 'they', 'were', 'if', 'from', 'there', 'no', 'would',\n",
    "       'when', \"'s\", 'one', 'or', 'an', 'do', 'what', 'we', 'been',\n",
    "       'could', 'up', 'out', 'very', 'their', 'who', 'them', 'are', 'now',\n",
    "       'more', 'will', 'your', 'into', 'upon', 'then', 'some', 'did',\n",
    "       'any', 'about', 'than', 'can', 'down', \"n't\", 'much', 'such',\n",
    "       'see', 'before', 'never', 'where', 'well', 'over', 'how', 'am',\n",
    "       'only', 'should', 'made', 'say', 'its', 'has', 'own', 'here',\n",
    "       'again', 'other', 'must', 'after', 'go', 'might', 'too', 'through',\n",
    "       'himself']\n",
    "X = sample_df[feature_cols] # Features\n",
    "y = sample_df.category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "acd591b8-c017-41e5-9a8b-7acfa8e641e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the Random Forest model\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "317d654c-ad2f-4470-9138-2a2983a6417b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9975\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d445bab6-cfca-4ae0-949b-d3407b79362d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   authentic       1.00      1.00      1.00       204\n",
      "   synthetic       1.00      0.99      1.00       196\n",
      "\n",
      "    accuracy                           1.00       400\n",
      "   macro avg       1.00      1.00      1.00       400\n",
      "weighted avg       1.00      1.00      1.00       400\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Classification Report:')\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d3e9c915-2fc4-4c8c-8734-19cdc4c632ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Incorrectly labeled samples (correct/predicted):\n",
      "                                  actual_category predicted_category\n",
      "ID                                                                  \n",
      "AUSTEN_synthetic_combined_160.txt       synthetic          authentic\n"
     ]
    }
   ],
   "source": [
    "incorrect_indices = y_test != y_pred\n",
    "\n",
    "# Use these indices to select incorrect rows\n",
    "incorrectly_labeled = X_test[incorrect_indices].copy()\n",
    "\n",
    "# Add actual and predicted categories\n",
    "incorrectly_labeled['actual_category'] = y_test[incorrect_indices]\n",
    "incorrectly_labeled['predicted_category'] = y_pred[incorrect_indices]\n",
    "\n",
    "# Print the incorrectly labeled rows sorted by actual category\n",
    "print(\"Incorrectly labeled samples (correct/predicted):\")\n",
    "print(incorrectly_labeled[['actual_category', 'predicted_category']].sort_values('actual_category'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "831c99b0-3d1b-4264-894b-f3c5775675d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "['AUSTEN_synthetic_combined_160.txt']\n"
     ]
    }
   ],
   "source": [
    "authentic_mislabeled = []\n",
    "synthetic_mislabeled = []\n",
    "\n",
    "for index, row in incorrectly_labeled.iterrows():\n",
    "    if row[\"actual_category\"] == \"authentic\":\n",
    "        authentic_mislabeled.append(index)\n",
    "    else:\n",
    "        synthetic_mislabeled.append(index)\n",
    "\n",
    "print(authentic_mislabeled)\n",
    "print(synthetic_mislabeled)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "994d11b2-f60e-4167-83e4-3fbd67f32481",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Importance:\n",
      "    feature  importance\n",
      "109      of    0.082202\n",
      "140   which    0.073789\n",
      "126     not    0.066537\n",
      "111       a    0.053919\n",
      "134    have    0.047496\n",
      "..      ...         ...\n",
      "52     door    0.000044\n",
      "71     year    0.000032\n",
      "149      's    0.000000\n",
      "179     n't    0.000000\n",
      "89     mean    0.000000\n",
      "\n",
      "[207 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "feature_importance = pd.DataFrame({\n",
    "    'feature': X.columns,\n",
    "    'importance': model.feature_importances_\n",
    "})\n",
    "\n",
    "# Sort features by importance\n",
    "feature_importance = feature_importance.sort_values(by='importance', ascending=False)\n",
    "\n",
    "# Print the feature importance\n",
    "print(\"Feature Importance:\")\n",
    "print(feature_importance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "68b3f360-485d-49bf-8c73-1c426ba1a220",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'of': 0.08220234768085538, 'which': 0.07378894328056752, 'not': 0.06653713966394073, 'a': 0.053918695462191454, 'have': 0.04749570294360368, 'said': 0.045701326461201994, 'the': 0.03955301960829261, 'to': 0.02775982570686983, 'mean_sen_len': 0.02353624917801923, 'had': 0.0228034582600506}\n"
     ]
    }
   ],
   "source": [
    "top_10_df = feature_importance[:10]\n",
    "top_10_df.head(10)\n",
    "\n",
    "top_10_features = dict(zip(top_10_df['feature'], top_10_df['importance']))\n",
    "\n",
    "print(top_10_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1cc7b0d7-85b2-462d-b5c9-93236aa60c18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>samples</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>authentic_mislabeled</th>\n",
       "      <th>synthetic_mislabeled</th>\n",
       "      <th>top_10_features</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[ALCOTT_synthetic_combined_64.txt, ALCOTT_synt...</td>\n",
       "      <td>0.9975</td>\n",
       "      <td>[]</td>\n",
       "      <td>[AUSTEN_synthetic_combined_160.txt]</td>\n",
       "      <td>{'of': 0.08220234768085538, 'which': 0.0737889...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             samples accuracy  \\\n",
       "0  [ALCOTT_synthetic_combined_64.txt, ALCOTT_synt...   0.9975   \n",
       "\n",
       "  authentic_mislabeled                 synthetic_mislabeled  \\\n",
       "0                   []  [AUSTEN_synthetic_combined_160.txt]   \n",
       "\n",
       "                                     top_10_features  \n",
       "0  {'of': 0.08220234768085538, 'which': 0.0737889...  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Printing results to csv\n",
    "\n",
    "results_df.at[0, 'samples'] = sampled_files_list\n",
    "results_df.at[0, 'accuracy'] = accuracy\n",
    "results_df.at[0, 'authentic_mislabeled'] = authentic_mislabeled\n",
    "results_df.at[0, 'synthetic_mislabeled'] = synthetic_mislabeled\n",
    "results_df.at[0, 'top_10_features'] = top_10_features\n",
    "\n",
    "results_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "43415f83-9834-49a6-a1fc-181cb0631a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#results_df.to_csv('random_forest_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f7a979ac-0d0c-4c9a-bae8-b9ead555cc79",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.to_csv('random_forest_data.csv', mode='a', header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d12d71cd-f497-4884-a8cb-044312735282",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>samples</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>authentic_mislabeled</th>\n",
       "      <th>synthetic_mislabeled</th>\n",
       "      <th>top_10_features</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>['ALCOTT_synthetic_combined_64.txt', 'ALCOTT_s...</td>\n",
       "      <td>0.9975</td>\n",
       "      <td>[]</td>\n",
       "      <td>['AUSTEN_synthetic_combined_160.txt']</td>\n",
       "      <td>{'of': 0.08220234768085538, 'which': 0.0737889...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             samples  accuracy  \\\n",
       "0  ['ALCOTT_synthetic_combined_64.txt', 'ALCOTT_s...    0.9975   \n",
       "\n",
       "  authentic_mislabeled                   synthetic_mislabeled  \\\n",
       "0                   []  ['AUSTEN_synthetic_combined_160.txt']   \n",
       "\n",
       "                                     top_10_features  \n",
       "0  {'of': 0.08220234768085538, 'which': 0.0737889...  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = pd.read_csv(\"random_forest_data.csv\")\n",
    "a.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bbc7f65-f152-482c-a467-bf3c8f9c3bee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
