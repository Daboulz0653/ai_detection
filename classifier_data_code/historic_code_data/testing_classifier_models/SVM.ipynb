{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea52ba10-fb2b-4843-a8ca-6f343be5f1ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7e3c5897-049c-443f-b331-cb6ffe22a7a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nation</th>\n",
       "      <th>gender</th>\n",
       "      <th>category</th>\n",
       "      <th>mean_sen_len</th>\n",
       "      <th>male_pronouns</th>\n",
       "      <th>female_pronouns</th>\n",
       "      <th>TTR</th>\n",
       "      <th>lex_density</th>\n",
       "      <th>VADER_sentiment</th>\n",
       "      <th>concreteness</th>\n",
       "      <th>...</th>\n",
       "      <th>again</th>\n",
       "      <th>other</th>\n",
       "      <th>must</th>\n",
       "      <th>after</th>\n",
       "      <th>go</th>\n",
       "      <th>might</th>\n",
       "      <th>too</th>\n",
       "      <th>through</th>\n",
       "      <th>himself</th>\n",
       "      <th>author</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Chesnutt_401.txt</th>\n",
       "      <td>American</td>\n",
       "      <td>male</td>\n",
       "      <td>authentic</td>\n",
       "      <td>20.958333</td>\n",
       "      <td>0.018470</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.583113</td>\n",
       "      <td>0.635884</td>\n",
       "      <td>-0.9533</td>\n",
       "      <td>2.747888</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002639</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002639</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>chesnutt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Austen_952.txt</th>\n",
       "      <td>British/Irish</td>\n",
       "      <td>female</td>\n",
       "      <td>authentic</td>\n",
       "      <td>20.320000</td>\n",
       "      <td>0.012920</td>\n",
       "      <td>0.007752</td>\n",
       "      <td>0.503876</td>\n",
       "      <td>0.573643</td>\n",
       "      <td>0.9944</td>\n",
       "      <td>2.461802</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.005168</td>\n",
       "      <td>0.007752</td>\n",
       "      <td>0.005168</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005168</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>austen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Stoker_555.txt</th>\n",
       "      <td>British/Irish</td>\n",
       "      <td>male</td>\n",
       "      <td>authentic</td>\n",
       "      <td>19.230769</td>\n",
       "      <td>0.004902</td>\n",
       "      <td>0.004902</td>\n",
       "      <td>0.512255</td>\n",
       "      <td>0.566176</td>\n",
       "      <td>0.9918</td>\n",
       "      <td>2.458560</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002451</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.009804</td>\n",
       "      <td>0.007353</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>stoker</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GRIGGS_synthetic_combined_168.txt</th>\n",
       "      <td>American</td>\n",
       "      <td>male</td>\n",
       "      <td>synthetic</td>\n",
       "      <td>26.315789</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.542222</td>\n",
       "      <td>0.520000</td>\n",
       "      <td>-0.5383</td>\n",
       "      <td>2.447906</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002222</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002222</td>\n",
       "      <td>0.0</td>\n",
       "      <td>griggs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BRONTE_synthetic_combined_58.txt</th>\n",
       "      <td>British/Irish</td>\n",
       "      <td>female</td>\n",
       "      <td>synthetic</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.015945</td>\n",
       "      <td>0.537585</td>\n",
       "      <td>0.580866</td>\n",
       "      <td>-0.9674</td>\n",
       "      <td>2.567873</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006834</td>\n",
       "      <td>0.0</td>\n",
       "      <td>bronte</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 211 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          nation  gender   category  \\\n",
       "ID                                                                    \n",
       "Chesnutt_401.txt                        American    male  authentic   \n",
       "Austen_952.txt                     British/Irish  female  authentic   \n",
       "Stoker_555.txt                     British/Irish    male  authentic   \n",
       "GRIGGS_synthetic_combined_168.txt       American    male  synthetic   \n",
       "BRONTE_synthetic_combined_58.txt   British/Irish  female  synthetic   \n",
       "\n",
       "                                   mean_sen_len  male_pronouns  \\\n",
       "ID                                                               \n",
       "Chesnutt_401.txt                      20.958333       0.018470   \n",
       "Austen_952.txt                        20.320000       0.012920   \n",
       "Stoker_555.txt                        19.230769       0.004902   \n",
       "GRIGGS_synthetic_combined_168.txt     26.315789       0.000000   \n",
       "BRONTE_synthetic_combined_58.txt      25.000000       0.000000   \n",
       "\n",
       "                                   female_pronouns       TTR  lex_density  \\\n",
       "ID                                                                          \n",
       "Chesnutt_401.txt                          0.000000  0.583113     0.635884   \n",
       "Austen_952.txt                            0.007752  0.503876     0.573643   \n",
       "Stoker_555.txt                            0.004902  0.512255     0.566176   \n",
       "GRIGGS_synthetic_combined_168.txt         0.000000  0.542222     0.520000   \n",
       "BRONTE_synthetic_combined_58.txt          0.015945  0.537585     0.580866   \n",
       "\n",
       "                                   VADER_sentiment  concreteness  ...  again  \\\n",
       "ID                                                                ...          \n",
       "Chesnutt_401.txt                           -0.9533      2.747888  ...    0.0   \n",
       "Austen_952.txt                              0.9944      2.461802  ...    0.0   \n",
       "Stoker_555.txt                              0.9918      2.458560  ...    0.0   \n",
       "GRIGGS_synthetic_combined_168.txt          -0.5383      2.447906  ...    0.0   \n",
       "BRONTE_synthetic_combined_58.txt           -0.9674      2.567873  ...    0.0   \n",
       "\n",
       "                                      other      must     after        go  \\\n",
       "ID                                                                          \n",
       "Chesnutt_401.txt                   0.000000  0.000000  0.000000  0.002639   \n",
       "Austen_952.txt                     0.005168  0.007752  0.005168  0.000000   \n",
       "Stoker_555.txt                     0.000000  0.002451  0.000000  0.000000   \n",
       "GRIGGS_synthetic_combined_168.txt  0.000000  0.002222  0.000000  0.000000   \n",
       "BRONTE_synthetic_combined_58.txt   0.000000  0.000000  0.000000  0.000000   \n",
       "\n",
       "                                      might       too   through  himself  \\\n",
       "ID                                                                         \n",
       "Chesnutt_401.txt                   0.000000  0.002639  0.000000      0.0   \n",
       "Austen_952.txt                     0.000000  0.005168  0.000000      0.0   \n",
       "Stoker_555.txt                     0.009804  0.007353  0.000000      0.0   \n",
       "GRIGGS_synthetic_combined_168.txt  0.000000  0.000000  0.002222      0.0   \n",
       "BRONTE_synthetic_combined_58.txt   0.000000  0.000000  0.006834      0.0   \n",
       "\n",
       "                                     author  \n",
       "ID                                           \n",
       "Chesnutt_401.txt                   chesnutt  \n",
       "Austen_952.txt                       austen  \n",
       "Stoker_555.txt                       stoker  \n",
       "GRIGGS_synthetic_combined_168.txt    griggs  \n",
       "BRONTE_synthetic_combined_58.txt     bronte  \n",
       "\n",
       "[5 rows x 211 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Getting list of 20 different clusters of text\n",
    "\n",
    "import random\n",
    "\n",
    "df = pd.read_csv(\"master_stats.csv\")\n",
    "master_text_list = df['ID'].tolist()\n",
    "prefixes = [name.replace('.txt', '') for name in master_text_list]\n",
    "\n",
    "# Getting list of all 500 token chunk filenames\n",
    "\n",
    "chunks_dir = Path('corpus_chunks')\n",
    "\n",
    "chunk_names = [f for f in os.listdir(chunks_dir) if os.path.isfile(os.path.join(chunks_dir, f))]\n",
    "\n",
    "\n",
    "files_by_prefix = {prefix: [] for prefix in prefixes}\n",
    "\n",
    "# Iterate over all files in the directory and group them by prefix\n",
    "for file_path in chunks_dir.iterdir():\n",
    "    if file_path.is_file():\n",
    "        # Extract the prefix (the part before the last underscore)\n",
    "        file_name = file_path.name\n",
    "        parts = file_name.rsplit('_', 1)  # Split from the right at the last underscore\n",
    "        file_prefix = parts[0] if len(parts) > 1 else file_name  # Get the part before the last underscore\n",
    "\n",
    "        # If the prefix is one of the predefined prefixes, add the file to the list\n",
    "        if file_prefix in files_by_prefix:\n",
    "            files_by_prefix[file_prefix].append(file_path)\n",
    "\n",
    "## Initialize a list to store the sampled file names\n",
    "sampled_files_list = []\n",
    "\n",
    "# Randomly sample 100 files from each prefix group\n",
    "for prefix, files in files_by_prefix.items():\n",
    "    # Check if there are at least 100 files to sample\n",
    "    if len(files) > 100:\n",
    "        sampled = random.sample(files, 100)\n",
    "    else:\n",
    "        sampled = files  # If fewer than 100 files, sample all of them\n",
    "\n",
    "    # Add the filenames of the sampled files to the list\n",
    "    sampled_files_list.extend(file.name for file in sampled)\n",
    "\n",
    "\n",
    "chunks_df = pd.read_csv(\"master_features_chunks.csv\")\n",
    "chunks_df['author'] = chunks_df['ID'].apply(lambda x: x.split('_')[0].lower())\n",
    "\n",
    "sample_df = chunks_df.loc[chunks_df['ID'].isin(sampled_files_list)]\n",
    "sample_df = sample_df.set_index('ID')\n",
    "sample_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b2ea6470-f8d4-4f7e-8a2a-6413b217eea8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 2000 entries, Chesnutt_401.txt to ALCOTT_synthetic_combined_192.txt\n",
      "Columns: 211 entries, nation to author\n",
      "dtypes: float64(205), int64(2), object(4)\n",
      "memory usage: 3.2+ MB\n"
     ]
    }
   ],
   "source": [
    "sample_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "adc3a485-a1fc-4a49-99f2-ca6b14edaa0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(columns=['samples', 'accuracy', 'authentic_mislabeled', 'synthetic_mislabeled', 'top_10_features'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "00f7a815-df3c-4779-a7fb-40a69d829017",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_cols = ['mean_sen_len',\n",
    "       'male_pronouns', 'female_pronouns', 'TTR', 'lex_density',\n",
    "       'VADER_sentiment', 'concreteness', 'said', 'mr', 'little', 'time',\n",
    "       'like', 'know', 'man', 'old', 'hand', 'come', 'miss', 'day',\n",
    "       'good', 'eye', 'thought', 'way', 'think', 'face', 'sir', 'great',\n",
    "       'came', 'thing', 'long', 'heart', 'away', 'young', 'went', 'look',\n",
    "       'word', 'lady', 'life', 'dear', 'head', 'room', 'house', 'looked',\n",
    "       'night', 'mind', 'shall', 'friend', 'tell', 'place', 'woman',\n",
    "       'child', 'took', 'door', 'let', 'found', 'mother', 'home', 'got',\n",
    "       'gentleman', 'father', 'saw', 'better', 'love', 'don', 'going',\n",
    "       'knew', 'boy', 'people', 'right', 'hope', 'moment', 'year',\n",
    "       'world', 'voice', 'left', 'poor', 'looking', 'asked', 'girl',\n",
    "       'felt', 'sat', 'new', 'air', 'oh', 'round', 'want', 'having',\n",
    "       'soon', 'heard', 'mean', 'stood', 'find', 'light', 'men', 'yes',\n",
    "       'told', 'hour', 'street', 'seen', 'sure', 'morning', 'returned',\n",
    "       'manner', 'cried', 'kind', 'replied', 'work', 'the', 'and', 'of',\n",
    "       'to', 'a', 'i', 'in', 'that', 'was', 'it', 'he', 'her', 'his',\n",
    "       'you', 'with', 'as', 'for', 'she', 'had', 'not', 'but', 'at', 'on',\n",
    "       'be', 'is', 'my', 'him', 'have', 'me', 'so', 'all', 'by', 'this',\n",
    "       'which', 'they', 'were', 'if', 'from', 'there', 'no', 'would',\n",
    "       'when', \"'s\", 'one', 'or', 'an', 'do', 'what', 'we', 'been',\n",
    "       'could', 'up', 'out', 'very', 'their', 'who', 'them', 'are', 'now',\n",
    "       'more', 'will', 'your', 'into', 'upon', 'then', 'some', 'did',\n",
    "       'any', 'about', 'than', 'can', 'down', \"n't\", 'much', 'such',\n",
    "       'see', 'before', 'never', 'where', 'well', 'over', 'how', 'am',\n",
    "       'only', 'should', 'made', 'say', 'its', 'has', 'own', 'here',\n",
    "       'again', 'other', 'must', 'after', 'go', 'might', 'too', 'through',\n",
    "       'himself']\n",
    "X = sample_df[feature_cols] # Features\n",
    "y = sample_df.category\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "model = svm.SVC(kernel='linear')\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "99a448da-0768-4746-a330-d5af7a09938f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.87\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "022849ba-2469-460d-b20d-c973a5c4a083",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   authentic       0.96      0.79      0.86       211\n",
      "   synthetic       0.80      0.96      0.88       189\n",
      "\n",
      "    accuracy                           0.87       400\n",
      "   macro avg       0.88      0.87      0.87       400\n",
      "weighted avg       0.88      0.87      0.87       400\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Classification Report:')\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5fbd8185-4e0f-4055-8276-7a2ff25bc67f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Incorrectly labeled samples (correct/predicted):\n",
      "                                 actual_category predicted_category\n",
      "ID                                                                 \n",
      "Griggs_503.txt                         authentic          synthetic\n",
      "Bronte_729.txt                         authentic          synthetic\n",
      "Austen_734.txt                         authentic          synthetic\n",
      "Hopkins_35.txt                         authentic          synthetic\n",
      "Austen_144.txt                         authentic          synthetic\n",
      "Twain_1935.txt                         authentic          synthetic\n",
      "Griggs_395.txt                         authentic          synthetic\n",
      "Gaskell_2166.txt                       authentic          synthetic\n",
      "Chesnutt_592.txt                       authentic          synthetic\n",
      "Twain_2089.txt                         authentic          synthetic\n",
      "Gaskell_2823.txt                       authentic          synthetic\n",
      "Chesnutt_8.txt                         authentic          synthetic\n",
      "Alcott_2714.txt                        authentic          synthetic\n",
      "Twain_2739.txt                         authentic          synthetic\n",
      "Alcott_205.txt                         authentic          synthetic\n",
      "Gaskell_351.txt                        authentic          synthetic\n",
      "Austen_552.txt                         authentic          synthetic\n",
      "Bronte_1233.txt                        authentic          synthetic\n",
      "Twain_316.txt                          authentic          synthetic\n",
      "Gaskell_92.txt                         authentic          synthetic\n",
      "Alcott_1961.txt                        authentic          synthetic\n",
      "Stoker_805.txt                         authentic          synthetic\n",
      "Bronte_737.txt                         authentic          synthetic\n",
      "Gaskell_619.txt                        authentic          synthetic\n",
      "Chesnutt_288.txt                       authentic          synthetic\n",
      "Austen_357.txt                         authentic          synthetic\n",
      "Stoker_1585.txt                        authentic          synthetic\n",
      "Dickens_3308.txt                       authentic          synthetic\n",
      "Alcott_2505.txt                        authentic          synthetic\n",
      "Alcott_1543.txt                        authentic          synthetic\n",
      "Dickens_2498.txt                       authentic          synthetic\n",
      "Stoker_188.txt                         authentic          synthetic\n",
      "Gaskell_2251.txt                       authentic          synthetic\n",
      "Gaskell_159.txt                        authentic          synthetic\n",
      "Gaskell_37.txt                         authentic          synthetic\n",
      "Dickens_7913.txt                       authentic          synthetic\n",
      "Chesnutt_631.txt                       authentic          synthetic\n",
      "Griggs_500.txt                         authentic          synthetic\n",
      "Alcott_863.txt                         authentic          synthetic\n",
      "Griggs_92.txt                          authentic          synthetic\n",
      "Bronte_666.txt                         authentic          synthetic\n",
      "Bronte_594.txt                         authentic          synthetic\n",
      "Bronte_969.txt                         authentic          synthetic\n",
      "Chesnutt_397.txt                       authentic          synthetic\n",
      "Dickens_7168.txt                       authentic          synthetic\n",
      "TWAIN_synthetic_combined_18.txt        synthetic          authentic\n",
      "TWAIN_synthetic_combined_17.txt        synthetic          authentic\n",
      "TWAIN_synthetic_combined_39.txt        synthetic          authentic\n",
      "TWAIN_synthetic_combined_190.txt       synthetic          authentic\n",
      "AUSTEN_synthetic_combined_13.txt       synthetic          authentic\n",
      "ALCOTT_synthetic_combined_8.txt        synthetic          authentic\n",
      "TWAIN_synthetic_combined_28.txt        synthetic          authentic\n"
     ]
    }
   ],
   "source": [
    "incorrect_indices = y_test != y_pred\n",
    "\n",
    "# Use these indices to select incorrect rows\n",
    "incorrectly_labeled = X_test[incorrect_indices].copy()\n",
    "\n",
    "# Add actual and predicted categories\n",
    "incorrectly_labeled['actual_category'] = y_test[incorrect_indices]\n",
    "incorrectly_labeled['predicted_category'] = y_pred[incorrect_indices]\n",
    "\n",
    "# Print the incorrectly labeled rows sorted by actual category\n",
    "print(\"Incorrectly labeled samples (correct/predicted):\")\n",
    "print(incorrectly_labeled[['actual_category', 'predicted_category']].sort_values('actual_category'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a77bb9b8-4399-483f-b636-8e9c348199f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Stoker_1585.txt', 'Dickens_3308.txt', 'Alcott_2505.txt', 'Alcott_1543.txt', 'Dickens_2498.txt', 'Stoker_188.txt', 'Gaskell_159.txt', 'Chesnutt_397.txt', 'Gaskell_37.txt', 'Dickens_7913.txt', 'Chesnutt_631.txt', 'Griggs_500.txt', 'Alcott_863.txt', 'Griggs_92.txt', 'Bronte_666.txt', 'Bronte_594.txt', 'Bronte_969.txt', 'Gaskell_619.txt', 'Gaskell_2251.txt', 'Bronte_737.txt', 'Alcott_1961.txt', 'Bronte_729.txt', 'Hopkins_35.txt', 'Austen_144.txt', 'Twain_1935.txt', 'Griggs_395.txt', 'Chesnutt_592.txt', 'Twain_2089.txt', 'Gaskell_2823.txt', 'Chesnutt_8.txt', 'Alcott_2714.txt', 'Twain_2739.txt', 'Alcott_205.txt', 'Gaskell_351.txt', 'Austen_552.txt', 'Bronte_1233.txt', 'Twain_316.txt', 'Gaskell_92.txt', 'Stoker_805.txt', 'Dickens_7168.txt', 'Gaskell_2166.txt', 'Austen_357.txt', 'Chesnutt_288.txt', 'Austen_734.txt', 'Griggs_503.txt']\n",
      "['TWAIN_synthetic_combined_17.txt', 'AUSTEN_synthetic_combined_13.txt', 'TWAIN_synthetic_combined_190.txt', 'TWAIN_synthetic_combined_28.txt', 'TWAIN_synthetic_combined_39.txt', 'ALCOTT_synthetic_combined_8.txt', 'TWAIN_synthetic_combined_18.txt']\n"
     ]
    }
   ],
   "source": [
    "authentic_mislabeled = []\n",
    "synthetic_mislabeled = []\n",
    "\n",
    "for index, row in incorrectly_labeled.iterrows():\n",
    "    if row[\"actual_category\"] == \"authentic\":\n",
    "        authentic_mislabeled.append(index)\n",
    "    else:\n",
    "        synthetic_mislabeled.append(index)\n",
    "\n",
    "print(authentic_mislabeled)\n",
    "print(synthetic_mislabeled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "74d8b086-6311-44a2-b905-cfc8475b9817",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Feature  Importance\n",
      "107     the   10.082099\n",
      "109      of    8.639379\n",
      "111       a    7.065955\n",
      "110      to    4.876597\n",
      "112       i    4.086469\n",
      "..      ...         ...\n",
      "85     want    0.005662\n",
      "97   street    0.002887\n",
      "18      day    0.001008\n",
      "149      's    0.000000\n",
      "179     n't    0.000000\n",
      "\n",
      "[207 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "feature_importances = abs(model.coef_[0])  # Take the absolute values of the coefficients\n",
    "\n",
    "# Assuming the features are not named, create a feature list\n",
    "feature_names = [f'feature_{i}' for i in range(X.shape[1])]\n",
    "\n",
    "# Create a DataFrame with feature names and their importance scores\n",
    "feature_importance_df = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'Importance': feature_importances\n",
    "})\n",
    "\n",
    "# Sort the DataFrame by importance scores in descending order\n",
    "feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(feature_importance_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6537cab3-fc65-4c91-822b-8cfb6d51437f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'the': 10.082099153229182, 'of': 8.639379456010932, 'a': 7.0659546201480525, 'to': 4.876597115049602, 'i': 4.086469184523478, 'he': 3.251434358914396, 'male_pronouns': 3.251434358914396, 'her': 3.1799090919003126, 'it': 2.778046705562153, 'had': 2.729513495160517}\n"
     ]
    }
   ],
   "source": [
    "top_10_df = feature_importance_df[:10]\n",
    "top_10_df.head(10)\n",
    "\n",
    "top_10_features = dict(zip(top_10_df['Feature'], top_10_df['Importance']))\n",
    "\n",
    "print(top_10_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b6e68aa3-034a-42d8-a014-b1c5684772ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>samples</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>authentic_mislabeled</th>\n",
       "      <th>synthetic_mislabeled</th>\n",
       "      <th>top_10_features</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[ALCOTT_synthetic_combined_122.txt, ALCOTT_syn...</td>\n",
       "      <td>0.87</td>\n",
       "      <td>[Stoker_1585.txt, Dickens_3308.txt, Alcott_250...</td>\n",
       "      <td>[TWAIN_synthetic_combined_17.txt, AUSTEN_synth...</td>\n",
       "      <td>{'the': 10.082099153229182, 'of': 8.6393794560...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             samples accuracy  \\\n",
       "0  [ALCOTT_synthetic_combined_122.txt, ALCOTT_syn...     0.87   \n",
       "\n",
       "                                authentic_mislabeled  \\\n",
       "0  [Stoker_1585.txt, Dickens_3308.txt, Alcott_250...   \n",
       "\n",
       "                                synthetic_mislabeled  \\\n",
       "0  [TWAIN_synthetic_combined_17.txt, AUSTEN_synth...   \n",
       "\n",
       "                                     top_10_features  \n",
       "0  {'the': 10.082099153229182, 'of': 8.6393794560...  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Printing results to csv\n",
    "\n",
    "results_df.at[0, 'samples'] = sampled_files_list\n",
    "results_df.at[0, 'accuracy'] = accuracy\n",
    "results_df.at[0, 'authentic_mislabeled'] = authentic_mislabeled\n",
    "results_df.at[0, 'synthetic_mislabeled'] = synthetic_mislabeled\n",
    "results_df.at[0, 'top_10_features'] = top_10_features\n",
    "\n",
    "results_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4f2a89bc-3f70-4756-9307-5476c7bef5c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.to_csv('SVM_data.csv', mode='a', header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "429e309c-f616-4caa-8a90-94959df6e9da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>samples</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>authentic_mislabeled</th>\n",
       "      <th>synthetic_mislabeled</th>\n",
       "      <th>top_10_features</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>['ALCOTT_synthetic_combined_122.txt', 'ALCOTT_...</td>\n",
       "      <td>0.87</td>\n",
       "      <td>['Stoker_1585.txt', 'Dickens_3308.txt', 'Alcot...</td>\n",
       "      <td>['TWAIN_synthetic_combined_17.txt', 'AUSTEN_sy...</td>\n",
       "      <td>{'the': 10.082099153229182, 'of': 8.6393794560...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             samples  accuracy  \\\n",
       "0  ['ALCOTT_synthetic_combined_122.txt', 'ALCOTT_...      0.87   \n",
       "\n",
       "                                authentic_mislabeled  \\\n",
       "0  ['Stoker_1585.txt', 'Dickens_3308.txt', 'Alcot...   \n",
       "\n",
       "                                synthetic_mislabeled  \\\n",
       "0  ['TWAIN_synthetic_combined_17.txt', 'AUSTEN_sy...   \n",
       "\n",
       "                                     top_10_features  \n",
       "0  {'the': 10.082099153229182, 'of': 8.6393794560...  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a981bac2-d22c-4e1f-9342-04b6cbed2cad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
