{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1c1edf90-e83a-49bd-ab3b-4e98df962931",
   "metadata": {},
   "source": [
    "import pandas as pd"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f2f24e31-7974-43fc-beb5-9d13fb0d885b",
   "metadata": {},
   "source": [
    "master_df = pd.read_csv(\"master_stats.csv\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9ef4dac1-dee9-402b-9f68-0cfb1acb2113",
   "metadata": {},
   "source": [
    "master_df.head()\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ee1b417d-db28-4b57-b87c-8b8befaf9289",
   "metadata": {},
   "source": [
    "master_df['ID']"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf2fe0c3-272c-459f-965e-1582e181e32b",
   "metadata": {},
   "source": [],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ae0cdc0a-8296-4519-bb8c-9d128276d203",
   "metadata": {},
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Set up directories\n",
    "corpus_dir = Path('corpus')\n",
    "chunks_dir = Path('corpus_chunks')\n",
    "\n",
    "# Create chunks_dir if it does not exist\n",
    "chunks_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Sample DataFrame - in practice, load it from your source\n",
    "# master_df = pd.read_csv('path_to_master_df.csv')  # or however you obtain the DataFrame\n",
    "\n",
    "\n",
    "# Function to read a file and tokenize the text\n",
    "def read_and_tokenize_file(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        text = file.read()\n",
    "    tokens = word_tokenize(text)\n",
    "    return tokens\n",
    "\n",
    "# Function to save a chunk to a file\n",
    "def save_chunk(tokens, original_filename, chunk_id):\n",
    "    chunk_text = ' '.join(tokens)\n",
    "    stripped_filename = file_id.replace('.txt', '')\n",
    "    chunk_filename = chunks_dir / f\"{stripped_filename}_{chunk_id}.txt\"\n",
    "    with open(chunk_filename, 'w', encoding='utf-8') as file:\n",
    "        file.write(chunk_text)\n",
    "\n",
    "# Process each file listed in master_df\n",
    "for file_id in master_df['ID']:\n",
    "    file_path = corpus_dir / file_id\n",
    "    \n",
    "    # Read and tokenize file\n",
    "    tokens = read_and_tokenize_file(file_path)\n",
    "    \n",
    "    # Chunking the tokens into 500-token parts\n",
    "    chunk_size = 500\n",
    "    for i in range(0, len(tokens), chunk_size):\n",
    "        chunk_tokens = tokens[i:i+chunk_size]\n",
    "        chunk_id = i // chunk_size + 1\n",
    "        save_chunk(chunk_tokens, file_id, chunk_id)\n",
    "\n",
    "print(\"Processing complete.\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a19f7e68-0524-4255-a142-f1c950d4dedd",
   "metadata": {},
   "source": [
    "!pwd"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "cf2f73b1-cfa2-42ae-857a-fb28974bd24e",
   "metadata": {},
   "source": [
    "master_text_list = master_df['ID'].tolist()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8033cca3-83a0-499a-a8b7-a010c949b3f3",
   "metadata": {},
   "source": [
    "master_text_list"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6621b8be-81b8-46f1-8f28-1ad73fae29b4",
   "metadata": {},
   "source": [
    "# List all file names in the directory\n",
    "chunk_names = [f for f in os.listdir(chunks_dir) if os.path.isfile(os.path.join(chunks_dir, f))]"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "486d8bea-cc01-4d73-a360-2624ab145a29",
   "metadata": {},
   "source": [
    "chunk_names[:20]"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "58cc06ff-237e-4dbb-8ee7-fa0df88b80d2",
   "metadata": {},
   "source": [
    "for text in master_text_list:\n",
    "    stripped_text = text.replace('.txt', '')\n",
    "    count = 0\n",
    "    for subtext in chunk_names:\n",
    "        stripped_subtext = subtext.replace('.txt', '')\n",
    "        if stripped_text in stripped_subtext:\n",
    "            count += 1\n",
    "    print(text, count)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "7b698410-1360-45e5-8c30-bef5668264da",
   "metadata": {},
   "source": [
    "from pathlib import Path\n",
    "import nltk\n",
    "\n",
    "# Download the tokenizer models if not already done\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Define the directory containing the files\n",
    "directory = Path('corpus_chunks')  # Replace with your directory path\n",
    "\n",
    "# Initialize the list to store filenames with fewer than 500 tokens\n",
    "shorts = []\n",
    "\n",
    "# Iterate over each file in the directory\n",
    "for file_path in directory.iterdir():\n",
    "    if file_path.is_file():  # Ensure it's a file\n",
    "        # Read and tokenize the file\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            text = file.read()\n",
    "            tokens = word_tokenize(text)\n",
    "            num_tokens = len(tokens)\n",
    "        \n",
    "        # Check if the number of tokens is fewer than 500\n",
    "        if num_tokens < 500:\n",
    "            shorts.append(file_path.name)  # Append the filename to the list\n",
    "\n",
    "# Print the list of filenames with fewer than 500 tokens\n",
    "print(\"Files with fewer than 500 tokens:\", shorts)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "488bb20b-49d3-4748-9681-1de6c503303f",
   "metadata": {},
   "source": [
    "import random\n",
    "\n",
    "\n",
    "# Define the folder containing the files\n",
    "directory = Path('corpus_chunks')  # Replace with your directory path\n",
    "\n",
    "# Define the list of prefixes (the initial strings before the underscore)\n",
    "prefixes = [name.replace('.txt', '') for name in master_text_list]\n",
    "\n",
    "print(prefixes)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "286d2df0-616d-4c7b-9102-9bed24120a26",
   "metadata": {},
   "source": [
    "files_by_prefix = {prefix: [] for prefix in prefixes}\n",
    "\n",
    "# Iterate over all files in the directory and group them by prefix\n",
    "for file_path in directory.iterdir():\n",
    "    if file_path.is_file():\n",
    "        # Extract the prefix (the part before the last underscore)\n",
    "        file_name = file_path.name\n",
    "        parts = file_name.rsplit('_', 1)  # Split from the right at the last underscore\n",
    "        file_prefix = parts[0] if len(parts) > 1 else file_name  # Get the part before the last underscore\n",
    "\n",
    "        # If the prefix is one of the predefined prefixes, add the file to the list\n",
    "        if file_prefix in files_by_prefix:\n",
    "            files_by_prefix[file_prefix].append(file_path)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "b1d88b79-29bf-442f-9443-4db8a109f648",
   "metadata": {},
   "source": [
    "file_by_prefix = [x for x in files_by_prefix if x not in shorts]"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "bbe4e435-b7ca-40e7-8373-527464602c22",
   "metadata": {},
   "source": [
    "\n",
    "# Initialize a list to store the sampled file names\n",
    "sampled_files_list = []\n",
    "\n",
    "# Randomly sample 100 files from each prefix group\n",
    "for prefix, files in files_by_prefix.items():\n",
    "    # Check if there are at least 100 files to sample\n",
    "    if len(files) > 100:\n",
    "        sampled = random.sample(files, 100)\n",
    "    else:\n",
    "        sampled = files  # If fewer than 100 files, sample all of them\n",
    "\n",
    "    # Add the filenames of the sampled files to the list\n",
    "    sampled_files_list.extend(file.name for file in sampled)\n",
    "\n",
    "print(sampled_files_list)\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "1261df04-9d2d-42fc-8cc7-7356a6fc3b4a",
   "metadata": {},
   "source": [
    "len(sampled_files_list)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "6ff4391d-5cb1-448d-8308-02c10acd3756",
   "metadata": {},
   "source": [
    "samples_df = pd.DataFrame(sampled_files_list, columns=['ID'])"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "4b0925bd-3291-4485-bdce-0a8edf6ed26a",
   "metadata": {},
   "source": [
    "samples_df.head()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "f307f161-809e-4dfd-b7ff-991561aa8fea",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# Define a function to apply the condition\n",
    "def check_synthetic(value):\n",
    "    if 'synthetic' in value:\n",
    "        return 'synthetic'\n",
    "    else:\n",
    "        return 'authentic'\n",
    "\n",
    "# Apply the function to the 'text' column and create a new column 'status'\n",
    "samples_df['category'] = samples_df['ID'].apply(check_synthetic)\n",
    "\n",
    "samples_df.head(-50)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "3bee2c39-a0b5-4846-9a37-3fa9fd8025f2",
   "metadata": {},
   "source": [
    "samples_df.to_csv(\"samples_labels.csv\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "ab8772f9-9fc3-472e-bc4e-28e12bcaef54",
   "metadata": {},
   "source": [
    "with open('content_words.txt', 'r') as file:\n",
    "    content_words = [line.strip() for line in file]"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "4e111c5f-22da-4ef6-b60e-244959b9bb93",
   "metadata": {},
   "source": [
    "print(content_words)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "abd2c31e-21dd-457e-9a44-3ec5695fcd29",
   "metadata": {},
   "source": [
    "with open('top_stops.txt', 'r') as file:\n",
    "    top_stops = [line.strip() for line in file]\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "b0987495-bbf0-43fe-9dff-551a2a5032f4",
   "metadata": {},
   "source": [
    "print(top_stops)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "7eb421a3-61f8-44de-96e6-2d2fd58473af",
   "metadata": {},
   "source": [
    "# Add empty columns to the DataFrame based on the words list\n",
    "for word in top_stops:\n",
    "    samples_df[word] = pd.NA  # Use pd.NA to represent missing values\n",
    "\n",
    "samples_df.head()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "aa3d0b73-debd-4628-9ab5-c0674b931e8b",
   "metadata": {},
   "source": [
    "for word in content_words:\n",
    "    samples_df[word] = pd.NA  # Use pd.NA to represent missing values\n",
    "\n",
    "samples_df.head()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "9e72328c-b39d-42fc-b082-fbb097adeeb7",
   "metadata": {},
   "source": [
    "samples_df.info()"
   ],
   "outputs": []
  },
  {
   "cell_type": "raw",
   "id": "c57d2674-1f70-4ca1-bf20-db9da82cc75f",
   "metadata": {},
   "source": [
    "samples_df[\"TTR\"] = pd.NA\n",
    "samples_df[\"mean_sentence_length\"] = pd.NA\n",
    "samples_df[\"male_pronouns\"] = pd.NA\n",
    "samples_df[\"female_pronouns\"] = pd.NA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f13e61be-e7ad-47cc-93c1-b3aec4cc012c",
   "metadata": {},
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Ensure necessary resources are downloaded\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "# Function to get the WordNet POS tag\n",
    "def get_wordnet_pos(word):\n",
    "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "    return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "# Initialize the lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Define the folder where the files are located\n",
    "folder_path = 'corpus_chunks'\n",
    "\n",
    "# Iterate through the columns, excluding 'ID' and 'category'\n",
    "for column in samples_df.columns:\n",
    "    if column not in ['ID', 'category']:\n",
    "        # Iterate through each row in samples_df\n",
    "        for index, row in samples_df.iterrows():\n",
    "            file_id = row['ID']\n",
    "            file_path = os.path.join(folder_path, file_id)\n",
    "            \n",
    "            # Read the content of the file\n",
    "            with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                content = file.read()\n",
    "                \n",
    "            # Tokenize the content\n",
    "            tokens = word_tokenize(content)\n",
    "            \n",
    "            # Lemmatize each word in the content\n",
    "            lemmatized_tokens = [lemmatizer.lemmatize(token.lower(), get_wordnet_pos(token)) for token in tokens]\n",
    "            \n",
    "            # Count the occurrences of the lemmatized column name in the lemmatized content\n",
    "            token_count = lemmatized_tokens.count(lemmatizer.lemmatize(column.lower(), get_wordnet_pos(column)))\n",
    "            \n",
    "            # Calculate the total number of tokens\n",
    "            total_tokens = len(lemmatized_tokens)\n",
    "            \n",
    "            # Calculate the relative frequency\n",
    "            relative_frequency = token_count / total_tokens if total_tokens > 0 else 0\n",
    "            \n",
    "            # Store the relative frequency in the original column\n",
    "            samples_df.at[index, column] = relative_frequency\n",
    "\n",
    "# Now, samples_df will have the relative frequency directly in the original columns\n",
    "print(samples_df)\n",
    "\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e7101b7-1266-4aa9-9ee4-8d275a634ae7",
   "metadata": {},
   "source": [],
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
