{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "927cb351-1cac-4e6b-a8d4-e665c32240cc",
   "metadata": {},
   "source": [
    "!ls corpus_chunks"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5fff2b90-e03f-4bb3-b533-8abb12e85b72",
   "metadata": {},
   "source": [
    "import pandas as pd"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e23363e9-8406-4634-8f6f-88f99a509fca",
   "metadata": {},
   "source": [
    "samples_df = pd.DataFrame()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9ada1de5-daf5-487c-9e6d-7a46eeb2d3f7",
   "metadata": {},
   "source": [
    "samples_df[\"ID\"] = None"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8f50324c-e95f-4603-92d8-6cb912baf44f",
   "metadata": {},
   "source": [
    "samples_df.head()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dbd17cf6-665e-4bf5-bf5d-4a85d31ea18a",
   "metadata": {},
   "source": [
    "from pathlib import Path\n",
    "import nltk\n",
    "\n",
    "# Download the tokenizer models if not already done\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Initialize the list to store filenames with fewer than 500 tokens\n",
    "shorts = []\n",
    "ids = []\n",
    "\n",
    "import os\n",
    "\n",
    "# Path to the folder containing files\n",
    "folder_path = 'corpus_chunks' \n",
    "\n",
    "# Get a list of all filenames in the folder\n",
    "filenames = os.listdir(folder_path)\n",
    "\n",
    "for filename in filenames:\n",
    "    text =  open(os.path.join(folder_path, filename), 'r').read()\n",
    "    tokens = word_tokenize(text)\n",
    "    num_tokens = len(tokens)\n",
    "    if num_tokens < 500:\n",
    "        shorts.append(filename)  # Append the filename to the list\n",
    "    else:\n",
    "        ids.append(filename)\n",
    "            \n",
    "            \n",
    "print(ids[:20])"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f106d35f-4a10-4dc1-a279-4d60de58be25",
   "metadata": {},
   "source": [
    "samples_df['ID'] = ids"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4eb40cb3-1e6a-4419-93e1-8153cd20ac48",
   "metadata": {},
   "source": [
    "samples_df.head()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b017d5d8-87a5-47d7-ac4c-c34bf2276cc2",
   "metadata": {},
   "source": [
    "samples_df = samples_df.sort_values(by='ID', ascending=True)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f1ae3cb2-ac3a-471a-bbb3-29fddd45b171",
   "metadata": {},
   "source": [
    "samples_df.head()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "139c47e2-a2d8-4fbf-8028-08f72930059b",
   "metadata": {},
   "source": [
    "samples_df = samples_df.set_index('ID', inplace=False)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c38aecc2-0ec1-4131-ba0a-55c2aedf9991",
   "metadata": {},
   "source": [
    "samples_df.head()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c4671ae1-3dfd-42d5-aa8a-f6545d773016",
   "metadata": {},
   "source": [
    "samples_df[\"nation\"] = 0\n",
    "\n",
    "british = [\"dickens\", \"stoker\", \"austen\", \"bronte\", \"gaskell\"]\n",
    "american = [\"alcott\", \"hopkins\", \"twain\", \"griggs\", \"chesnutt\"]\n",
    "\n",
    "for idx in samples_df.index:\n",
    "    name = str(idx).split(\"_\")[0].lower()\n",
    "    if name in british:\n",
    "        samples_df.loc[idx, \"nation\"] = \"British/Irish\"\n",
    "    else:\n",
    "        samples_df.loc[idx, \"nation\"] = \"American\""
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "0b1c9f1a-2617-4950-a5b9-40f1fdac2254",
   "metadata": {},
   "source": [
    "# Define a function to apply the condition\n",
    "def check_synthetic(value):\n",
    "    if 'synthetic' in value:\n",
    "        return 'synthetic'\n",
    "    else:\n",
    "        return 'authentic'\n",
    "\n",
    "# Apply the function to the 'text' column and create a new column 'status'\n",
    "samples_df['category'] = samples_df.index.to_series().apply(check_synthetic)\n",
    "\n",
    "samples_df.head(-50)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f126ffd0-2426-45d8-b3dc-bda4acd97bc3",
   "metadata": {},
   "source": [
    "samples_df[\"mean_sen_len\"] = 0\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "for file in filenames:\n",
    "    text = open(os.path.join(\"corpus_chunks\", file), \"r\").read()\n",
    "    sentences = sent_tokenize(text)\n",
    "    tokens = word_tokenize(text)\n",
    "    mean_sen_len = (len(tokens))/(len(sentences))\n",
    "    samples_df.loc[file, \"mean_sen_len\"] = mean_sen_len\n",
    "    "
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "87aa6ea6-21cc-4f3b-828a-2454d5160a53",
   "metadata": {},
   "source": [
    "samples_df[\"male_pronouns\"] = 0\n",
    "\n",
    "male_pronouns = [\"him\", \"his\", \"he\"]\n",
    "\n",
    "for file in filenames:\n",
    "    text = open(os.path.join(\"corpus_chunks\", file), \"r\").read()\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [token.lower() for token in tokens if token.isalpha()]  # Tokenize and filter alphabetic words\n",
    "    \n",
    "    # Iterate through each content word to count its occurrences\n",
    "    for x in male_pronouns:\n",
    "        count_x = tokens.count(x)  # Use list count function for counting occurrences\n",
    "        rel_freq = (count_x/(len(tokens)))\n",
    "        samples_df.loc[file, \"male_pronouns\"] = rel_freq  # Set the count in the correct row and column"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "bd8f3eb4-cfe8-4ba9-8054-7021e14427bc",
   "metadata": {},
   "source": [
    "samples_df[\"female_pronouns\"] = 0\n",
    "\n",
    "female_pronouns = [\"her\", \"hers\", \"she\"]\n",
    "\n",
    "for file in filenames:\n",
    "    text = open(os.path.join(\"corpus_chunks\", file), \"r\").read()\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [token.lower() for token in tokens if token.isalpha()]  # Tokenize and filter alphabetic words\n",
    "    \n",
    "    # Iterate through each content word to count its occurrences\n",
    "    for x in female_pronouns:\n",
    "        count_x = tokens.count(x)  # Use list count function for counting occurrences\n",
    "        rel_freq = (count_x/(len(tokens)))\n",
    "        samples_df.loc[file, \"female_pronouns\"] = rel_freq  # Set the count in the correct row and column"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "70aa151d-8e6e-4657-9aed-fc05db412fe9",
   "metadata": {},
   "source": [
    "samples_df[\"TTR\"] = 0\n",
    "\n",
    "for file in filenames:\n",
    "    text = open(os.path.join(\"corpus_chunks\", file), \"r\").read()\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [token.lower() for token in tokens if token.isalpha()]  # Tokenize and filter alphabetic words\n",
    "    types = set(tokens)\n",
    "    total_tokens = len(tokens)\n",
    "    ttr = len(types) / total_tokens\n",
    "    samples_df.loc[file, \"TTR\"] = ttr"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9d012e0a-b145-4ca0-8b14-4cd68b070bbb",
   "metadata": {},
   "source": [
    "samples_df[\"lex_density\"] = 0\n",
    "\n",
    "from nltk import pos_tag\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "for file in filenames:\n",
    "    text = open(os.path.join(\"corpus_chunks\", file), \"r\").read()\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [token.lower() for token in tokens if token.isalpha()]  # Tokenize and filter alphabetic words\n",
    "    pos_tags = pos_tag(tokens)\n",
    "    lexical_pos = {'NN', 'NNS', 'NNP', 'NNPS',  # Nouns\n",
    "                   'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ',  # Verbs\n",
    "                   'JJ', 'JJR', 'JJS',  # Adjectives\n",
    "                   'RB', 'RBR', 'RBS'}  # Adverbs\n",
    "    \n",
    "    lexical_words = [word for word, pos in pos_tags if pos in lexical_pos]\n",
    "    total_words = len(tokens)\n",
    "    lexical_density = len(lexical_words) / total_words\n",
    "    samples_df.loc[file, \"lex_density\"] = lexical_density"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "51ba52ee-e7ef-4e4b-ac11-84a8e7c19bf8",
   "metadata": {},
   "source": [
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "samples_df[\"VADER_sentiment\"] = 0\n",
    "\n",
    "for file in filenames:\n",
    "    text = open(os.path.join(\"corpus_chunks\", file), \"r\").read()\n",
    "    sentiment_scores = SentimentIntensityAnalyzer().polarity_scores(text)\n",
    "    sentiment_score = sentiment_scores['compound']\n",
    "    samples_df.loc[file, \"VADER_sentiment\"] = sentiment_score\n",
    "\n",
    "\n",
    "\n",
    "samples_df['VADER_sentiment']"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "073c24d1-13c4-4033-9f71-a717d7370c5e",
   "metadata": {},
   "source": [
    "samples_df[\"gender\"] = 0\n",
    "\n",
    "women_writers = [\"alcott\", \"gaskell\", \"austen\", \"bronte\", \"hopkins\"]\n",
    "male_writers = [\"dickens\", \"stoker\", \"twain\", \"griggs\", \"chesnutt\"]\n",
    "\n",
    "for idx in samples_df.index:\n",
    "    name = str(idx).split(\"_\")[0].lower()\n",
    "    if name in women_writers:\n",
    "        samples_df.loc[idx, \"gender\"] = \"female\"\n",
    "    else:\n",
    "        samples_df.loc[idx, \"gender\"] = \"male\""
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ae91d4a1-518e-48dc-8d18-e19e85b0c2d6",
   "metadata": {},
   "source": [
    "concreteness_df = pd.read_excel(\"brysbaert_concreteness.xlsx\")\n",
    "\n",
    "concreteness_dict = dict(zip(concreteness_df['Word'].str.lower(), concreteness_df['Conc.M']))\n",
    "\n",
    "for file in filenames:\n",
    "    text = open(os.path.join(\"corpus_chunks\", file), \"r\").read()\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [token.lower() for token in tokens if token.isalpha()]  # Tokenize and filter alphabetic words\n",
    "    concreteness_scores = [concreteness_dict.get(token) for token in tokens if token in concreteness_dict]\n",
    "    avg_concreteness = sum(concreteness_scores) / len(concreteness_scores)\n",
    "    samples_df.loc[file, \"concreteness\"] = avg_concreteness"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d826f118-92e9-465c-ae13-b203c66d3c86",
   "metadata": {},
   "source": [
    "samples_df.columns.values"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "07611178-4ccf-4d59-af19-e622f9f94de9",
   "metadata": {},
   "source": [
    "samples_df[\"concreteness\"]"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "b1f1c866-6d43-4dc7-9a66-29ee6a61df84",
   "metadata": {},
   "source": [
    "content_words_file = open(\"content_words.txt\", \"r\").read()\n",
    "content_words = content_words_file.split(\"\\n\")\n",
    "content_words[:20]"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "537c501d-6174-47de-b402-65db43571a2b",
   "metadata": {},
   "source": [
    "# Create columns for each word in content_words if they don't already exist\n",
    "for x in content_words:\n",
    "    if x not in samples_df.columns:\n",
    "        samples_df[x] = 0  # Initialize the new column with 0\n",
    "\n",
    "# Iterate over each file in the file list\n",
    "for file in filenames:\n",
    "    text = open(os.path.join(\"corpus_chunks\", file), \"r\").read()\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [token.lower() for token in tokens if token.isalpha()]  # Tokenize and filter alphabetic words\n",
    "    \n",
    "    # Iterate through each content word to count its occurrences\n",
    "    for x in content_words:\n",
    "        count_x = tokens.count(x)  # Use list count function for counting occurrences\n",
    "        rel_freq = (count_x/(len(tokens)))\n",
    "        samples_df.loc[file, x] = rel_freq  # Set the count in the correct row and column"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "738da0d0-2d2c-4232-b5cb-43117d7e0065",
   "metadata": {},
   "source": [
    "top_stops_file = open(\"top_stops.txt\", \"r\").read()\n",
    "top_stops = top_stops_file.split(\"\\n\")\n",
    "top_stops[:20]"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "33fe7328-395f-46d5-b9ef-c67e60d51de2",
   "metadata": {},
   "source": [
    "# Create columns for each word in content_words if they don't already exist\n",
    "for x in top_stops:\n",
    "    if x not in samples_df.columns:\n",
    "        samples_df[x] = 0  # Initialize the new column with 0\n",
    "\n",
    "# Iterate over each file in the file list\n",
    "for file in filenames:\n",
    "    text = open(os.path.join(\"corpus_chunks\", file), \"r\").read()\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [token.lower() for token in tokens if token.isalpha()]  # Tokenize and filter alphabetic words\n",
    "    \n",
    "    # Iterate through each content word to count its occurrences\n",
    "    for x in top_stops:\n",
    "        count_x = tokens.count(x)  # Use list count function for counting occurrences\n",
    "        rel_freq = (count_x/(len(tokens)))\n",
    "        samples_df.loc[file, x] = rel_freq  # Set the count in the correct row and column"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "2e8701a0-a523-40dc-9ac7-4601fd59c097",
   "metadata": {},
   "source": [
    "samples_df.info()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "8e78f0e1-0ebc-47ec-9eae-9f5b59eb5e5a",
   "metadata": {},
   "source": [
    "samples_df.columns.values"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "cbe63d1f-5859-4bbc-b21a-576bf27a87d7",
   "metadata": {},
   "source": [
    "samples_df.to_csv(\"master_features_chunks.csv\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "72a3dc77-2eb5-4dc5-9798-53629e9f484a",
   "metadata": {},
   "source": [
    "samples_df.head()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "704976e9-72e0-4fc7-87dc-3836d623fd06",
   "metadata": {},
   "source": [],
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
