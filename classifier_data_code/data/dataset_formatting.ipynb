{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-10-26T19:13:30.717819Z",
     "start_time": "2025-10-26T19:13:28.480443Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "import csv\n",
    "from datasets import load_dataset\n",
    "import numpy as np"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## GPA2Bench Dataset",
   "id": "7a32527dd1c39c92"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-12T01:51:15.050844Z",
     "start_time": "2025-10-12T01:50:55.870542Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#GPA2Bench Dataset\n",
    "data = []\n",
    "directory = 'GPA2Bench/GPA2Bench_PHX' #change to whichever directory\n",
    "for file in os.listdir(directory):\n",
    "    file_df = pd.read_json(os.path.join(directory, file)).transpose()\n",
    "\n",
    "    if file == 'ground.json':\n",
    "        file_df['label'] = 'authentic'\n",
    "        file_df['prompt'] = 'not applicable'\n",
    "    else:\n",
    "        file_name = os.path.splitext(file)[0]\n",
    "        prompt = file_name.split('_')[-1]\n",
    "\n",
    "        file_df['label'] = 'synthetic'\n",
    "        file_df['prompt'] = prompt\n",
    "\n",
    "    data.append(file_df)\n",
    "\n",
    "df = pd.concat(data, ignore_index=True)\n",
    "df.to_csv(f'{directory}/GPA2Bench_PHX.csv', index=False)\n"
   ],
   "id": "a2dfa7c91ec924d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          id                                              title  \\\n",
      "0  0704.0001  calculation of prompt diphoton production cros...   \n",
      "1  0704.0135  a single trapped ion as a time-dependent harmo...   \n",
      "2  0704.0212  curvature and isocurvature perturbations in tw...   \n",
      "3  0704.0240   viscosity, black holes, and quantum field theory   \n",
      "4  0704.0245             one-loop mhv rules and pure yang-mills   \n",
      "\n",
      "                                            abstract      label   prompt  \\\n",
      "0  The production of prompt diphotons plays a cru...  synthetic  prompt1   \n",
      "1  This paper explores the dynamics of a single t...  synthetic  prompt1   \n",
      "2  This paper investigates the role of curvature ...  synthetic  prompt1   \n",
      "3  This paper explores the intriguing interplay b...  synthetic  prompt1   \n",
      "4  In this study, we explore the relationship bet...  synthetic  prompt1   \n",
      "\n",
      "  categories  \n",
      "0        NaN  \n",
      "1        NaN  \n",
      "2        NaN  \n",
      "3        NaN  \n",
      "4        NaN  \n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-12T02:13:48.664866Z",
     "start_time": "2025-10-12T02:13:34.507443Z"
    }
   },
   "cell_type": "code",
   "source": [
    "directory = 'GPA2Bench' #change to whichever directory\n",
    "data = []\n",
    "for file in os.listdir(directory):\n",
    "    if file.endswith('.csv'):\n",
    "        file_df = pd.read_csv(os.path.join(directory, file))\n",
    "\n",
    "        file_name = os.path.splitext(file)[0]\n",
    "        domain = file_name.split('_')[-1]\n",
    "\n",
    "        file_df['domain'] = domain\n",
    "        data.append(file_df)\n",
    "\n",
    "df = pd.concat(data, ignore_index=True)\n",
    "df.to_csv(f'{directory}/GPA2Bench_ALL.csv', index=False)\n"
   ],
   "id": "826c7d3e93b17519",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Zeina\\AppData\\Local\\Temp\\ipykernel_14996\\382968859.py:5: DtypeWarning: Columns (5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  file_df = pd.read_csv(os.path.join(directory, file))\n",
      "C:\\Users\\Zeina\\AppData\\Local\\Temp\\ipykernel_14996\\382968859.py:5: DtypeWarning: Columns (5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  file_df = pd.read_csv(os.path.join(directory, file))\n",
      "C:\\Users\\Zeina\\AppData\\Local\\Temp\\ipykernel_14996\\382968859.py:5: DtypeWarning: Columns (5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  file_df = pd.read_csv(os.path.join(directory, file))\n"
     ]
    }
   ],
   "execution_count": 29
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## M4 Dataset\n",
   "id": "380fc3398769c908"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "###### Fixing davinci (and also bloomz because its in there for some reason) for peerread (something with newlines, don't really know why this version works. full transparency, this is AI",
   "id": "8385752fe6366548"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# Read JSONL line by line\n",
    "data = []\n",
    "with open(\"M4/peerread/peerread_davinci.jsonl\", \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        obj = json.loads(line)\n",
    "        # Optionally clean machine_text or escape newlines here\n",
    "        # For example:\n",
    "        if \"machine_text\" in obj:\n",
    "            obj[\"machine_text\"] = [s.replace(\"\\n\", \" \") for s in obj[\"machine_text\"]]\n",
    "        data.append(obj)\n",
    "\n",
    "# Save cleaned JSONL\n",
    "with open(\"output.jsonl\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for obj in data:\n",
    "        json.dump(obj, f, ensure_ascii=False)\n",
    "        f.write(\"\\n\")\n",
    "\n",
    "# Load with pandas\n",
    "df = pd.read_json(\"M4/peerread/peerread_davinci_fixed.jsonl\", lines=True)\n",
    "print(df.head())"
   ],
   "id": "c877eba870f16e97"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-26T21:32:20.296399Z",
     "start_time": "2025-10-26T21:32:20.212797Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import re\n",
    "\n",
    "with open('M4/peerread/peerread_bloomz.jsonl', 'r', encoding='utf-8') as infile, \\\n",
    "     open('M4/peerread/peerread_bloomz_fixed.jsonl', 'w', encoding='utf-8') as outfile:\n",
    "\n",
    "    for line in infile:\n",
    "        # Remove the davinci_reviews field and its value using regex\n",
    "        # This handles the field appearing anywhere in the line\n",
    "        cleaned_line = re.sub(\n",
    "            r',\"davinci_reviews\":\\[.*?\\](?=,\"|$)',\n",
    "            '',\n",
    "            line\n",
    "        )\n",
    "\n",
    "        cleaned_line = re.sub(\n",
    "            r',\"chatgpt_reviews\":\\[.*?\\](?=,\"|$)',\n",
    "            '',\n",
    "            line\n",
    "        )\n",
    "\n",
    "        # Also handle if it's the first field (no leading comma)\n",
    "        cleaned_line = re.sub(\n",
    "            r'\"davinci_reviews\":\\[.*?\\],',\n",
    "            '',\n",
    "            cleaned_line\n",
    "        )\n",
    "\n",
    "        outfile.write(cleaned_line)\n",
    "\n",
    "print(\"Done! Check output.jsonl\")"
   ],
   "id": "86cad83aa43cdc54",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done! Check output.jsonl\n"
     ]
    }
   ],
   "execution_count": 147
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-26T21:32:21.389133Z",
     "start_time": "2025-10-26T21:32:21.321830Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "file = pd.read_json(\"M4/peerread/peerread_bloomz_fixed.jsonl\", lines=True)\n",
    "file.head()"
   ],
   "id": "de89f3a8d827971c",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "               source   id                                              title  \\\n",
       "0  PeerRead/acl_2017/  104  Bridge Text and Knowledge by Learning Multi-Pr...   \n",
       "1  PeerRead/acl_2017/  105  Morphological Inflection Generation with Hard ...   \n",
       "2  PeerRead/acl_2017/  107  Weakly Supervised Cross-Lingual Named Entity R...   \n",
       "3  PeerRead/acl_2017/  108  A Multigraph-based Model for Overlapping Entit...   \n",
       "4  PeerRead/acl_2017/  117  Improved Neural Relation Detection for Knowled...   \n",
       "\n",
       "                                            abstract  \\\n",
       "0  Integrating text and knowledge into a unified ...   \n",
       "1  We present a neural model for morphological in...   \n",
       "2  The state-of-the-art named entity recognition ...   \n",
       "3  In this paper, we propose a new model for pred...   \n",
       "4  Relation detection is a core component of many...   \n",
       "\n",
       "                                       human_reviews  \\\n",
       "0  [- Strengths:\\n* Outperforms ALIGN in supervis...   \n",
       "1  [- Strengths:\\nThe idea of hard monotonic atte...   \n",
       "2  [This paper presents several weakly supervised...   \n",
       "3  [- Strengths: the paper is well-written, excep...   \n",
       "4  [- Strengths: The paper addresses a relevant t...   \n",
       "\n",
       "                                             prompts  \\\n",
       "0  [Please write a peer review for the paper give...   \n",
       "1  [Please write a peer review for the paper give...   \n",
       "2  [Please write a peer review for the paper give...   \n",
       "3  [Please write a peer review for the paper give...   \n",
       "4  [Please write a peer review for the paper give...   \n",
       "\n",
       "                                       bloom_reviews     score  \\\n",
       "0  [The authors propose to learn multi-prototype ...  0.268284   \n",
       "1  [The authors propose an attention mechanism to...  0.208110   \n",
       "2  [The authors propose an effective method to re...  0.254676   \n",
       "3  [The authors present an interesting approach t...  0.299974   \n",
       "4  [The authors present an improved neural relati...  0.266504   \n",
       "\n",
       "                                              probas  \\\n",
       "0  [0.4606933594, 0.1846923828, 0.1619873047, 0.3...   \n",
       "1  [0.4658203125, 0.2700195312, 0.1539306641, 0.2...   \n",
       "2  [0.4738769531, 0.3132324219, 0.2371826172, 0.4...   \n",
       "3  [0.4094238281, 0.2763671875, 0.1287841797, 0.1...   \n",
       "4  [0.4230957031, 0.3024902344, 0.1875, 0.3144531...   \n",
       "\n",
       "                                              logits              model  \n",
       "0  [19.46875, 17.359375, 18.515625, 22.15625, 18....  bigscience/bloomz  \n",
       "1  [19.125, 17.234375, 18.25, 20.1875, 16.625, 19...  bigscience/bloomz  \n",
       "2  [18.953125, 21.234375, 19.78125, 21.875, 20.57...  bigscience/bloomz  \n",
       "3  [19.03125, 18.046875, 18.15625, 19.171875, 26....  bigscience/bloomz  \n",
       "4  [19.28125, 17.953125, 18.6875, 22.21875, 18.40...  bigscience/bloomz  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "      <th>human_reviews</th>\n",
       "      <th>prompts</th>\n",
       "      <th>bloom_reviews</th>\n",
       "      <th>score</th>\n",
       "      <th>probas</th>\n",
       "      <th>logits</th>\n",
       "      <th>model</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>PeerRead/acl_2017/</td>\n",
       "      <td>104</td>\n",
       "      <td>Bridge Text and Knowledge by Learning Multi-Pr...</td>\n",
       "      <td>Integrating text and knowledge into a unified ...</td>\n",
       "      <td>[- Strengths:\\n* Outperforms ALIGN in supervis...</td>\n",
       "      <td>[Please write a peer review for the paper give...</td>\n",
       "      <td>[The authors propose to learn multi-prototype ...</td>\n",
       "      <td>0.268284</td>\n",
       "      <td>[0.4606933594, 0.1846923828, 0.1619873047, 0.3...</td>\n",
       "      <td>[19.46875, 17.359375, 18.515625, 22.15625, 18....</td>\n",
       "      <td>bigscience/bloomz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>PeerRead/acl_2017/</td>\n",
       "      <td>105</td>\n",
       "      <td>Morphological Inflection Generation with Hard ...</td>\n",
       "      <td>We present a neural model for morphological in...</td>\n",
       "      <td>[- Strengths:\\nThe idea of hard monotonic atte...</td>\n",
       "      <td>[Please write a peer review for the paper give...</td>\n",
       "      <td>[The authors propose an attention mechanism to...</td>\n",
       "      <td>0.208110</td>\n",
       "      <td>[0.4658203125, 0.2700195312, 0.1539306641, 0.2...</td>\n",
       "      <td>[19.125, 17.234375, 18.25, 20.1875, 16.625, 19...</td>\n",
       "      <td>bigscience/bloomz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>PeerRead/acl_2017/</td>\n",
       "      <td>107</td>\n",
       "      <td>Weakly Supervised Cross-Lingual Named Entity R...</td>\n",
       "      <td>The state-of-the-art named entity recognition ...</td>\n",
       "      <td>[This paper presents several weakly supervised...</td>\n",
       "      <td>[Please write a peer review for the paper give...</td>\n",
       "      <td>[The authors propose an effective method to re...</td>\n",
       "      <td>0.254676</td>\n",
       "      <td>[0.4738769531, 0.3132324219, 0.2371826172, 0.4...</td>\n",
       "      <td>[18.953125, 21.234375, 19.78125, 21.875, 20.57...</td>\n",
       "      <td>bigscience/bloomz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>PeerRead/acl_2017/</td>\n",
       "      <td>108</td>\n",
       "      <td>A Multigraph-based Model for Overlapping Entit...</td>\n",
       "      <td>In this paper, we propose a new model for pred...</td>\n",
       "      <td>[- Strengths: the paper is well-written, excep...</td>\n",
       "      <td>[Please write a peer review for the paper give...</td>\n",
       "      <td>[The authors present an interesting approach t...</td>\n",
       "      <td>0.299974</td>\n",
       "      <td>[0.4094238281, 0.2763671875, 0.1287841797, 0.1...</td>\n",
       "      <td>[19.03125, 18.046875, 18.15625, 19.171875, 26....</td>\n",
       "      <td>bigscience/bloomz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PeerRead/acl_2017/</td>\n",
       "      <td>117</td>\n",
       "      <td>Improved Neural Relation Detection for Knowled...</td>\n",
       "      <td>Relation detection is a core component of many...</td>\n",
       "      <td>[- Strengths: The paper addresses a relevant t...</td>\n",
       "      <td>[Please write a peer review for the paper give...</td>\n",
       "      <td>[The authors present an improved neural relati...</td>\n",
       "      <td>0.266504</td>\n",
       "      <td>[0.4230957031, 0.3024902344, 0.1875, 0.3144531...</td>\n",
       "      <td>[19.28125, 17.953125, 18.6875, 22.21875, 18.40...</td>\n",
       "      <td>bigscience/bloomz</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 148
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### looping through files, isolating machine and human text, and creating larger df\n",
   "id": "51e8a2508c63b5ae"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-26T21:08:09.863564Z",
     "start_time": "2025-10-26T21:08:09.055250Z"
    }
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": 121,
   "source": [
    "all_files = []\n",
    "directory = 'M4/peerread'\n",
    "for file in os.listdir(directory):\n",
    "    if 'bloomz' not in file: #bloomz formatted differently, a lot different col names\n",
    "        df = pd.read_json(os.path.join(directory, file), lines=True)\n",
    "\n",
    "        if file == 'peerread_davinci.jsonl':\n",
    "            continue\n",
    "\n",
    "        # Create human_text entries\n",
    "        human_df = df.loc[:, df.columns != 'machine_text'].copy()\n",
    "        if ('peerread' in file):\n",
    "            human_df = human_df.explode('human_text')\n",
    "        human_df.rename(columns={'human_text': 'text'}, inplace=True)\n",
    "        human_df['model'] = 'not applicable'\n",
    "        human_df['label'] = 'authentic'\n",
    "\n",
    "        # Create machine_text entries\n",
    "        machine_df = df.loc[:, df.columns != 'human_text'].copy()\n",
    "        if ('peerread' in file):\n",
    "            machine_df = machine_df.explode('machine_text')\n",
    "        machine_df.rename(columns={'machine_text': 'text'}, inplace=True)\n",
    "        machine_df['label'] = 'synthetic'\n",
    "\n",
    "        # Combine both\n",
    "        combined = pd.concat([human_df, machine_df], ignore_index=True)\n",
    "\n",
    "        all_files.append(combined)\n",
    "\n",
    "# Optional: sort or reset index\n",
    "final_df = pd.concat(all_files, ignore_index=True)\n",
    "\n",
    "# fixing some formatting + merge issues\n",
    "# comment out for wikihow\n",
    "# final_df.drop(columns=['Unnamed: 0' ], inplace=True)\n",
    "\n",
    "final_df.to_csv(f'M4/peerread/M4_peerread.csv', encoding='utf-8', errors=False, index=False)"
   ],
   "id": "acf1e62adbcd2b0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-26T21:08:11.415087Z",
     "start_time": "2025-10-26T21:08:11.409969Z"
    }
   },
   "cell_type": "code",
   "source": "final_df.head()",
   "id": "99d567e77bbd9f55",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                                              prompt  \\\n",
       "0  [Please write a peer review for the paper [], ...   \n",
       "1  [Please write a peer review for the paper [], ...   \n",
       "2  [Please write a peer review for the paper [], ...   \n",
       "3  [Please write a peer review for the paper [], ...   \n",
       "4  [Please write a peer review for the paper [], ...   \n",
       "\n",
       "                                                text           model  \\\n",
       "0  - Strengths:\\n* Outperforms ALIGN in supervise...  not applicable   \n",
       "1  This paper addresses the problem of disambigua...  not applicable   \n",
       "2  - Strengths:\\nGood ideas, simple neural learni...  not applicable   \n",
       "3  - Strengths:\\nThe idea of hard monotonic atten...  not applicable   \n",
       "4  - Strengths: A new encoder-decoder model is pr...  not applicable   \n",
       "\n",
       "               source  source_ID      label  \n",
       "0  PeerRead/acl_2017/        104  authentic  \n",
       "1  PeerRead/acl_2017/        104  authentic  \n",
       "2  PeerRead/acl_2017/        104  authentic  \n",
       "3  PeerRead/acl_2017/        105  authentic  \n",
       "4  PeerRead/acl_2017/        105  authentic  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt</th>\n",
       "      <th>text</th>\n",
       "      <th>model</th>\n",
       "      <th>source</th>\n",
       "      <th>source_ID</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[Please write a peer review for the paper [], ...</td>\n",
       "      <td>- Strengths:\\n* Outperforms ALIGN in supervise...</td>\n",
       "      <td>not applicable</td>\n",
       "      <td>PeerRead/acl_2017/</td>\n",
       "      <td>104</td>\n",
       "      <td>authentic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[Please write a peer review for the paper [], ...</td>\n",
       "      <td>This paper addresses the problem of disambigua...</td>\n",
       "      <td>not applicable</td>\n",
       "      <td>PeerRead/acl_2017/</td>\n",
       "      <td>104</td>\n",
       "      <td>authentic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[Please write a peer review for the paper [], ...</td>\n",
       "      <td>- Strengths:\\nGood ideas, simple neural learni...</td>\n",
       "      <td>not applicable</td>\n",
       "      <td>PeerRead/acl_2017/</td>\n",
       "      <td>104</td>\n",
       "      <td>authentic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[Please write a peer review for the paper [], ...</td>\n",
       "      <td>- Strengths:\\nThe idea of hard monotonic atten...</td>\n",
       "      <td>not applicable</td>\n",
       "      <td>PeerRead/acl_2017/</td>\n",
       "      <td>105</td>\n",
       "      <td>authentic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[Please write a peer review for the paper [], ...</td>\n",
       "      <td>- Strengths: A new encoder-decoder model is pr...</td>\n",
       "      <td>not applicable</td>\n",
       "      <td>PeerRead/acl_2017/</td>\n",
       "      <td>105</td>\n",
       "      <td>authentic</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 122
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Fixing BLOOMZ for arXiv",
   "id": "b9be2616788bfa88"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-26T04:52:11.396766Z",
     "start_time": "2025-10-26T04:52:11.080533Z"
    }
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": 110,
   "source": [
    "bloomz_df = pd.read_json('M4/arxiv/arxiv_bloomz.jsonl', lines=True)\n",
    "all_files = pd.read_csv('M4/arxiv/M4_arxiv.csv')\n",
    "\n",
    "all_files['source_ID'] = all_files['source_ID'].fillna(df['source_id'])\n",
    "all_files.drop(columns=['source_id'], inplace=True)\n",
    "\n",
    "# removing unnecessary columns + renaming some columns\n",
    "# for contect, prompt is the template, machine_text is the prompt with the filled in title and abstract\n",
    "# kept machine_text\n",
    "bloomz_df.drop(columns=['probas', 'logits', 'score', 'title', 'prompt'], inplace=True)\n",
    "bloomz_df.rename(columns={'source_id':'source_ID', 'machine_text': 'prompt'}, inplace=True)\n",
    "\n",
    "# isolating human data, stored in 'abstract'\n",
    "human_df = bloomz_df.loc[:, bloomz_df.columns != 'machine_abstract'].copy()\n",
    "human_df.rename(columns={'abstract': 'text'}, inplace=True)\n",
    "human_df['model'] = 'not applicable'\n",
    "human_df['label'] = 'authentic'\n",
    "\n",
    "# isolating machine data, stored in 'machine_abstract'\n",
    "machine_df = bloomz_df.loc[:, bloomz_df.columns != 'abstract'].copy()\n",
    "machine_df.rename(columns={'machine_abstract': 'text'}, inplace=True)\n",
    "machine_df['label'] = 'synthetic'\n",
    "\n",
    "# concatenating\n",
    "all_files = pd.concat([all_files,human_df, machine_df], ignore_index=True)"
   ],
   "id": "8b9432b04d355b81"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-26T04:52:13.911701Z",
     "start_time": "2025-10-26T04:52:13.907376Z"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30000, 6)\n",
      "model\n",
      "bigscience/bloomz          3000\n",
      "command-xlarge-nightly     3000\n",
      "dolly-v2-12b               3000\n",
      "gpt-3.5-turbo              3000\n",
      "not applicable            15000\n",
      "text-davinci-003           3000\n",
      "dtype: int64\n"
     ]
    }
   ],
   "execution_count": 120,
   "source": [
    "#removing duplicates, consequence of the way I concatenate. probably a better way to do this\n",
    "all_files_groups = all_files.groupby('model').size()\n",
    "print(all_files_groups)\n",
    "\n",
    "all_files.drop_duplicates(subset=['text'], inplace=True)\n",
    "\n",
    "all_files_groups = all_files.groupby('model').size()\n",
    "print(all_files_groups)\n",
    "\n"
   ],
   "id": "75cfce9881c60679"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-26T04:52:59.685560Z",
     "start_time": "2025-10-26T04:52:59.382284Z"
    }
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": 122,
   "source": "all_files.to_csv(f'M4/arxiv/M4_arxiv.csv', encoding='utf-8', errors=False, index=False)",
   "id": "2cbcaf10339d45d3"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Fixing BLOOMZ for reddit",
   "id": "96e5e553b067386d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-26T19:20:31.990212Z",
     "start_time": "2025-10-26T19:20:31.611041Z"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                                              prompt  \\\n",
       "0  I will ask you a question. For this question, ...   \n",
       "1  I will ask you a question. For this question, ...   \n",
       "2  I will ask you a question. For this question, ...   \n",
       "3  I will ask you a question. For this question, ...   \n",
       "4  I will ask you a question. For this question, ...   \n",
       "\n",
       "                                                text           model source  \\\n",
       "0  Henry died in a joust against the captain of h...  not applicable   eli5   \n",
       "1  In 1801, James Monroe and Robert R. Livingston...  not applicable   eli5   \n",
       "2  Good question! [I answered this a few years ba...  not applicable   eli5   \n",
       "3  Watergate is an incredibly interesting period ...  not applicable   eli5   \n",
       "4  No, in medieval Europe,  there were no restric...  not applicable   eli5   \n",
       "\n",
       "  source_ID      label  \n",
       "0    79zzik  authentic  \n",
       "1    7p61s8  authentic  \n",
       "2    a1z5e9  authentic  \n",
       "3    6wi6jh  authentic  \n",
       "4    9wb4m4  authentic  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt</th>\n",
       "      <th>text</th>\n",
       "      <th>model</th>\n",
       "      <th>source</th>\n",
       "      <th>source_ID</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I will ask you a question. For this question, ...</td>\n",
       "      <td>Henry died in a joust against the captain of h...</td>\n",
       "      <td>not applicable</td>\n",
       "      <td>eli5</td>\n",
       "      <td>79zzik</td>\n",
       "      <td>authentic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I will ask you a question. For this question, ...</td>\n",
       "      <td>In 1801, James Monroe and Robert R. Livingston...</td>\n",
       "      <td>not applicable</td>\n",
       "      <td>eli5</td>\n",
       "      <td>7p61s8</td>\n",
       "      <td>authentic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I will ask you a question. For this question, ...</td>\n",
       "      <td>Good question! [I answered this a few years ba...</td>\n",
       "      <td>not applicable</td>\n",
       "      <td>eli5</td>\n",
       "      <td>a1z5e9</td>\n",
       "      <td>authentic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I will ask you a question. For this question, ...</td>\n",
       "      <td>Watergate is an incredibly interesting period ...</td>\n",
       "      <td>not applicable</td>\n",
       "      <td>eli5</td>\n",
       "      <td>6wi6jh</td>\n",
       "      <td>authentic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I will ask you a question. For this question, ...</td>\n",
       "      <td>No, in medieval Europe,  there were no restric...</td>\n",
       "      <td>not applicable</td>\n",
       "      <td>eli5</td>\n",
       "      <td>9wb4m4</td>\n",
       "      <td>authentic</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 9,
   "source": [
    "bloomz_df = pd.read_json('M4/reddit/reddit_bloomz.jsonl', lines=True)\n",
    "all_files = pd.read_csv('M4/reddit/M4_reddit.csv')\n",
    "\n",
    "all_files.drop(columns={'question_id', 'answer_id'}, inplace=True)\n",
    "\n",
    "bloomz_df.drop(columns=['a_id','score_x', 'probas', 'logits', 'score_y', 'title', 'prompt'],inplace=True)\n",
    "bloomz_df.rename(columns={'q_id':'source_ID', 'machine_text': 'prompt'}, inplace=True)\n",
    "bloomz_df['source'] = 'eli5'\n",
    "\n",
    "# isolating human data, stored in 'abstract'\n",
    "human_df = bloomz_df.loc[:, bloomz_df.columns != 'machine_answer'].copy()\n",
    "human_df['model'] = 'not applicable'\n",
    "human_df['label'] = 'authentic'\n",
    "\n",
    "# isolating machine data, stored in 'machine_abstract'\n",
    "machine_df = bloomz_df.loc[:, bloomz_df.columns != 'text'].copy()\n",
    "machine_df.rename(columns={'machine_answer': 'text'}, inplace=True)\n",
    "machine_df['label'] = 'synthetic'\n",
    "\n",
    "all_files = pd.concat([all_files,human_df, machine_df], ignore_index=True)\n",
    "all_files.head()\n"
   ],
   "id": "f8cebe4be02d5a88"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-26T19:20:33.617009Z",
     "start_time": "2025-10-26T19:20:33.597710Z"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model\n",
      "bigscience/bloomz     3000\n",
      "cohere                3000\n",
      "dolly-v2-12b          3000\n",
      "gpt-3.5-turbo         3000\n",
      "not applicable       15000\n",
      "text-davinci-003      3000\n",
      "dtype: int64\n",
      "model\n",
      "bigscience/bloomz    2999\n",
      "cohere               1221\n",
      "dolly-v2-12b         3000\n",
      "gpt-3.5-turbo        3000\n",
      "not applicable       3000\n",
      "text-davinci-003     3000\n",
      "dtype: int64\n"
     ]
    }
   ],
   "execution_count": 10,
   "source": [
    "all_files_groups = all_files.groupby('model').size()\n",
    "print(all_files_groups)\n",
    "\n",
    "all_files.drop_duplicates(subset=['text'], inplace=True)\n",
    "\n",
    "all_files_groups = all_files.groupby('model').size()\n",
    "print(all_files_groups)"
   ],
   "id": "ee377066809219ad"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-26T19:20:59.791403Z",
     "start_time": "2025-10-26T19:20:59.496876Z"
    }
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": 11,
   "source": "all_files.to_csv(f'M4/reddit/M4_reddit.csv', encoding='utf-8', errors=False, index=False)",
   "id": "1995703bda8206c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Fixing Bloomz for wikihow",
   "id": "4c611dbd44d04816"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-26T19:36:13.005705Z",
     "start_time": "2025-10-26T19:36:12.138033Z"
    }
   },
   "cell_type": "code",
   "source": [
    "bloomz_df = pd.read_json('M4/wikihow/wikihow_bloomz.jsonl', lines=True)\n",
    "all_files = pd.read_csv('M4/wikihow/M4_wikihow.csv')\n",
    "\n",
    "all_files.drop(columns={'source_ID', 'title', 'headline'}, inplace=True)\n",
    "\n",
    "bloomz_df.drop(columns=['headline','title', 'probas', 'logits', 'score', 'title', 'prompt'],inplace=True)\n",
    "bloomz_df.rename(columns={'machine_text': 'prompt'}, inplace=True)\n",
    "\n",
    "# isolating human data, stored in 'abstract'\n",
    "human_df = bloomz_df.loc[:, bloomz_df.columns != 'machine_abstract'].copy()\n",
    "human_df['model'] = 'not applicable'\n",
    "human_df['label'] = 'authentic'\n",
    "\n",
    "# isolating machine data, stored in 'machine_abstract'\n",
    "machine_df = bloomz_df.loc[:, bloomz_df.columns != 'text'].copy()\n",
    "machine_df.rename(columns={'machine_abstract': 'text'}, inplace=True)\n",
    "machine_df['label'] = 'synthetic'\n",
    "\n",
    "all_files = pd.concat([all_files,human_df, machine_df], ignore_index=True)\n",
    "all_files.head()"
   ],
   "id": "e7fd0832f66d59f2",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                                              prompt  \\\n",
       "0  Please, generate wikihow article with length a...   \n",
       "1  Please, generate wikihow article with length a...   \n",
       "2  Please, generate wikihow article with length a...   \n",
       "3  Please, generate wikihow article with length a...   \n",
       "4  Please, generate wikihow article with length a...   \n",
       "\n",
       "                                                text           model   source  \\\n",
       "0   It will work with the Forza Motorsport disc i...  not applicable  wikihow   \n",
       "1   They are about $20 a card. Or, if you want to...  not applicable  wikihow   \n",
       "2  ;\\n,,\\n\\n\\nLots of people don't bother to read...  not applicable  wikihow   \n",
       "3   Perfumes are a blend of different levels of s...  not applicable  wikihow   \n",
       "4   Is it slow? Fast? You have to consider this, ...  not applicable  wikihow   \n",
       "\n",
       "       label  \n",
       "0  authentic  \n",
       "1  authentic  \n",
       "2  authentic  \n",
       "3  authentic  \n",
       "4  authentic  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt</th>\n",
       "      <th>text</th>\n",
       "      <th>model</th>\n",
       "      <th>source</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Please, generate wikihow article with length a...</td>\n",
       "      <td>It will work with the Forza Motorsport disc i...</td>\n",
       "      <td>not applicable</td>\n",
       "      <td>wikihow</td>\n",
       "      <td>authentic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Please, generate wikihow article with length a...</td>\n",
       "      <td>They are about $20 a card. Or, if you want to...</td>\n",
       "      <td>not applicable</td>\n",
       "      <td>wikihow</td>\n",
       "      <td>authentic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Please, generate wikihow article with length a...</td>\n",
       "      <td>;\\n,,\\n\\n\\nLots of people don't bother to read...</td>\n",
       "      <td>not applicable</td>\n",
       "      <td>wikihow</td>\n",
       "      <td>authentic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Please, generate wikihow article with length a...</td>\n",
       "      <td>Perfumes are a blend of different levels of s...</td>\n",
       "      <td>not applicable</td>\n",
       "      <td>wikihow</td>\n",
       "      <td>authentic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Please, generate wikihow article with length a...</td>\n",
       "      <td>Is it slow? Fast? You have to consider this, ...</td>\n",
       "      <td>not applicable</td>\n",
       "      <td>wikihow</td>\n",
       "      <td>authentic</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 32
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-26T19:36:29.439101Z",
     "start_time": "2025-10-26T19:36:29.405452Z"
    }
   },
   "cell_type": "code",
   "source": [
    "all_files_groups = all_files.groupby('model').size()\n",
    "print(all_files_groups)\n",
    "\n",
    "all_files.drop_duplicates(subset=['text'], inplace=True)\n",
    "\n",
    "all_files_groups = all_files.groupby('model').size()\n",
    "print(all_files_groups)"
   ],
   "id": "ba09b8777bf70d98",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model\n",
      "bigscience/bloomz          3000\n",
      "command-xlarge-nightly     3000\n",
      "dolly-v2-12b               3000\n",
      "gpt-3.5-turbo              3000\n",
      "not applicable            15000\n",
      "text-davinci-003           3000\n",
      "dtype: int64\n",
      "model\n",
      "bigscience/bloomz         3000\n",
      "command-xlarge-nightly    3000\n",
      "dolly-v2-12b              3000\n",
      "gpt-3.5-turbo             3000\n",
      "not applicable            3003\n",
      "text-davinci-003          3000\n",
      "dtype: int64\n"
     ]
    }
   ],
   "execution_count": 33
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-26T19:36:58.657698Z",
     "start_time": "2025-10-26T19:36:58.002749Z"
    }
   },
   "cell_type": "code",
   "source": "all_files.to_csv(f'M4/wikihow/M4_wikihow.csv', encoding='utf-8', errors=False, index=False)",
   "id": "1c731b2bb7c4cdd5",
   "outputs": [],
   "execution_count": 34
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Fixing Bloomz for Wikipedia",
   "id": "6c5c3ce96f5f850a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-26T19:42:01.309123Z",
     "start_time": "2025-10-26T19:42:01.231655Z"
    }
   },
   "cell_type": "code",
   "source": [
    "bloomz_df = pd.read_json('M4/wikipedia/wikipedia_bloomz.jsonl', lines=True)\n",
    "bloomz_df.head()"
   ],
   "id": "55fccfdad6d452d4",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                                    id  \\\n",
       "0                   William Whitehouse   \n",
       "1                  Cheryl S. McWatters   \n",
       "2             Lithuanian Lands Militia   \n",
       "3  Mizoram–Manipur–Kachin rain forests   \n",
       "4                            Salesbury   \n",
       "\n",
       "                                                 url  \\\n",
       "0  https://en.wikipedia.org/wiki/William%20Whiteh...   \n",
       "1  https://en.wikipedia.org/wiki/Cheryl%20S.%20Mc...   \n",
       "2  https://en.wikipedia.org/wiki/Lithuanian%20Lan...   \n",
       "3  https://en.wikipedia.org/wiki/Mizoram%E2%80%93...   \n",
       "4            https://en.wikipedia.org/wiki/Salesbury   \n",
       "\n",
       "                                 title  \\\n",
       "0                   William Whitehouse   \n",
       "1                  Cheryl S. McWatters   \n",
       "2             Lithuanian Lands Militia   \n",
       "3  Mizoram–Manipur–Kachin rain forests   \n",
       "4                            Salesbury   \n",
       "\n",
       "                                                text  \\\n",
       "0  William Edward Whitehouse (20 May 1859 – 12 Ja...   \n",
       "1  Cheryl S. McWatters is professor and Father Ed...   \n",
       "2  The Lithuanian Lands Militia () was a military...   \n",
       "3  The Mizoram-Manipur-Kachin rain forests is a s...   \n",
       "4  Salesbury is a village and civil parish in Rib...   \n",
       "\n",
       "                                    machine_abstract  \\\n",
       "0  The William Whitehouse was the first steam-pow...   \n",
       "1  McWatters, Cheryl S. (born March 31, 1953) is ...   \n",
       "2  The Lithuanian lands militia was the first nat...   \n",
       "3  The Mizoram-Manipur-Kachin Rain Forests (MMRF)...   \n",
       "4  The following is the complete list of settleme...   \n",
       "\n",
       "                                        machine_text  \\\n",
       "0  Write an abstract for a Wikipedia article with...   \n",
       "1  Generate an abstract for a Wikipedia article w...   \n",
       "2  Write an abstract for a Wikipedia article with...   \n",
       "3  Write an abstract for a Wikipedia article with...   \n",
       "4  Write an abstract for a Wikipedia article with...   \n",
       "\n",
       "                                              prompt     score  \\\n",
       "0  Write an abstract for a Wikipedia article with...  0.258311   \n",
       "1  Generate an abstract for a Wikipedia article w...  0.353501   \n",
       "2  Write an abstract for a Wikipedia article with...  0.341150   \n",
       "3  Write an abstract for a Wikipedia article with...  0.350881   \n",
       "4  Write an abstract for a Wikipedia article with...  0.295999   \n",
       "\n",
       "                                              probas  \\\n",
       "0  [0.0997314453125, 0.19091796875, 0.42236328125...   \n",
       "1  [0.12054443359375, 0.45849609375, 0.984375, 0....   \n",
       "2  [0.450927734375, 0.90869140625, 0.7685546875, ...   \n",
       "3  [0.5048828125, 0.33154296875, 0.97412109375, 0...   \n",
       "4  [0.137939453125, 0.058319091796875, 0.29516601...   \n",
       "\n",
       "                                              logits              model  \\\n",
       "0  [15.625, 17.375, 17.875, 18.5625, 17.046875, 1...  bigscience/bloomz   \n",
       "1  [15.296875, 19.03125, 26.46875, 18.5625, 18.78...  bigscience/bloomz   \n",
       "2  [17.75, 20.75, 21.578125, 17.28125, 20.953125,...  bigscience/bloomz   \n",
       "3  [17.640625, 17.734375, 22.9375, 22.3125, 24.42...  bigscience/bloomz   \n",
       "4  [16.015625, 15.9140625, 16.671875, 20.1875, 15...  bigscience/bloomz   \n",
       "\n",
       "      source  \n",
       "0  wikipedia  \n",
       "1  wikipedia  \n",
       "2  wikipedia  \n",
       "3  wikipedia  \n",
       "4  wikipedia  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>url</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>machine_abstract</th>\n",
       "      <th>machine_text</th>\n",
       "      <th>prompt</th>\n",
       "      <th>score</th>\n",
       "      <th>probas</th>\n",
       "      <th>logits</th>\n",
       "      <th>model</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>William Whitehouse</td>\n",
       "      <td>https://en.wikipedia.org/wiki/William%20Whiteh...</td>\n",
       "      <td>William Whitehouse</td>\n",
       "      <td>William Edward Whitehouse (20 May 1859 – 12 Ja...</td>\n",
       "      <td>The William Whitehouse was the first steam-pow...</td>\n",
       "      <td>Write an abstract for a Wikipedia article with...</td>\n",
       "      <td>Write an abstract for a Wikipedia article with...</td>\n",
       "      <td>0.258311</td>\n",
       "      <td>[0.0997314453125, 0.19091796875, 0.42236328125...</td>\n",
       "      <td>[15.625, 17.375, 17.875, 18.5625, 17.046875, 1...</td>\n",
       "      <td>bigscience/bloomz</td>\n",
       "      <td>wikipedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Cheryl S. McWatters</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Cheryl%20S.%20Mc...</td>\n",
       "      <td>Cheryl S. McWatters</td>\n",
       "      <td>Cheryl S. McWatters is professor and Father Ed...</td>\n",
       "      <td>McWatters, Cheryl S. (born March 31, 1953) is ...</td>\n",
       "      <td>Generate an abstract for a Wikipedia article w...</td>\n",
       "      <td>Generate an abstract for a Wikipedia article w...</td>\n",
       "      <td>0.353501</td>\n",
       "      <td>[0.12054443359375, 0.45849609375, 0.984375, 0....</td>\n",
       "      <td>[15.296875, 19.03125, 26.46875, 18.5625, 18.78...</td>\n",
       "      <td>bigscience/bloomz</td>\n",
       "      <td>wikipedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Lithuanian Lands Militia</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Lithuanian%20Lan...</td>\n",
       "      <td>Lithuanian Lands Militia</td>\n",
       "      <td>The Lithuanian Lands Militia () was a military...</td>\n",
       "      <td>The Lithuanian lands militia was the first nat...</td>\n",
       "      <td>Write an abstract for a Wikipedia article with...</td>\n",
       "      <td>Write an abstract for a Wikipedia article with...</td>\n",
       "      <td>0.341150</td>\n",
       "      <td>[0.450927734375, 0.90869140625, 0.7685546875, ...</td>\n",
       "      <td>[17.75, 20.75, 21.578125, 17.28125, 20.953125,...</td>\n",
       "      <td>bigscience/bloomz</td>\n",
       "      <td>wikipedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Mizoram–Manipur–Kachin rain forests</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Mizoram%E2%80%93...</td>\n",
       "      <td>Mizoram–Manipur–Kachin rain forests</td>\n",
       "      <td>The Mizoram-Manipur-Kachin rain forests is a s...</td>\n",
       "      <td>The Mizoram-Manipur-Kachin Rain Forests (MMRF)...</td>\n",
       "      <td>Write an abstract for a Wikipedia article with...</td>\n",
       "      <td>Write an abstract for a Wikipedia article with...</td>\n",
       "      <td>0.350881</td>\n",
       "      <td>[0.5048828125, 0.33154296875, 0.97412109375, 0...</td>\n",
       "      <td>[17.640625, 17.734375, 22.9375, 22.3125, 24.42...</td>\n",
       "      <td>bigscience/bloomz</td>\n",
       "      <td>wikipedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Salesbury</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Salesbury</td>\n",
       "      <td>Salesbury</td>\n",
       "      <td>Salesbury is a village and civil parish in Rib...</td>\n",
       "      <td>The following is the complete list of settleme...</td>\n",
       "      <td>Write an abstract for a Wikipedia article with...</td>\n",
       "      <td>Write an abstract for a Wikipedia article with...</td>\n",
       "      <td>0.295999</td>\n",
       "      <td>[0.137939453125, 0.058319091796875, 0.29516601...</td>\n",
       "      <td>[16.015625, 15.9140625, 16.671875, 20.1875, 15...</td>\n",
       "      <td>bigscience/bloomz</td>\n",
       "      <td>wikipedia</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 39
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-26T19:45:39.407871Z",
     "start_time": "2025-10-26T19:45:38.912331Z"
    }
   },
   "cell_type": "code",
   "source": [
    "bloomz_df = pd.read_json('M4/wikipedia/wikipedia_bloomz.jsonl', lines=True)\n",
    "all_files = pd.read_csv('M4/wikipedia/M4_wikipedia.csv')\n",
    "\n",
    "\n",
    "bloomz_df.drop(columns=['id','url', 'probas', 'logits', 'score', 'title', 'prompt'],inplace=True)\n",
    "bloomz_df.rename(columns={'machine_text': 'prompt'}, inplace=True)\n",
    "\n",
    "# isolating human data, stored in 'abstract'\n",
    "human_df = bloomz_df.loc[:, bloomz_df.columns != 'machine_abstract'].copy()\n",
    "human_df['model'] = 'not applicable'\n",
    "human_df['label'] = 'authentic'\n",
    "\n",
    "# isolating machine data, stored in 'machine_abstract'\n",
    "machine_df = bloomz_df.loc[:, bloomz_df.columns != 'text'].copy()\n",
    "machine_df.rename(columns={'machine_abstract': 'text'}, inplace=True)\n",
    "machine_df['label'] = 'synthetic'\n",
    "\n",
    "all_files = pd.concat([all_files,human_df, machine_df], ignore_index=True)\n",
    "all_files.head()"
   ],
   "id": "c65c16ce6dce6690",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                                              prompt  \\\n",
       "0  Write a Wikipedia article with the title \"Will...   \n",
       "1  Write a Wikipedia article with the title \"Cher...   \n",
       "2  Write a Wikipedia article with the title \"Mizo...   \n",
       "3  Write a Wikipedia article with the title \"Sale...   \n",
       "4  Write a Wikipedia article with the title \"Maur...   \n",
       "\n",
       "                                                text           model  \\\n",
       "0  William Edward Whitehouse (20 May 1859 – 12 Ja...  not applicable   \n",
       "1  Cheryl S. McWatters is professor and Father Ed...  not applicable   \n",
       "2  The Mizoram-Manipur-Kachin rain forests is a s...  not applicable   \n",
       "3  Salesbury is a village and civil parish in Rib...  not applicable   \n",
       "4  Sir Maurice Eustace (c. 1590 – 22 June 1665) w...  not applicable   \n",
       "\n",
       "                  source   source_id      label  \n",
       "0  wikipedia-20220301.en  11663875.0  authentic  \n",
       "1  wikipedia-20220301.en  46716205.0  authentic  \n",
       "2  wikipedia-20220301.en  21243283.0  authentic  \n",
       "3  wikipedia-20220301.en  14717133.0  authentic  \n",
       "4  wikipedia-20220301.en  30728425.0  authentic  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt</th>\n",
       "      <th>text</th>\n",
       "      <th>model</th>\n",
       "      <th>source</th>\n",
       "      <th>source_id</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Write a Wikipedia article with the title \"Will...</td>\n",
       "      <td>William Edward Whitehouse (20 May 1859 – 12 Ja...</td>\n",
       "      <td>not applicable</td>\n",
       "      <td>wikipedia-20220301.en</td>\n",
       "      <td>11663875.0</td>\n",
       "      <td>authentic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Write a Wikipedia article with the title \"Cher...</td>\n",
       "      <td>Cheryl S. McWatters is professor and Father Ed...</td>\n",
       "      <td>not applicable</td>\n",
       "      <td>wikipedia-20220301.en</td>\n",
       "      <td>46716205.0</td>\n",
       "      <td>authentic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Write a Wikipedia article with the title \"Mizo...</td>\n",
       "      <td>The Mizoram-Manipur-Kachin rain forests is a s...</td>\n",
       "      <td>not applicable</td>\n",
       "      <td>wikipedia-20220301.en</td>\n",
       "      <td>21243283.0</td>\n",
       "      <td>authentic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Write a Wikipedia article with the title \"Sale...</td>\n",
       "      <td>Salesbury is a village and civil parish in Rib...</td>\n",
       "      <td>not applicable</td>\n",
       "      <td>wikipedia-20220301.en</td>\n",
       "      <td>14717133.0</td>\n",
       "      <td>authentic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Write a Wikipedia article with the title \"Maur...</td>\n",
       "      <td>Sir Maurice Eustace (c. 1590 – 22 June 1665) w...</td>\n",
       "      <td>not applicable</td>\n",
       "      <td>wikipedia-20220301.en</td>\n",
       "      <td>30728425.0</td>\n",
       "      <td>authentic</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 42
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-26T19:46:26.659322Z",
     "start_time": "2025-10-26T19:46:26.635164Z"
    }
   },
   "cell_type": "code",
   "source": [
    "all_files_groups = all_files.groupby('model').size()\n",
    "print(all_files_groups)\n",
    "\n",
    "all_files.drop_duplicates(subset=['text'], inplace=True)\n",
    "\n",
    "all_files_groups = all_files.groupby('model').size()\n",
    "print(all_files_groups)"
   ],
   "id": "c58a4129944a1495",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model\n",
      "bigscience/bloomz         3000\n",
      "cohere-xlarge-nightly     2336\n",
      "dolly-v2-12b              2702\n",
      "gpt-3.5-turbo             2995\n",
      "not applicable           14033\n",
      "text-davinci-003          3000\n",
      "dtype: int64\n",
      "model\n",
      "bigscience/bloomz        2999\n",
      "cohere-xlarge-nightly    2336\n",
      "dolly-v2-12b             2702\n",
      "gpt-3.5-turbo            2995\n",
      "not applicable           4291\n",
      "text-davinci-003         3000\n",
      "dtype: int64\n"
     ]
    }
   ],
   "execution_count": 43
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-26T19:47:00.650430Z",
     "start_time": "2025-10-26T19:47:00.276691Z"
    }
   },
   "cell_type": "code",
   "source": "all_files.to_csv(f'M4/wikipedia/M4_wikipedia.csv', encoding='utf-8', errors=False, index=False)",
   "id": "24b2a0b1fb66644",
   "outputs": [],
   "execution_count": 44
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Fixing Bloomz for peerread",
   "id": "ef8e4aa128eef1da"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-26T21:33:58.785924Z",
     "start_time": "2025-10-26T21:33:58.721201Z"
    }
   },
   "cell_type": "code",
   "source": [
    "bloomz_df = pd.read_json('M4/peerread/peerread_bloomz_fixed.jsonl', lines=True)\n",
    "bloomz_df.head()"
   ],
   "id": "95a0b9959d26e946",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "               source   id                                              title  \\\n",
       "0  PeerRead/acl_2017/  104  Bridge Text and Knowledge by Learning Multi-Pr...   \n",
       "1  PeerRead/acl_2017/  105  Morphological Inflection Generation with Hard ...   \n",
       "2  PeerRead/acl_2017/  107  Weakly Supervised Cross-Lingual Named Entity R...   \n",
       "3  PeerRead/acl_2017/  108  A Multigraph-based Model for Overlapping Entit...   \n",
       "4  PeerRead/acl_2017/  117  Improved Neural Relation Detection for Knowled...   \n",
       "\n",
       "                                            abstract  \\\n",
       "0  Integrating text and knowledge into a unified ...   \n",
       "1  We present a neural model for morphological in...   \n",
       "2  The state-of-the-art named entity recognition ...   \n",
       "3  In this paper, we propose a new model for pred...   \n",
       "4  Relation detection is a core component of many...   \n",
       "\n",
       "                                       human_reviews  \\\n",
       "0  [- Strengths:\\n* Outperforms ALIGN in supervis...   \n",
       "1  [- Strengths:\\nThe idea of hard monotonic atte...   \n",
       "2  [This paper presents several weakly supervised...   \n",
       "3  [- Strengths: the paper is well-written, excep...   \n",
       "4  [- Strengths: The paper addresses a relevant t...   \n",
       "\n",
       "                                             prompts  \\\n",
       "0  [Please write a peer review for the paper give...   \n",
       "1  [Please write a peer review for the paper give...   \n",
       "2  [Please write a peer review for the paper give...   \n",
       "3  [Please write a peer review for the paper give...   \n",
       "4  [Please write a peer review for the paper give...   \n",
       "\n",
       "                                       bloom_reviews     score  \\\n",
       "0  [The authors propose to learn multi-prototype ...  0.268284   \n",
       "1  [The authors propose an attention mechanism to...  0.208110   \n",
       "2  [The authors propose an effective method to re...  0.254676   \n",
       "3  [The authors present an interesting approach t...  0.299974   \n",
       "4  [The authors present an improved neural relati...  0.266504   \n",
       "\n",
       "                                              probas  \\\n",
       "0  [0.4606933594, 0.1846923828, 0.1619873047, 0.3...   \n",
       "1  [0.4658203125, 0.2700195312, 0.1539306641, 0.2...   \n",
       "2  [0.4738769531, 0.3132324219, 0.2371826172, 0.4...   \n",
       "3  [0.4094238281, 0.2763671875, 0.1287841797, 0.1...   \n",
       "4  [0.4230957031, 0.3024902344, 0.1875, 0.3144531...   \n",
       "\n",
       "                                              logits              model  \n",
       "0  [19.46875, 17.359375, 18.515625, 22.15625, 18....  bigscience/bloomz  \n",
       "1  [19.125, 17.234375, 18.25, 20.1875, 16.625, 19...  bigscience/bloomz  \n",
       "2  [18.953125, 21.234375, 19.78125, 21.875, 20.57...  bigscience/bloomz  \n",
       "3  [19.03125, 18.046875, 18.15625, 19.171875, 26....  bigscience/bloomz  \n",
       "4  [19.28125, 17.953125, 18.6875, 22.21875, 18.40...  bigscience/bloomz  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "      <th>human_reviews</th>\n",
       "      <th>prompts</th>\n",
       "      <th>bloom_reviews</th>\n",
       "      <th>score</th>\n",
       "      <th>probas</th>\n",
       "      <th>logits</th>\n",
       "      <th>model</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>PeerRead/acl_2017/</td>\n",
       "      <td>104</td>\n",
       "      <td>Bridge Text and Knowledge by Learning Multi-Pr...</td>\n",
       "      <td>Integrating text and knowledge into a unified ...</td>\n",
       "      <td>[- Strengths:\\n* Outperforms ALIGN in supervis...</td>\n",
       "      <td>[Please write a peer review for the paper give...</td>\n",
       "      <td>[The authors propose to learn multi-prototype ...</td>\n",
       "      <td>0.268284</td>\n",
       "      <td>[0.4606933594, 0.1846923828, 0.1619873047, 0.3...</td>\n",
       "      <td>[19.46875, 17.359375, 18.515625, 22.15625, 18....</td>\n",
       "      <td>bigscience/bloomz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>PeerRead/acl_2017/</td>\n",
       "      <td>105</td>\n",
       "      <td>Morphological Inflection Generation with Hard ...</td>\n",
       "      <td>We present a neural model for morphological in...</td>\n",
       "      <td>[- Strengths:\\nThe idea of hard monotonic atte...</td>\n",
       "      <td>[Please write a peer review for the paper give...</td>\n",
       "      <td>[The authors propose an attention mechanism to...</td>\n",
       "      <td>0.208110</td>\n",
       "      <td>[0.4658203125, 0.2700195312, 0.1539306641, 0.2...</td>\n",
       "      <td>[19.125, 17.234375, 18.25, 20.1875, 16.625, 19...</td>\n",
       "      <td>bigscience/bloomz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>PeerRead/acl_2017/</td>\n",
       "      <td>107</td>\n",
       "      <td>Weakly Supervised Cross-Lingual Named Entity R...</td>\n",
       "      <td>The state-of-the-art named entity recognition ...</td>\n",
       "      <td>[This paper presents several weakly supervised...</td>\n",
       "      <td>[Please write a peer review for the paper give...</td>\n",
       "      <td>[The authors propose an effective method to re...</td>\n",
       "      <td>0.254676</td>\n",
       "      <td>[0.4738769531, 0.3132324219, 0.2371826172, 0.4...</td>\n",
       "      <td>[18.953125, 21.234375, 19.78125, 21.875, 20.57...</td>\n",
       "      <td>bigscience/bloomz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>PeerRead/acl_2017/</td>\n",
       "      <td>108</td>\n",
       "      <td>A Multigraph-based Model for Overlapping Entit...</td>\n",
       "      <td>In this paper, we propose a new model for pred...</td>\n",
       "      <td>[- Strengths: the paper is well-written, excep...</td>\n",
       "      <td>[Please write a peer review for the paper give...</td>\n",
       "      <td>[The authors present an interesting approach t...</td>\n",
       "      <td>0.299974</td>\n",
       "      <td>[0.4094238281, 0.2763671875, 0.1287841797, 0.1...</td>\n",
       "      <td>[19.03125, 18.046875, 18.15625, 19.171875, 26....</td>\n",
       "      <td>bigscience/bloomz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PeerRead/acl_2017/</td>\n",
       "      <td>117</td>\n",
       "      <td>Improved Neural Relation Detection for Knowled...</td>\n",
       "      <td>Relation detection is a core component of many...</td>\n",
       "      <td>[- Strengths: The paper addresses a relevant t...</td>\n",
       "      <td>[Please write a peer review for the paper give...</td>\n",
       "      <td>[The authors present an improved neural relati...</td>\n",
       "      <td>0.266504</td>\n",
       "      <td>[0.4230957031, 0.3024902344, 0.1875, 0.3144531...</td>\n",
       "      <td>[19.28125, 17.953125, 18.6875, 22.21875, 18.40...</td>\n",
       "      <td>bigscience/bloomz</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 149
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-26T21:40:10.141886Z",
     "start_time": "2025-10-26T21:40:09.789433Z"
    }
   },
   "cell_type": "code",
   "source": [
    "bloomz_df = pd.read_json('M4/peerread/peerread_bloomz_fixed.jsonl', lines=True)\n",
    "all_files = pd.read_csv('M4/peerread/M4_peerread.csv')\n",
    "\n",
    "\n",
    "bloomz_df.drop(columns=['probas', 'logits', 'score', 'title', 'abstract'],inplace=True)\n",
    "bloomz_df.rename(columns={'id': 'source_ID', 'prompts': 'prompt'}, inplace=True)\n",
    "\n",
    "# isolating human data, stored in 'abstract'\n",
    "human_df = bloomz_df.loc[:, bloomz_df.columns != 'bloom_reviews'].copy()\n",
    "human_df.rename(columns={'human_reviews': 'text'}, inplace=True)\n",
    "human_df = human_df.explode('text')\n",
    "human_df['model'] = 'not applicable'\n",
    "human_df['label'] = 'authentic'\n",
    "\n",
    "# isolating machine data, stored in 'machine_abstract'\n",
    "machine_df = bloomz_df.loc[:, bloomz_df.columns != 'human_reviews'].copy()\n",
    "machine_df.rename(columns={'bloom_reviews': 'text'}, inplace=True)\n",
    "machine_df = machine_df.explode('text')\n",
    "machine_df['label'] = 'synthetic'\n",
    "\n",
    "all_files = pd.concat([all_files,human_df, machine_df], ignore_index=True)\n",
    "all_files.head()"
   ],
   "id": "a84b83a4c320d428",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                                              prompt  \\\n",
       "0  ['Please write a peer review for the paper []'...   \n",
       "1  ['Please write a peer review for the paper []'...   \n",
       "2  ['Please write a peer review for the paper []'...   \n",
       "3  ['Please write a peer review for the paper []'...   \n",
       "4  ['Please write a peer review for the paper []'...   \n",
       "\n",
       "                                                text           model  \\\n",
       "0  - Strengths:\\n* Outperforms ALIGN in supervise...  not applicable   \n",
       "1  This paper addresses the problem of disambigua...  not applicable   \n",
       "2  - Strengths:\\nGood ideas, simple neural learni...  not applicable   \n",
       "3  - Strengths:\\nThe idea of hard monotonic atten...  not applicable   \n",
       "4  - Strengths: A new encoder-decoder model is pr...  not applicable   \n",
       "\n",
       "               source  source_ID      label  \n",
       "0  PeerRead/acl_2017/        104  authentic  \n",
       "1  PeerRead/acl_2017/        104  authentic  \n",
       "2  PeerRead/acl_2017/        104  authentic  \n",
       "3  PeerRead/acl_2017/        105  authentic  \n",
       "4  PeerRead/acl_2017/        105  authentic  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt</th>\n",
       "      <th>text</th>\n",
       "      <th>model</th>\n",
       "      <th>source</th>\n",
       "      <th>source_ID</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>['Please write a peer review for the paper []'...</td>\n",
       "      <td>- Strengths:\\n* Outperforms ALIGN in supervise...</td>\n",
       "      <td>not applicable</td>\n",
       "      <td>PeerRead/acl_2017/</td>\n",
       "      <td>104</td>\n",
       "      <td>authentic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>['Please write a peer review for the paper []'...</td>\n",
       "      <td>This paper addresses the problem of disambigua...</td>\n",
       "      <td>not applicable</td>\n",
       "      <td>PeerRead/acl_2017/</td>\n",
       "      <td>104</td>\n",
       "      <td>authentic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>['Please write a peer review for the paper []'...</td>\n",
       "      <td>- Strengths:\\nGood ideas, simple neural learni...</td>\n",
       "      <td>not applicable</td>\n",
       "      <td>PeerRead/acl_2017/</td>\n",
       "      <td>104</td>\n",
       "      <td>authentic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>['Please write a peer review for the paper []'...</td>\n",
       "      <td>- Strengths:\\nThe idea of hard monotonic atten...</td>\n",
       "      <td>not applicable</td>\n",
       "      <td>PeerRead/acl_2017/</td>\n",
       "      <td>105</td>\n",
       "      <td>authentic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>['Please write a peer review for the paper []'...</td>\n",
       "      <td>- Strengths: A new encoder-decoder model is pr...</td>\n",
       "      <td>not applicable</td>\n",
       "      <td>PeerRead/acl_2017/</td>\n",
       "      <td>105</td>\n",
       "      <td>authentic</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 153
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-26T21:40:24.607579Z",
     "start_time": "2025-10-26T21:40:24.589110Z"
    }
   },
   "cell_type": "code",
   "source": [
    "all_files_groups = all_files.groupby('model').size()\n",
    "print(all_files_groups)\n",
    "\n",
    "all_files.drop_duplicates(subset=['text'], inplace=True)\n",
    "\n",
    "all_files_groups = all_files.groupby('model').size()\n",
    "print(all_files_groups)"
   ],
   "id": "d40ef32fe74a9166",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model\n",
      "bigscience/bloomz     2340\n",
      "chatgpt               2344\n",
      "cohere                2344\n",
      "davinci               2344\n",
      "dolly-v2              2344\n",
      "not applicable       34322\n",
      "dtype: int64\n",
      "model\n",
      "bigscience/bloomz    2334\n",
      "chatgpt              2344\n",
      "cohere               2342\n",
      "davinci              2344\n",
      "dolly-v2             2344\n",
      "not applicable       2859\n",
      "dtype: int64\n"
     ]
    }
   ],
   "execution_count": 155
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-26T21:40:50.067739Z",
     "start_time": "2025-10-26T21:40:49.799343Z"
    }
   },
   "cell_type": "code",
   "source": "all_files.to_csv(f'M4/peerread/M4_peerread.csv', encoding='utf-8', errors=False, index=False)",
   "id": "2dce04db05d4c875",
   "outputs": [],
   "execution_count": 156
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-26T21:48:04.212946Z",
     "start_time": "2025-10-26T21:48:01.047391Z"
    }
   },
   "cell_type": "code",
   "source": [
    "files = ['M4/peerread/M4_peerread.csv', 'M4/wikipedia/M4_wikipedia.csv','M4/wikihow/M4_wikihow.csv','M4/reddit/M4_reddit.csv','M4/arxiv/M4_arxiv.csv']\n",
    "\n",
    "data =[]\n",
    "\n",
    "for file in files:\n",
    "        file_df = pd.read_csv(file)\n",
    "        data.append(file_df)\n",
    "\n",
    "final_df = pd.concat(data, ignore_index=True)\n",
    "final_df['source_id'] = final_df['source_id'].fillna(final_df['source_ID'])\n",
    "final_df.drop(columns={'source_ID'}, inplace=True)# final_df.to_csv('M4/M4_ALL.csv')\n",
    "final_df.to_csv(f'M4/M4_ALL.csv', encoding='utf-8', errors=False, index=False)"
   ],
   "id": "789c8f2e7acce02b",
   "outputs": [],
   "execution_count": 163
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-26T21:48:16.863443Z",
     "start_time": "2025-10-26T21:48:16.861485Z"
    }
   },
   "cell_type": "code",
   "source": [
    "final_df.head()\n",
    "print(final_df.shape)"
   ],
   "id": "2940fd30f33b68c9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(85111, 6)\n"
     ]
    }
   ],
   "execution_count": 165
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## RAID Dataset",
   "id": "f959deb54241e444"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-26T02:27:26.757648Z",
     "start_time": "2025-10-26T02:25:28.673912Z"
    }
   },
   "cell_type": "code",
   "source": "raid = load_dataset(\"liamdugan/raid\", on_bad_lines='skip')",
   "id": "d1ef0a5e7e33aaa6",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2e2b50b987dd420a9fa37dccb4d3a27d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Generating extra split: 0 examples [00:00, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9d116546f60c488ab6ea9cf249ea8ef5"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Loading dataset shards:   0%|          | 0/24 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8ab3a3326993495baa3749d202ab044c"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-26T02:29:22.700699Z",
     "start_time": "2025-10-26T02:28:24.224817Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_data = raid['train']\n",
    "filtered_noattack = train_data.filter(lambda entry : entry.get('attack') == 'none')\n",
    "filtered_noattack.to_csv(\"RAID/RAID_noattack.csv\")\n",
    "# filtered_noattack.to_csv(\"RAID/RAID_noattack\")\n",
    "# filtered_gpt4 = filtered_noattack.filter(lambda entry : any(term in entry.get('model') for term in ['gpt4', 'human']))\n",
    "# filtered_chatgpt = filtered_noattack.filter(lambda entry : any(term in entry.get('model') for term in ['chatgpt', 'human']))"
   ],
   "id": "3e7ce3d867ed55f2",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Filter:   0%|          | 0/5615820 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "836c617f199141f1b26cb1fddc7b3fe0"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Creating CSV from Arrow format:   0%|          | 0/468 [00:00<?, ?ba/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9f8ae7c6315945ec8a58cb92c1a18533"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "802130727"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-12T05:48:50.384266Z",
     "start_time": "2025-10-12T05:48:48.286874Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df = filtered_gpt4.to_pandas()\n",
    "df.to_csv(\"RAID/RAID_gpt4_noattack.csv\", index=False)"
   ],
   "id": "d823e6a2bf189adf",
   "outputs": [],
   "execution_count": 48
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-12T05:49:05.032693Z",
     "start_time": "2025-10-12T05:49:03.915866Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df = filtered_chatgpt.to_pandas()\n",
    "df.to_csv(\"RAID/RAID_chatgpt_noattack.csv\", index=False)"
   ],
   "id": "dcc483751d99f0d1",
   "outputs": [],
   "execution_count": 49
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "df['label'] = np.where(df['model'] == 'human', 'authentic', 'synthetic')\n",
    "df = df.loc[:, ~df.columns.str.contains('^Unnamed')]\n",
    "df.to_csv(\"RAID/RAID_chatgpt_noattack.csv\", index=False)"
   ],
   "id": "f8cb505146f4be0b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## MAGE Dataset",
   "id": "5759f57d90db6444"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-26T01:58:00.435256Z",
     "start_time": "2025-10-26T01:57:10.038480Z"
    }
   },
   "cell_type": "code",
   "source": "mage = load_dataset(\"yaful/MAGE\")",
   "id": "69248862913208be",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "train.csv:   0%|          | 0.00/404M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ee60fabb04e7487ba29591487aae41c3"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "valid.csv:   0%|          | 0.00/72.3M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ace9fc4a5d674754b873cbb9b31ff09d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "test.csv:   0%|          | 0.00/71.7M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "54f40124e4644ab797c3a42f8ddffaff"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "test_ood_set_gpt.csv: 0.00B [00:00, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "dfff1e1ee12a48a8a87a103d95d144a6"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "test_ood_set_gpt_para.csv: 0.00B [00:00, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "aa4f0a08bd9f49aebb5fea20033c0773"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Generating train split:   0%|          | 0/319071 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6ed00b6b07934753bf2b1a82642f557c"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Generating validation split:   0%|          | 0/56792 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ac9b949920934014a2f97e4be63d494a"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Generating test split:   0%|          | 0/60743 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0da82ac67b844d88b249da0d5a5eac22"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-26T02:16:58.524806Z",
     "start_time": "2025-10-26T02:16:55.228783Z"
    }
   },
   "cell_type": "code",
   "source": [
    "mage_train_data = mage['train'].to_pandas()\n",
    "mage_train_data['label'] = np.where(mage_train_data['label'] == 1, 'authentic', 'synthetic')\n",
    "mage_train_data.to_csv(\"MAGE/MAGE_(train_only).csv\", index=False)\n"
   ],
   "id": "72346a5004068801",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T05:16:42.786581Z",
     "start_time": "2025-10-27T05:16:42.228256Z"
    }
   },
   "cell_type": "code",
   "source": [
    "file = pd.read_csv('MAGE/MAGE_(train_only)_sample.csv')\n",
    "print(file['domain'].unique())\n",
    "print(len(file['model'].unique()))"
   ],
   "id": "c2452379798b7c5d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cmv' 'eli5' 'tldr' 'xsum' 'wp' 'roct' 'hswag' 'yelp' 'squad' 'sci_gen']\n",
      "34\n"
     ]
    }
   ],
   "execution_count": 170
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T05:17:47.194696Z",
     "start_time": "2025-10-27T05:17:45.983371Z"
    }
   },
   "cell_type": "code",
   "source": [
    "file = pd.read_csv('M4/M4_ALL.csv')\n",
    "print(len(file['source'].unique()))\n",
    "print(len(file['model'].unique()))"
   ],
   "id": "292accfb5818d4c9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n",
      "11\n"
     ]
    }
   ],
   "execution_count": 173
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T06:04:47.523681Z",
     "start_time": "2025-10-27T06:04:46.297128Z"
    }
   },
   "cell_type": "code",
   "source": [
    "file = pd.read_csv('RAID/RAID_noattack_sample.csv')\n",
    "print(file['domain'].unique())\n",
    "print(len(file['model'].unique()))"
   ],
   "id": "33a0d013e7b08d2f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['abstracts' 'books' 'news' 'poetry' 'recipes' 'reddit' 'reviews' 'wiki']\n",
      "12\n"
     ]
    }
   ],
   "execution_count": 174
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "e58119511839c613"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
